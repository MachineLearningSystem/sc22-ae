{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifact Evaluation \n",
    "> #### STRONGHOLD: Fast and Affordable Billion-scale Deep Learning Model Training (pap381s2)\n",
    "\n",
    "Deep neural networks (DNNs) with billion-scale parameters have demonstrated impressive performance in solving many tasks. Unfortunately, training a billion-scale DNN requires high-performance GPU servers that are too expensive to purchase and maintain. Existing solutions for enabling larger DNN training with limited resources are inadequate because they suffer from high training time overhead. \n",
    "\n",
    "We present STRONGHOLD, a better approach for enabling large DNN model training by dynamically offloading data to the CPU RAM and using the secondary storage (e.g., an SSD drive). It maintains a working window to overlap the GPU computation with CPU-GPU data movement carefully and exploits the multi-core CPU for optimizer update. Compared to the state-of-the-art offloading-based solutions, STRONGHOLD improves the trainable model size by 1.9x∼6.5x on a 32GB V100 GPU, with 1.2x∼3.7x improvement on the training throughput.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This interactive Jupyter notebook showcases the performance evaluation and comparison between STRONGHOLD and the existing state-of-the-art methods (e.g., Megatron-LM, L2L, ZeRO-Offload and ZeRO-Infinity). \n",
    "\n",
    "\n",
    "<!-- The main results of our works are to compare the performance of our approach (i.e. STRONGHOLD) with the existing state-of-the-art methods. The evaluation criteria include throughput, model sizes, scalability and inference time.\n",
    "\n",
    "STRONGHOLD we proposed aims to increase the trainable model size through dynamic offloading. In this artifact, we show the performance of our apporach (i.e. STRONGHOLD) against the existing state-of-the-art methods:  Megatron-LM, L2L, Zero-offload and Zero-Infinity. The evaluation criteria includes throughput, model sizes, scalability and inference time. We will go through all the evalutions on the largest trainable deep neural network (DNN) size on a single GPU (NVIDIA 32GB V100). -->\n",
    "\n",
    "- **Megatron-LM [1]**: the library supporting Transformer-based models, released by NVIDIA, optimized for tensor parallelism. We choose the `tags/v2.6` version as a reference for the testing throughput and trainable model size.\n",
    "\n",
    "- **L2L [2]**: an offloading strategy, keeping only one Transformer layer in the GPU memory at a time and offloading model parameters between the GPU memory and CPU RAM sequentially. Since L2L stores the optimizer states on the GPU memory, it is limited mainly by the capacity of GPU memory.\n",
    "\n",
    "- **ZeRO-Offload [3]**: a training method, statically storing the model states in the GPU memory and optimizer states in the CPU RAM. It utilizes the CPU computation cycle to update the model parameters through a CPU-version Adam optimizer.\n",
    "\n",
    "- **ZeRO-Infinity [4]**: a training method based on ZeRO-3 [5], utilizing GPU, CPU RAM and/or NVMe secondary storage. In this notebook, we test STRONGHOLD on CPU and GPU against ZeRO-Infinity, since keeping high NVMe I/O for a long time causes a mistake issue of overload disk in the cloud platform, which triggers the process to be killed by the underlying hypervisor.\n",
    "\n",
    "\n",
    "The metric assessed in this notebook includes throughput, model size, scalability, etc., shown as:\n",
    "\n",
    "- The **largest trainable model size**, corresponding to **Fig.6a** in paper.\n",
    "- The **throughput comparison on respective maximum trainable model size** of each baseline, corresponding to **Fig.7a** in the paper.\n",
    "- The **throughput comparison on the maximum trainable model size** of Megatron-LM, corresponding to **Fig.8a** in the paper.\n",
    "- The **nearly linear scaling** on iteration time of STRONGHOLD, corresponding to **Fig.8b** in the paper.\n",
    "- The **impact of GPU working window size** on the performance of STRONGHOLD, corresponding to **Fig.9** in the paper.\n",
    "\n",
    "<!-- - The inference time of running the different model sizes of STRONGHOLD.\n",
    "- Changing two key settings of STONGHOLD (i.e **window size** and **multi-stream**) and evaluate the changes of the throughput. -->\n",
    "\n",
    "PS: The present notebook runs on a rented ECS (virtual machine) with one 32GB-V100 GPU, 90GB CPU RAM and 12 CPU Cores. The hardware configuration differs from that in our paper. Thus, we reproduce the equivalent cases, and results reported in the paper but keep the relevant ratio value the same.\n",
    "\n",
    "<!-- The absolute values might be distinct, but the relevant ratio keeps the same. -->\n",
    "\n",
    "\n",
    "### Reference\n",
    "> [1] Megatron-LM. https://github.com/NVIDIA/Megatron-LM. <br>\n",
    "> [2] B. Pudipeddi et al., “Training large neural networks with constant memory using a new execution algorithm,” arXiv, 2020. <br>\n",
    "> [3] J. Ren et al., “Zero-offload: Democratizing billion-scale model training,” in OSDI, 2021. <br>\n",
    "> [4] S. Rajbhandari, O. Ruwase et al., “Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning,” in SC, 2021, pp. 1–14. <br>\n",
    "> [5] S. Rajbhandari, J. Rasley, O. Ruwase et al., “Zero: Memory optimiza- tions toward training trillion parameter models,” in SC, 2020, pp. 1–16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Important Notes\n",
    "\n",
    "**A few bash scripts take more than half an hour to complete; Please wait for the results before executing the next one.**\n",
    "\n",
    "Overload might lead to a longer wait for results. This issue may occur if multiple reviewers simultaneously run the scripts to generate results. One possible way is to check the running process by `ps aux` before executing any other script.\n",
    "\n",
    "The experiments are customisable as reviewers can edit the Jupyter notebook on the spot. Type your changes with different docker scripts we provided and re-run using **Cell > Run Cells** from the menu.\n",
    "\n",
    "<!-- All experiments run on a single 32GB V100 GPU in order to briefly show that all of the evaultion results in our paper can be reproduced. Since multi-GPU running is not used, some of the results are not included in this artifact, but researchers can surely follow the similar ways to reproduce those results. -->\n",
    "\n",
    "\n",
    "### Links to The Paper\n",
    "\n",
    "**For each step, we highlight that the current evaluation is corresponding to which Section or Figure in the submitted paper.**\n",
    "\n",
    "The main results presented in this notebook correspond to the submitted paper's Figures 6, 7, 8, and 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Setup\n",
    "\n",
    "### 1.1 Let's ensure the docker container (NAME: aetesting) is launched. \n",
    "\n",
    "`docker ps` command shows the current running docker containers. The next cell in this notebook should output the following information. If not produce similar output, please run `!docker stop aetesting` and `!docker start aetesting` commands in a cell to restart it.\n",
    "\n",
    "```\n",
    "CONTAINER ID   IMAGE                    COMMAND       CREATED         STATUS         PORTS     NAMES\n",
    "d67abb7f151c   strongh/sc22-ae:latest   \"/bin/bash\"   3 minutes ago   Up 3 minutes             aetesting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                    COMMAND       CREATED          STATUS          PORTS     NAMES\r\n",
      "f31d08089dcc   strongh/sc22-ae:latest   \"/bin/bash\"   39 minutes ago   Up 39 minutes             aetesting\r\n"
     ]
    }
   ],
   "source": [
    "# to define a magic function for launching scripts in this notebook\n",
    "%alias docker_exec docker exec -w /home/sys/STRONGHOLD -it aetesting /bin/bash -c '\\\n",
    "export PYENV_VIRTUALENV_DISABLE_PROMPT=1 && \\\n",
    "export PYENV_ROOT=\"/root/.pyenv\" && \\\n",
    "export PATH=\"$PYENV_ROOT/bin:$PATH\" && \\\n",
    "eval \"$(pyenv init -)\" && \\\n",
    "eval \"$(pyenv virtualenv-init -)\" && \\\n",
    "pyenv activate py3.9.10 && \\\n",
    "\\\n",
    "%l && \\\n",
    "\\\n",
    "pyenv deactivate'\n",
    "\n",
    "# to show the current running docker containers\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Let's check the runtime environment in the docker container works well. \n",
    "\n",
    "`docker exec` supports executing a bash script in one running container. The following cell executes a command that should output the information, shown as the following:\n",
    "\n",
    "```\n",
    "root\n",
    "/home/sys/STRONGHOLD\n",
    "torch                         1.10.0a0+git71f889c /root/.pyenv/......3.9.10/lib/python3.9/site-packages\n",
    "torchvision                   0.11.0a0+05eae32\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "/home/sys/STRONGHOLD\n",
      "torch                         1.10.0a0+git71f889c /root/.pyenv/versions/3.9.10/envs/py3.9.10/lib/python3.9/site-packages\n",
      "torchvision                   0.11.0a0+05eae32\n"
     ]
    }
   ],
   "source": [
    "%docker_exec whoami && pwd && pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How to run customized commands in the docker container?\n",
    "\n",
    "If executing the customized commands in the docker container, please add `docker_exec` as a prefix to your commands. \n",
    "\n",
    "For example, if executing `nvidia-smi` in the container, the right typing is `docker_exec nvidia-smi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation\n",
    ">The jupyter notebook runs on a VM with a 32G V100 on a public cloud platform. The environment is diffrent from that in the submitted paper. **Herein, the output (absolute values) may differ the results in the paper, but the relative relationship keeps the same!**\n",
    "\n",
    "Next, we use five cases to test performance separately on the largest trainable model size, throughput and scalability. Each case matches a figure in the submitted paper.\n",
    "\n",
    "To reuse the existing log files produced in the previous cases, we recommend you to run these cases one by one, which reduces the total execution time to **about 5 hours**. \n",
    "\n",
    "- 2.1 CASE - The largest trainable model size (Figure 6a in Section VI.A) - around 130 mins\n",
    "- 2.2 CASE - Throughput  on the largest trainable model size supported by each baseline (Figure 7a in Section VI.B) - around 40 mins\n",
    "- 2.3 CASE - Throughput on the largest trainable model size of Megatron-LM (Figure 8a in Section VI.B) - around 45 mins\n",
    "- 2.4 CASE - Nearly linear scaling as model size increases (Figure 8b in Section VI.B) - around 60 mins\n",
    "- 2.5 CASE - Impact of working window size (Figure 9 in Section VI.C) - around 50 mins\n",
    "\n",
    "**All log files will be stored in `/home/sys/STRONGHOLD/results`** as a format of `log_[method]_l-[layers]_h-[hidden size]_bs-[BATCH_SIZE]_ws-[WINDOW_SIZE]_[date].txt`. We print the core content in the log files via `grep` and `awk` for you at the end of each execution.\n",
    "\n",
    "**Launch script** `./examples/run.sh -m [method] -l [layers] -h [hidden size] -b [batch size] -w [window size]` accepts five arguements, where `[method]` takes the values of `megatron-lm`, `l2l`, `zero-offload`, `zero-infinity`, `stronghold` and `all`. Using all to automatically evaluate all approaches. Default values for `[layers]`, `[hidden size]`, `[batch size]`, `[window size]` are 16, 2048, 4 and 4, respectively.\n",
    "\n",
    "PS: some cases would consume over a half-hour because we have to execute all baselines. Please have a coffee and wait for the output before the subsequent execution.\n",
    "\n",
    "<!-- > #### Description of the output log\n",
    ">\n",
    "> `------------------------ arguments ------------------------` and the below information shows the running parameters in details.\n",
    "> \n",
    "> `>>> done with compiling and loading fused kernels. Compilation time: X seconds` means that all the compiling settings including CUDA linking and loading CUDA module have been successfully done. And it also gives the compilation time..\n",
    "> \n",
    "> `>>> done with compiling and loading strongh utils. Compilation time: X seconds` means all the optimizer modules have been successfully linked and compiled.\n",
    "> \n",
    "> `> building train, validation, and test datasets ...` shows that we are now building the training, validation and test dataloaders. And the following information will show the dataset in details.\n",
    "> \n",
    "> `[before the start of training step] datetime: 2022-06-22 22:22:59` <br>\n",
    "> `done with setup ...` shows everything works well before starting the actual training. \n",
    "> \n",
    "> `time (ms) | model-and-optimizer-setup: X | train/valid/test-data-iterators-setup: X` shows each of the setup cost (ms).\n",
    "> \n",
    "> `training ...` and the below log information shows the traing breakdowns. \n",
    "> \n",
    "> **The below log shows the actual model running results, thus should be the one we pay attention for. And those results correspond to Figure 7(a) and Figure 8(a) in our paper. Please refer to our paper for more details.**\n",
    "> \n",
    "> `iteration       X/      50` shows the iteration level, where `X` is the number represents `X` out of the total iteration (50 in this artifact) .\n",
    "> \n",
    "> `[Rank X] (after 10 iterations) memory (MB) | ...` shows the memory footprint.\n",
    ">\n",
    "> `time (ms) | ...` shows the training time breakdowns.\n",
    ">\n",
    "> **All the logs will be stored in `/home/sys/STRONGHOLD/results`** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To clean up the previous log files in the `results` folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%docker_exec rm -rf /home/sys/STRONGHOLD/results/*.txt /home/sys/STRONGHOLD/results/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%docker_exec ls /home/sys/STRONGHOLD/results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CASE - The largest trainable model size (Figure 6a in Section VI.A)\n",
    "\n",
    "In this case, we use GPT-like models to exploit each method's largest trainable model size. Model size changes via increasing/decreasing the number of transformer layers.\n",
    "\n",
    "Here, we evaluate Megatron-LM, L2L, ZeRO-Offload, ZeRO-Infinity and STRONGHOLD on a virtual machine with one 32GB V100, 90GB CPU RAM and 12 CPU Cores to exploit their largest trainable model size and bottleneck. During this process, we configure the `Heads=16, Sequence Length=1024, Batch Size=4` in all GPT-like models and training setups.\n",
    "\n",
    "The largest model sizes have been tested in this notebook, shown in the following table. Please run the following cells to reproduce it. Thanks.\n",
    "\n",
    "| Methods | Largest Trainable Size | Layers | Hidden Size | Heads | Sequence Length | Batch Size |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| Megatron-LM | **1.717 B**| **32** | 2048 | 16 | 1024 | 4 |\n",
    "| L2L | **4.033 B**| **78** | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Offload | **2.522 B**| **48** | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Infinity | **2.522 B**| **48** | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **5.141 B**| **100** | 2048 | 16 | 1024 | 4 |\n",
    "\n",
    "PS: `Errors about GPU/CPU OOM` might be represented as other information, such as 'can not create XXX'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `ps aux` to check if there exists other running processes launched by other reviwers in case of GPU overlead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   4240  3512 pts/0    Ss+  07:09   0:00 /bin/bash\r\n",
      "root         209  0.0  0.0   4340  3808 pts/1    Ss+  07:10   0:00 /bin/bash\r\n",
      "root        3377  0.0  0.0   3976  3224 pts/2    Ss+  07:15   0:00 /bin/bash -c \r\n",
      "root        3535  0.0  0.0   5892  2952 pts/2    R+   07:15   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ps aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 6a in the submitted paper. Please refers to Section VI.A on page 8 for more details. <br><br>Runs around 130 mins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " !!! The training model size in megatron-lm might be much smaller than others, such as zero-offload, stronghold, etc. !!! \n",
      "\n",
      " \n",
      "cd /home/sys/STRONGHOLD/examples/../Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../Megatron-LM/examples/sc22-gpt-megatron.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_megatron-lm_l-32_hs-2048_bs-4_ws-4_2022-07-02.1656730240.txt && cd -\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt2\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.626 seconds\n",
      "time to initialize megatron (seconds): 2.965\n",
      "[after megatron is initialized] datetime: 2022-07-02 02:50:47 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.24\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 02:50:47 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.001054 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 02:50:47 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 137.60 | train/valid/test-data-iterators-setup: 337.08\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 02:50:47 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 5400.3 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.090563E+01 | loss scale: 1.0 | grad norm: 36.791 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.42 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.400259041786194;  SamplesPerSecond: 0.7407052085925413\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 26195.06494140625 | max allocated: 29775.16845703125 | reserved: 29928.0 | max reserved: 29928.0\n",
      "time (ms) | forward-compute: 1379.60 | backward-compute: 3890.09 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 102.96 | batch-generator: 1.98\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 5340.1 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 9.688580E+00 | loss scale: 1.0 | grad norm: 17.054 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.53 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.340105748176574;  SamplesPerSecond: 0.7490488369759035\n",
      "time (ms) | forward-compute: 1312.73 | backward-compute: 3903.03 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 96.67 | batch-generator: 1.33\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 5336.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.169696E+00 | loss scale: 1.0 | grad norm: 11.516 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.54 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.33610475063324;  SamplesPerSecond: 0.7496104718568947\n",
      "time (ms) | forward-compute: 1311.14 | backward-compute: 3900.65 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 96.72 | batch-generator: 1.34\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 5340.9 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 8.941277E+00 | loss scale: 1.0 | grad norm: 15.119 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.53 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.340861296653747;  SamplesPerSecond: 0.748942872286564\n",
      "time (ms) | forward-compute: 1312.15 | backward-compute: 3904.47 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 96.64 | batch-generator: 1.39\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 5339.3 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 8.780692E+00 | loss scale: 1.0 | grad norm: 7.671 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.53 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.339278388023376;  SamplesPerSecond: 0.7491649075598804\n",
      "time (ms) | forward-compute: 1311.55 | backward-compute: 3903.51 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 96.60 | batch-generator: 1.39\n",
      "saving checkpoint at iteration      50 to checkpoints/gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  successfully saved checkpoint at iteration      50 to checkpoints/gpt2\n",
      "time (ms) | save-checkpoint: 62657.14\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 02:56:17 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/sc22-gpt-l2l.sh 78 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_l2l_l-78_hs-2048_bs-4_ws-4_2022-07-02.1656730580.txt && cd -\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_l2l ...................................... True\n",
      "  enbale_strongh .................................. None\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt2_345m_ds\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 78\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ checkpoints/gpt2_345m_ds\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.178 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.990 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module ds_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module ds_cpu_adam...\n",
      ">>> done with compiling and loading strongh utils. Compilation time: 0.635 seconds\n",
      "time to initialize megatron (seconds): 4.091\n",
      "[after megatron is initialized] datetime: 2022-07-02 02:56:29 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4033069056\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_345m_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.26\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 02:56:45 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000687 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 02:56:46 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 16358.41 | train/valid/test-data-iterators-setup: 1110.93\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 02:56:46 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 67767.8 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.126409E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 1.95 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 67.76779985427856;  SamplesPerSecond: 0.05902508283581907\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 15979.15771484375 | max allocated: 21094.26123046875 | reserved: 21240.0 | max reserved: 31392.0\n",
      "time (ms) | forward-compute: 16171.77 | backward-compute: 40047.10 | backward-params-all-reduce: 39.45 | backward-embedding-all-reduce: 0.04 | optimizer: 11250.74 | batch-generator: 3.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/sc22-gpt-l2l.sh: line 56: 31047 Killed                  PYTHONGIL=1 python pretrain_gpt.py --num-layers ${NLAYERS} --hidden-size ${NHIDDEN} --num-attention-heads ${HEADS} --micro-batch-size ${BATCHSIZE} --global-batch-size ${BATCHSIZE} --seq-length ${SEQ} --max-position-embeddings ${SEQ} --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save $CHECKPOINT_PATH --load $CHECKPOINT_PATH --data-path $DATA_PATH --vocab-file ${VOCAB_PATH} --merge-file ${MERGE_PATH} --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 0.00015 --min-lr 1.0e-5 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --lr-warmup-fraction .01 --activations-checkpoint-method uniform --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --enable-l2l\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-offloading.sh 48 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-offload_l-48_hs-2048_bs-4_ws-4_2022-07-02.1656731649.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 03:14:11,719] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-07-02 03:14:13,365] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 03:14:14,242] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-07-02 03:14:14,243] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 48\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 2\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-07-02 03:14:18,212] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-07-02 03:14:18,213] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-07-02 03:14:18,213] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.06 GB, percent = 3.4%\n",
      "[2022-07-02 03:14:18,214] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f046c497310>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-07-02 03:14:18,214] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-07-02 03:14:18,403] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-07-02 03:14:18,404] [INFO] [utils.py:823:see_memory_usage] MA 9.39 GB         Max_MA 9.39 GB         CA 9.39 GB         Max_CA 9 GB \n",
      "[2022-07-02 03:14:18,404] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.07 GB, percent = 3.4%\n",
      " > number of parameters on model parallel rank 0            2.522 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-07-02 03:14:18,409] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f046c497310>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-07-02 03:14:18,414] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-07-02 03:14:18,436] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-07-02 03:14:18,436] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-07-02 03:14:18,437] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-07-02 03:14:18,437] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-07-02 03:14:18,521] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-07-02 03:14:18,521] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-07-02 03:14:18,521] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-07-02 03:14:18,571] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-07-02 03:14:18,571] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-07-02 03:14:18,571] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
      "[2022-07-02 03:14:18,572] [INFO] [stage2.py:113:__init__] Reduce bucket size 50000000\n",
      "[2022-07-02 03:14:18,572] [INFO] [stage2.py:114:__init__] Allgather bucket size 50000000\n",
      "[2022-07-02 03:14:18,572] [INFO] [stage2.py:115:__init__] CPU Offload: True\n",
      "[2022-07-02 03:14:18,572] [INFO] [stage2.py:116:__init__] Round robin gradient partitioning: False\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.22762680053710938 seconds\n",
      "Rank: 0 partition count [1, 1] and sizes[(2521038848, False), (1282048, False)] \n",
      "[2022-07-02 03:14:43,796] [INFO] [utils.py:822:see_memory_usage] Before initializing optimizer states\n",
      "[2022-07-02 03:14:43,797] [INFO] [utils.py:823:see_memory_usage] MA 9.78 GB         Max_MA 9.78 GB         CA 18.8 GB         Max_CA 19 GB \n",
      "[2022-07-02 03:14:43,797] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 13.71 GB, percent = 15.2%\n",
      "[2022-07-02 03:14:59,225] [INFO] [utils.py:822:see_memory_usage] After initializing optimizer states\n",
      "[2022-07-02 03:14:59,226] [INFO] [utils.py:823:see_memory_usage] MA 9.78 GB         Max_MA 9.78 GB         CA 18.8 GB         Max_CA 19 GB \n",
      "[2022-07-02 03:14:59,226] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 42.12 GB, percent = 46.6%\n",
      "[2022-07-02 03:14:59,226] [INFO] [stage2.py:483:__init__] optimizer state initialized\n",
      "[2022-07-02 03:14:59,259] [INFO] [utils.py:822:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2022-07-02 03:14:59,259] [INFO] [utils.py:823:see_memory_usage] MA 9.78 GB         Max_MA 9.78 GB         CA 18.8 GB         Max_CA 19 GB \n",
      "[2022-07-02 03:14:59,260] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 42.12 GB, percent = 46.6%\n",
      "[2022-07-02 03:14:59,260] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2022-07-02 03:14:59,260] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-07-02 03:14:59,260] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f046bfbd190>\n",
      "[2022-07-02 03:14:59,260] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:14:59,261] [INFO] [config.py:1044:print] DeepSpeedEngine configuration:\n",
      "[2022-07-02 03:14:59,261] [INFO] [config.py:1048:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-07-02 03:14:59,261] [INFO] [config.py:1048:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   allreduce_always_fp32 ........ False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   amp_enabled .................. False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   amp_params ................... False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   bfloat16_enabled ............. False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   curriculum_enabled ........... False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   curriculum_params ............ False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   dataloader_drop_last ......... False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   disable_allgather ............ False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   dump_state ................... False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_enabled ........... False\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-07-02 03:14:59,262] [INFO] [config.py:1048:print]   eigenvalue_verbose ........... False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   elasticity_enabled ........... False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   fp16_enabled ................. False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   fp16_mixed_quantize .......... False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   global_rank .................. 0\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   gradient_accumulation_steps .. 1\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   gradient_clipping ............ 1.0\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   loss_scale ................... 0\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   memory_breakdown ............. False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   optimizer_name ............... adam\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   optimizer_params ............. {'lr': 0.00015, 'max_grad_norm': 0.0, 'betas': [0.9, 0.95]}\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   pld_enabled .................. False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   pld_params ................... False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   prescale_gradients ........... False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_change_rate ......... 0.001\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_groups .............. 1\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_offset .............. 1000\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_period .............. 1000\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_rounding ............ 0\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_start_bits .......... 16\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_target_bits ......... 8\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_training_enabled .... False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_type ................ 0\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   quantize_verbose ............. False\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   scheduler_name ............... None\n",
      "[2022-07-02 03:14:59,263] [INFO] [config.py:1048:print]   scheduler_params ............. None\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   sparse_attention ............. None\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   sparse_gradients_enabled ..... False\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   steps_per_print .............. 10\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   tensorboard_enabled .......... False\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   tensorboard_output_path ...... \n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   train_batch_size ............. 4\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   train_micro_batch_size_per_gpu  4\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   use_quantizer_kernel ......... False\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   wall_clock_breakdown ......... True\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   world_size ................... 1\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   zero_allow_untested_optimizer  False\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   zero_config .................. {\n",
      "    \"stage\": 2, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+07, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": null, \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": false, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   zero_enabled ................. True\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1048:print]   zero_optimization_stage ...... 2\n",
      "[2022-07-02 03:14:59,264] [INFO] [config.py:1050:print]   json = {\n",
      "    \"train_batch_size\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+07, \n",
      "        \"reduce_bucket_size\": 5.000000e+07, \n",
      "        \"cpu_offload\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.00015, \n",
      "            \"max_grad_norm\": 0.0, \n",
      "            \"betas\": [0.9, 0.95]\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"zero_allow_untested_optimizer\": false\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000499725341796875 seconds\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT2 ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.001022 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT2 datasets ...\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      "done with setups ...\n",
      "time (ms) | model and optimizer: 41095.73 | train/valid/test data iterators: 1955.25\n",
      "training ...\n",
      "[2022-07-02 03:15:34,792] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.45 | optimizer_gradients: 278.44 | optimizer_step: 23013.73\n",
      "[2022-07-02 03:15:34,793] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 3836.74 | backward_microstep: 6353.38 | backward_inner_microstep: 6313.55 | backward_allreduce_microstep: 39.71 | step_microstep: 23351.37\n",
      "[2022-07-02 03:15:34,793] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 3836.83 | backward: 6353.38 | backward_inner: 6313.56 | backward_allreduce: 39.72 | step: 23351.37\n",
      "[2022-07-02 03:15:50,604] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.38 | optimizer_gradients: 265.85 | optimizer_step: 7381.91\n",
      "[2022-07-02 03:15:50,605] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1748.88 | backward_microstep: 6353.43 | backward_inner_microstep: 6314.90 | backward_allreduce_microstep: 38.44 | step_microstep: 7706.26\n",
      "[2022-07-02 03:15:50,605] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1748.96 | backward: 6353.43 | backward_inner: 6314.91 | backward_allreduce: 38.44 | step: 7706.28\n",
      "[2022-07-02 03:16:06,395] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 263.63 | optimizer_step: 7366.34\n",
      "[2022-07-02 03:16:06,396] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1753.33 | backward_microstep: 6346.41 | backward_inner_microstep: 6307.94 | backward_allreduce_microstep: 38.38 | step_microstep: 7688.51\n",
      "[2022-07-02 03:16:06,396] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1753.42 | backward: 6346.41 | backward_inner: 6307.95 | backward_allreduce: 38.38 | step: 7688.52\n",
      "[2022-07-02 03:16:22,208] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.40 | optimizer_gradients: 265.00 | optimizer_step: 7376.84\n",
      "[2022-07-02 03:16:22,208] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.22 | backward_microstep: 6353.09 | backward_inner_microstep: 6314.65 | backward_allreduce_microstep: 38.34 | step_microstep: 7700.49\n",
      "[2022-07-02 03:16:22,209] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.30 | backward: 6353.09 | backward_inner: 6314.66 | backward_allreduce: 38.35 | step: 7700.49\n",
      "[2022-07-02 03:16:38,029] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 264.43 | optimizer_step: 7367.74\n",
      "[2022-07-02 03:16:38,030] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.96 | backward_microstep: 6369.04 | backward_inner_microstep: 6330.65 | backward_allreduce_microstep: 38.30 | step_microstep: 7690.79\n",
      "[2022-07-02 03:16:38,030] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.03 | backward: 6369.04 | backward_inner: 6330.65 | backward_allreduce: 38.30 | step: 7690.79\n",
      "[2022-07-02 03:16:53,840] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.32 | optimizer_gradients: 266.95 | optimizer_step: 7373.78\n",
      "[2022-07-02 03:16:53,841] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.28 | backward_microstep: 6353.14 | backward_inner_microstep: 6314.71 | backward_allreduce_microstep: 38.33 | step_microstep: 7699.23\n",
      "[2022-07-02 03:16:53,841] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.35 | backward: 6353.13 | backward_inner: 6314.72 | backward_allreduce: 38.34 | step: 7699.24\n",
      "[2022-07-02 03:17:09,701] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 273.70 | optimizer_step: 7394.42\n",
      "[2022-07-02 03:17:09,701] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1761.35 | backward_microstep: 6369.49 | backward_inner_microstep: 6331.03 | backward_allreduce_microstep: 38.37 | step_microstep: 7726.73\n",
      "[2022-07-02 03:17:09,701] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1761.41 | backward: 6369.49 | backward_inner: 6331.04 | backward_allreduce: 38.37 | step: 7726.73\n",
      "[2022-07-02 03:17:25,532] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 269.16 | optimizer_step: 7370.85\n",
      "[2022-07-02 03:17:25,533] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.24 | backward_microstep: 6370.89 | backward_inner_microstep: 6332.01 | backward_allreduce_microstep: 38.78 | step_microstep: 7698.55\n",
      "[2022-07-02 03:17:25,533] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.32 | backward: 6370.89 | backward_inner: 6332.02 | backward_allreduce: 38.79 | step: 7698.55\n",
      "[2022-07-02 03:17:41,387] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 263.75 | optimizer_step: 7392.60\n",
      "[2022-07-02 03:17:41,387] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1760.02 | backward_microstep: 6376.48 | backward_inner_microstep: 6338.10 | backward_allreduce_microstep: 38.29 | step_microstep: 7714.89\n",
      "[2022-07-02 03:17:41,387] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1760.10 | backward: 6376.48 | backward_inner: 6338.11 | backward_allreduce: 38.29 | step: 7714.89\n",
      "[2022-07-02 03:17:57,220] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 268.00 | optimizer_step: 7375.97\n",
      "[2022-07-02 03:17:57,220] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:17:57,221] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.25276723364655274\n",
      "[2022-07-02 03:17:57,221] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.35 | backward_microstep: 6369.24 | backward_inner_microstep: 6330.79 | backward_allreduce_microstep: 38.36 | step_microstep: 7702.81\n",
      "[2022-07-02 03:17:57,221] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.43 | backward: 6369.24 | backward_inner: 6330.80 | backward_allreduce: 38.36 | step: 7702.82\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 17599.6 | learning rate: 4.687E-07 | lm loss: 1.082407E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "after 10 iterations memory (MB) | allocated: 10016.89111328125 | max allocated: 13945.46484375 | reserved: 29776.0 | max reserved: 29776.0\n",
      "time (ms) | forward: 1969.29 | backward: 6361.55 | backward-backward: 6361.52 | backward-allreduce: 0.00 | optimizer: 9268.23 | batch generator: 3.27\n",
      "Effective Tera Flops per GPU: 0.47 and total parameters 2.522 B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 03:18:13,045] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 266.75 | optimizer_step: 7368.17\n",
      "[2022-07-02 03:18:13,045] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.15 | backward_microstep: 6368.03 | backward_inner_microstep: 6327.11 | backward_allreduce_microstep: 40.83 | step_microstep: 7693.52\n",
      "[2022-07-02 03:18:13,046] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.22 | backward: 6368.03 | backward_inner: 6327.12 | backward_allreduce: 40.83 | step: 7693.53\n",
      "[2022-07-02 03:18:28,862] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 266.73 | optimizer_step: 7355.07\n",
      "[2022-07-02 03:18:28,863] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.44 | backward_microstep: 6373.52 | backward_inner_microstep: 6334.81 | backward_allreduce_microstep: 38.63 | step_microstep: 7680.41\n",
      "[2022-07-02 03:18:28,863] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.50 | backward: 6373.52 | backward_inner: 6334.81 | backward_allreduce: 38.63 | step: 7680.41\n",
      "[2022-07-02 03:18:44,684] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 266.02 | optimizer_step: 7359.37\n",
      "[2022-07-02 03:18:44,685] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.72 | backward_microstep: 6375.51 | backward_inner_microstep: 6335.48 | backward_allreduce_microstep: 39.94 | step_microstep: 7683.97\n",
      "[2022-07-02 03:18:44,685] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.78 | backward: 6375.51 | backward_inner: 6335.48 | backward_allreduce: 39.94 | step: 7683.97\n",
      "[2022-07-02 03:19:00,519] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 267.68 | optimizer_step: 7361.26\n",
      "[2022-07-02 03:19:00,520] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1761.46 | backward_microstep: 6382.67 | backward_inner_microstep: 6344.22 | backward_allreduce_microstep: 38.36 | step_microstep: 7687.60\n",
      "[2022-07-02 03:19:00,520] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1761.54 | backward: 6382.67 | backward_inner: 6344.23 | backward_allreduce: 38.37 | step: 7687.60\n",
      "[2022-07-02 03:19:16,373] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 267.31 | optimizer_step: 7375.69\n",
      "[2022-07-02 03:19:16,373] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1762.55 | backward_microstep: 6386.89 | backward_inner_microstep: 6348.50 | backward_allreduce_microstep: 38.31 | step_microstep: 7701.62\n",
      "[2022-07-02 03:19:16,374] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1762.63 | backward: 6386.89 | backward_inner: 6348.50 | backward_allreduce: 38.31 | step: 7701.62\n",
      "[2022-07-02 03:19:32,211] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 265.12 | optimizer_step: 7376.25\n",
      "[2022-07-02 03:19:32,212] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1762.21 | backward_microstep: 6373.23 | backward_inner_microstep: 6334.87 | backward_allreduce_microstep: 38.27 | step_microstep: 7699.83\n",
      "[2022-07-02 03:19:32,212] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1762.29 | backward: 6373.23 | backward_inner: 6334.87 | backward_allreduce: 38.28 | step: 7699.83\n",
      "[2022-07-02 03:19:48,066] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 268.25 | optimizer_step: 7390.15\n",
      "[2022-07-02 03:19:48,067] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.89 | backward_microstep: 6377.51 | backward_inner_microstep: 6339.11 | backward_allreduce_microstep: 38.32 | step_microstep: 7717.02\n",
      "[2022-07-02 03:19:48,067] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.97 | backward: 6377.51 | backward_inner: 6339.11 | backward_allreduce: 38.32 | step: 7717.02\n",
      "[2022-07-02 03:20:03,874] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 265.20 | optimizer_step: 7362.29\n",
      "[2022-07-02 03:20:03,875] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.56 | backward_microstep: 6363.92 | backward_inner_microstep: 6325.52 | backward_allreduce_microstep: 38.30 | step_microstep: 7686.00\n",
      "[2022-07-02 03:20:03,875] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.65 | backward: 6363.92 | backward_inner: 6325.53 | backward_allreduce: 38.31 | step: 7686.01\n",
      "[2022-07-02 03:20:19,723] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 267.83 | optimizer_step: 7387.56\n",
      "[2022-07-02 03:20:19,724] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.02 | backward_microstep: 6376.20 | backward_inner_microstep: 6337.77 | backward_allreduce_microstep: 38.35 | step_microstep: 7714.02\n",
      "[2022-07-02 03:20:19,724] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.09 | backward: 6376.20 | backward_inner: 6337.78 | backward_allreduce: 38.35 | step: 7714.02\n",
      "[2022-07-02 03:20:35,552] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 268.24 | optimizer_step: 7380.21\n",
      "[2022-07-02 03:20:35,552] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:20:35,553] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.2527159791505918\n",
      "[2022-07-02 03:20:35,553] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.62 | backward_microstep: 6362.06 | backward_inner_microstep: 6323.61 | backward_allreduce_microstep: 38.36 | step_microstep: 7707.22\n",
      "[2022-07-02 03:20:35,553] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.69 | backward: 6362.06 | backward_inner: 6323.61 | backward_allreduce: 38.36 | step: 7707.23\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 15833.2 | learning rate: 9.375E-07 | lm loss: 9.506392E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1760.84 | backward: 6374.05 | backward-backward: 6374.01 | backward-allreduce: 0.00 | optimizer: 7697.40 | batch generator: 1.30\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "[2022-07-02 03:20:51,389] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 267.23 | optimizer_step: 7377.40\n",
      "[2022-07-02 03:20:51,390] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.06 | backward_microstep: 6372.45 | backward_inner_microstep: 6333.93 | backward_allreduce_microstep: 38.42 | step_microstep: 7703.31\n",
      "[2022-07-02 03:20:51,390] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.12 | backward: 6372.45 | backward_inner: 6333.94 | backward_allreduce: 38.43 | step: 7703.31\n",
      "[2022-07-02 03:21:07,229] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 269.42 | optimizer_step: 7387.72\n",
      "[2022-07-02 03:21:07,230] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.82 | backward_microstep: 6362.90 | backward_inner_microstep: 6324.32 | backward_allreduce_microstep: 38.48 | step_microstep: 7715.66\n",
      "[2022-07-02 03:21:07,230] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.88 | backward: 6362.90 | backward_inner: 6324.33 | backward_allreduce: 38.49 | step: 7715.66\n",
      "[2022-07-02 03:21:23,097] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 267.13 | optimizer_step: 7403.36\n",
      "[2022-07-02 03:21:23,098] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.72 | backward_microstep: 6376.64 | backward_inner_microstep: 6337.74 | backward_allreduce_microstep: 38.81 | step_microstep: 7729.14\n",
      "[2022-07-02 03:21:23,098] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.78 | backward: 6376.64 | backward_inner: 6337.75 | backward_allreduce: 38.80 | step: 7729.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 03:21:38,925] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 264.91 | optimizer_step: 7361.74\n",
      "[2022-07-02 03:21:38,926] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1761.91 | backward_microstep: 6377.65 | backward_inner_microstep: 6339.18 | backward_allreduce_microstep: 38.38 | step_microstep: 7685.17\n",
      "[2022-07-02 03:21:38,926] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1762.00 | backward: 6377.65 | backward_inner: 6339.19 | backward_allreduce: 38.39 | step: 7685.17\n",
      "[2022-07-02 03:21:54,750] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 268.84 | optimizer_step: 7367.99\n",
      "[2022-07-02 03:21:54,751] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.15 | backward_microstep: 6369.27 | backward_inner_microstep: 6330.28 | backward_allreduce_microstep: 38.91 | step_microstep: 7695.51\n",
      "[2022-07-02 03:21:54,751] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.22 | backward: 6369.27 | backward_inner: 6330.28 | backward_allreduce: 38.91 | step: 7695.51\n",
      "[2022-07-02 03:22:10,624] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 270.84 | optimizer_step: 7396.81\n",
      "[2022-07-02 03:22:10,625] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.96 | backward_microstep: 6385.67 | backward_inner_microstep: 6347.12 | backward_allreduce_microstep: 38.46 | step_microstep: 7726.21\n",
      "[2022-07-02 03:22:10,625] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.04 | backward: 6385.67 | backward_inner: 6347.13 | backward_allreduce: 38.46 | step: 7726.21\n",
      "[2022-07-02 03:22:26,463] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 270.88 | optimizer_step: 7386.50\n",
      "[2022-07-02 03:22:26,463] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.93 | backward_microstep: 6364.58 | backward_inner_microstep: 6326.03 | backward_allreduce_microstep: 38.45 | step_microstep: 7715.93\n",
      "[2022-07-02 03:22:26,463] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.99 | backward: 6364.58 | backward_inner: 6326.04 | backward_allreduce: 38.45 | step: 7715.93\n",
      "[2022-07-02 03:22:42,259] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 265.65 | optimizer_step: 7350.16\n",
      "[2022-07-02 03:22:42,260] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.22 | backward_microstep: 6363.88 | backward_inner_microstep: 6325.47 | backward_allreduce_microstep: 38.33 | step_microstep: 7674.47\n",
      "[2022-07-02 03:22:42,260] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.28 | backward: 6363.88 | backward_inner: 6325.48 | backward_allreduce: 38.33 | step: 7674.48\n",
      "[2022-07-02 03:22:58,068] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 266.59 | optimizer_step: 7357.46\n",
      "[2022-07-02 03:22:58,068] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.17 | backward_microstep: 6368.47 | backward_inner_microstep: 6329.99 | backward_allreduce_microstep: 38.39 | step_microstep: 7682.71\n",
      "[2022-07-02 03:22:58,068] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.25 | backward: 6368.47 | backward_inner: 6330.00 | backward_allreduce: 38.39 | step: 7682.71\n",
      "[2022-07-02 03:23:13,884] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 264.28 | optimizer_step: 7369.77\n",
      "[2022-07-02 03:23:13,885] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:23:13,885] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.25270009174814145\n",
      "[2022-07-02 03:23:13,885] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.54 | backward_microstep: 6365.42 | backward_inner_microstep: 6326.94 | backward_allreduce_microstep: 38.39 | step_microstep: 7692.86\n",
      "[2022-07-02 03:23:13,885] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.60 | backward: 6365.42 | backward_inner: 6326.95 | backward_allreduce: 38.39 | step: 7692.86\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 15833.2 | learning rate: 1.406E-06 | lm loss: 9.063718E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1759.29 | backward: 6370.79 | backward-backward: 6370.75 | backward-allreduce: 0.00 | optimizer: 7702.35 | batch generator: 1.24\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "[2022-07-02 03:23:29,716] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 266.64 | optimizer_step: 7361.87\n",
      "[2022-07-02 03:23:29,717] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.07 | backward_microstep: 6381.74 | backward_inner_microstep: 6343.28 | backward_allreduce_microstep: 38.37 | step_microstep: 7687.02\n",
      "[2022-07-02 03:23:29,717] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.14 | backward: 6381.74 | backward_inner: 6343.29 | backward_allreduce: 38.37 | step: 7687.02\n",
      "[2022-07-02 03:23:45,565] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 270.74 | optimizer_step: 7378.04\n",
      "[2022-07-02 03:23:45,566] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.81 | backward_microstep: 6380.14 | backward_inner_microstep: 6341.67 | backward_allreduce_microstep: 38.38 | step_microstep: 7707.22\n",
      "[2022-07-02 03:23:45,566] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.88 | backward: 6380.14 | backward_inner: 6341.68 | backward_allreduce: 38.39 | step: 7707.24\n",
      "[2022-07-02 03:24:01,410] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 265.78 | optimizer_step: 7389.75\n",
      "[2022-07-02 03:24:01,411] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1752.11 | backward_microstep: 6376.10 | backward_inner_microstep: 6337.73 | backward_allreduce_microstep: 38.28 | step_microstep: 7714.02\n",
      "[2022-07-02 03:24:01,411] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1752.19 | backward: 6376.10 | backward_inner: 6337.74 | backward_allreduce: 38.28 | step: 7714.03\n",
      "[2022-07-02 03:24:17,259] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 265.47 | optimizer_step: 7395.63\n",
      "[2022-07-02 03:24:17,260] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.84 | backward_microstep: 6370.83 | backward_inner_microstep: 6332.45 | backward_allreduce_microstep: 38.29 | step_microstep: 7719.68\n",
      "[2022-07-02 03:24:17,260] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.92 | backward: 6370.83 | backward_inner: 6332.45 | backward_allreduce: 38.29 | step: 7719.68\n",
      "[2022-07-02 03:24:33,117] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.32 | optimizer_gradients: 268.49 | optimizer_step: 7394.85\n",
      "[2022-07-02 03:24:33,117] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.16 | backward_microstep: 6374.13 | backward_inner_microstep: 6335.58 | backward_allreduce_microstep: 38.45 | step_microstep: 7721.83\n",
      "[2022-07-02 03:24:33,117] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.25 | backward: 6374.12 | backward_inner: 6335.59 | backward_allreduce: 38.45 | step: 7721.83\n",
      "[2022-07-02 03:24:48,970] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.39 | optimizer_gradients: 266.11 | optimizer_step: 7403.37\n",
      "[2022-07-02 03:24:48,971] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.28 | backward_microstep: 6367.32 | backward_inner_microstep: 6328.67 | backward_allreduce_microstep: 38.56 | step_microstep: 7728.04\n",
      "[2022-07-02 03:24:48,971] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.35 | backward: 6367.32 | backward_inner: 6328.68 | backward_allreduce: 38.56 | step: 7728.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 03:25:04,828] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 266.55 | optimizer_step: 7398.14\n",
      "[2022-07-02 03:25:04,829] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.12 | backward_microstep: 6374.27 | backward_inner_microstep: 6335.79 | backward_allreduce_microstep: 38.39 | step_microstep: 7723.30\n",
      "[2022-07-02 03:25:04,829] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.21 | backward: 6374.27 | backward_inner: 6335.80 | backward_allreduce: 38.39 | step: 7723.30\n",
      "[2022-07-02 03:25:20,726] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 267.64 | optimizer_step: 7447.88\n",
      "[2022-07-02 03:25:20,726] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.63 | backward_microstep: 6363.83 | backward_inner_microstep: 6325.35 | backward_allreduce_microstep: 38.37 | step_microstep: 7774.18\n",
      "[2022-07-02 03:25:20,726] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.70 | backward: 6363.83 | backward_inner: 6325.36 | backward_allreduce: 38.38 | step: 7774.18\n",
      "[2022-07-02 03:25:36,576] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 267.38 | optimizer_step: 7386.35\n",
      "[2022-07-02 03:25:36,576] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.21 | backward_microstep: 6376.30 | backward_inner_microstep: 6337.91 | backward_allreduce_microstep: 38.30 | step_microstep: 7712.38\n",
      "[2022-07-02 03:25:36,576] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.29 | backward: 6376.30 | backward_inner: 6337.92 | backward_allreduce: 38.30 | step: 7712.40\n",
      "[2022-07-02 03:25:52,435] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 270.73 | optimizer_step: 7394.21\n",
      "[2022-07-02 03:25:52,436] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:25:52,436] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.25260113402464923\n",
      "[2022-07-02 03:25:52,436] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.35 | backward_microstep: 6375.36 | backward_inner_microstep: 6336.93 | backward_allreduce_microstep: 38.34 | step_microstep: 7723.68\n",
      "[2022-07-02 03:25:52,436] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.42 | backward: 6375.36 | backward_inner: 6336.93 | backward_allreduce: 38.35 | step: 7723.69\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 15855.1 | learning rate: 1.875E-06 | lm loss: 8.885904E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1758.77 | backward: 6374.09 | backward-backward: 6374.06 | backward-allreduce: 0.00 | optimizer: 7721.39 | batch generator: 1.30\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "[2022-07-02 03:26:08,267] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 265.34 | optimizer_step: 7378.39\n",
      "[2022-07-02 03:26:08,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.06 | backward_microstep: 6368.26 | backward_inner_microstep: 6329.79 | backward_allreduce_microstep: 38.38 | step_microstep: 7702.36\n",
      "[2022-07-02 03:26:08,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.15 | backward: 6368.26 | backward_inner: 6329.80 | backward_allreduce: 38.39 | step: 7702.36\n",
      "[2022-07-02 03:26:24,096] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.33 | optimizer_gradients: 265.21 | optimizer_step: 7389.60\n",
      "[2022-07-02 03:26:24,096] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.57 | backward_microstep: 6357.68 | backward_inner_microstep: 6319.32 | backward_allreduce_microstep: 38.27 | step_microstep: 7713.30\n",
      "[2022-07-02 03:26:24,096] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.64 | backward: 6357.68 | backward_inner: 6319.33 | backward_allreduce: 38.27 | step: 7713.30\n",
      "[2022-07-02 03:26:39,961] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.36 | optimizer_gradients: 270.45 | optimizer_step: 7400.00\n",
      "[2022-07-02 03:26:39,961] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1758.92 | backward_microstep: 6374.22 | backward_inner_microstep: 6335.85 | backward_allreduce_microstep: 38.28 | step_microstep: 7729.00\n",
      "[2022-07-02 03:26:39,961] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1758.98 | backward: 6374.22 | backward_inner: 6335.86 | backward_allreduce: 38.28 | step: 7729.00\n",
      "[2022-07-02 03:26:55,782] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 266.92 | optimizer_step: 7362.99\n",
      "[2022-07-02 03:26:55,783] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.30 | backward_microstep: 6373.96 | backward_inner_microstep: 6335.60 | backward_allreduce_microstep: 38.27 | step_microstep: 7688.71\n",
      "[2022-07-02 03:26:55,783] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.37 | backward: 6373.96 | backward_inner: 6335.60 | backward_allreduce: 38.28 | step: 7688.71\n",
      "[2022-07-02 03:27:11,626] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.40 | optimizer_gradients: 266.81 | optimizer_step: 7384.01\n",
      "[2022-07-02 03:27:11,627] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.83 | backward_microstep: 6375.59 | backward_inner_microstep: 6337.15 | backward_allreduce_microstep: 38.35 | step_microstep: 7709.38\n",
      "[2022-07-02 03:27:11,627] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1755.89 | backward: 6375.59 | backward_inner: 6337.16 | backward_allreduce: 38.36 | step: 7709.39\n",
      "[2022-07-02 03:27:27,473] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.35 | optimizer_gradients: 268.42 | optimizer_step: 7378.09\n",
      "[2022-07-02 03:27:27,474] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1759.33 | backward_microstep: 6379.64 | backward_inner_microstep: 6341.27 | backward_allreduce_microstep: 38.28 | step_microstep: 7705.05\n",
      "[2022-07-02 03:27:27,474] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1759.40 | backward: 6379.65 | backward_inner: 6341.28 | backward_allreduce: 38.28 | step: 7705.05\n",
      "[2022-07-02 03:27:43,315] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 271.58 | optimizer_step: 7371.01\n",
      "[2022-07-02 03:27:43,316] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1757.44 | backward_microstep: 6380.71 | backward_inner_microstep: 6342.27 | backward_allreduce_microstep: 38.36 | step_microstep: 7701.07\n",
      "[2022-07-02 03:27:43,316] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1757.50 | backward: 6380.71 | backward_inner: 6342.27 | backward_allreduce: 38.37 | step: 7701.07\n",
      "[2022-07-02 03:27:59,183] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.32 | optimizer_gradients: 272.28 | optimizer_step: 7398.80\n",
      "[2022-07-02 03:27:59,183] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1756.49 | backward_microstep: 6378.11 | backward_inner_microstep: 6339.06 | backward_allreduce_microstep: 38.95 | step_microstep: 7729.62\n",
      "[2022-07-02 03:27:59,183] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.57 | backward: 6378.11 | backward_inner: 6339.06 | backward_allreduce: 38.96 | step: 7729.63\n",
      "[2022-07-02 03:28:15,053] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.37 | optimizer_gradients: 270.62 | optimizer_step: 7413.37\n",
      "[2022-07-02 03:28:15,054] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1755.95 | backward_microstep: 6368.67 | backward_inner_microstep: 6330.32 | backward_allreduce_microstep: 38.26 | step_microstep: 7742.65\n",
      "[2022-07-02 03:28:15,054] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1756.02 | backward: 6368.67 | backward_inner: 6330.32 | backward_allreduce: 38.27 | step: 7742.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 03:28:30,903] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 52.34 | optimizer_gradients: 268.86 | optimizer_step: 7403.55\n",
      "[2022-07-02 03:28:30,904] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:28:30,904] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.2525706111582965\n",
      "[2022-07-02 03:28:30,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1754.24 | backward_microstep: 6361.98 | backward_inner_microstep: 6323.60 | backward_allreduce_microstep: 38.30 | step_microstep: 7731.42\n",
      "[2022-07-02 03:28:30,904] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1754.32 | backward: 6361.98 | backward_inner: 6323.61 | backward_allreduce: 38.30 | step: 7731.43\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 15846.9 | learning rate: 2.344E-06 | lm loss: 8.758082E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1758.53 | backward: 6371.98 | backward-backward: 6371.94 | backward-allreduce: 0.00 | optimizer: 7715.52 | batch generator: 1.22\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 2.522 B\n",
      "rank: 0 | time: 2022-07-02 03:28:30 | exiting the program at iteration 50\n",
      "[2022-07-02 03:28:37,072] [INFO] [launch.py:159:main] Process 31681 exits successfully.\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-infinity-cpu.sh 48 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-infinity_l-48_hs-2048_bs-4_ws-4_2022-07-02.1656732519.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 03:28:40,636] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-07-02 03:28:42,258] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-07-02 03:28:43,120] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-07-02 03:28:43,121] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-07-02 03:28:43,121] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-07-02 03:28:43,121] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 48\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 3\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-07-02 03:28:46,799] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-07-02 03:28:46,799] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-07-02 03:28:46,800] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.05 GB, percent = 3.4%\n",
      "[2022-07-02 03:28:46,800] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f68981da310>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-07-02 03:28:54,257] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-07-02 03:28:54,258] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.38 GB         CA 0.39 GB         Max_CA 0 GB \n",
      "[2022-07-02 03:28:54,259] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 12.71 GB, percent = 14.1%\n",
      " > number of parameters on model parallel rank 0            2.522 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-07-02 03:28:54,263] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f68981da310>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-07-02 03:28:54,277] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-07-02 03:28:54,277] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-07-02 03:28:54,278] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-07-02 03:28:54,278] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-07-02 03:28:54,281] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-07-02 03:28:54,281] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-07-02 03:28:54,281] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-07-02 03:28:54,331] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-07-02 03:28:54,332] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-07-02 03:28:54,332] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "Initializing ZeRO Stage 3\n",
      "[2022-07-02 03:28:54,337] [INFO] [stage3.py:639:__init__] Reduce bucket size 90000000\n",
      "[2022-07-02 03:28:54,337] [INFO] [stage3.py:640:__init__] Allgather bucket size 50000000.0\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.2228085994720459 seconds\n",
      "[2022-07-02 03:29:17,019] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 14507.76\n",
      "[2022-07-02 03:29:17,034] [INFO] [stage3.py:811:__init__] optimizer state initialized\n",
      "[2022-07-02 03:29:18,136] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2022-07-02 03:29:18,137] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-07-02 03:29:18,137] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f68970ac4c0>\n",
      "[2022-07-02 03:29:18,137] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:29:18,139] [INFO] [config.py:1044:print] DeepSpeedEngine configuration:\n",
      "[2022-07-02 03:29:18,139] [INFO] [config.py:1048:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-07-02 03:29:18,139] [INFO] [config.py:1048:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-07-02 03:29:18,139] [INFO] [config.py:1048:print]   allreduce_always_fp32 ........ False\n",
      "[2022-07-02 03:29:18,139] [INFO] [config.py:1048:print]   amp_enabled .................. False\n",
      "[2022-07-02 03:29:18,139] [INFO] [config.py:1048:print]   amp_params ................... False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   bfloat16_enabled ............. False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   curriculum_enabled ........... False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   curriculum_params ............ False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   dataloader_drop_last ......... False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   disable_allgather ............ False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   dump_state ................... False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_enabled ........... False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   eigenvalue_verbose ........... False\n",
      "[2022-07-02 03:29:18,140] [INFO] [config.py:1048:print]   elasticity_enabled ........... False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   fp16_enabled ................. False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   fp16_mixed_quantize .......... False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   global_rank .................. 0\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   gradient_accumulation_steps .. 1\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   gradient_clipping ............ 1.0\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   loss_scale ................... 0\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   memory_breakdown ............. False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   optimizer_name ............... None\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   optimizer_params ............. None\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   pld_enabled .................. False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   pld_params ................... False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   prescale_gradients ........... False\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_change_rate ......... 0.001\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_groups .............. 1\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_offset .............. 1000\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_period .............. 1000\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_rounding ............ 0\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_start_bits .......... 16\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_target_bits ......... 8\n",
      "[2022-07-02 03:29:18,141] [INFO] [config.py:1048:print]   quantize_training_enabled .... False\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   quantize_type ................ 0\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   quantize_verbose ............. False\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   scheduler_name ............... None\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   scheduler_params ............. None\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   sparse_attention ............. None\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   sparse_gradients_enabled ..... False\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   steps_per_print .............. 10\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   tensorboard_enabled .......... False\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   tensorboard_output_path ...... \n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   train_batch_size ............. 4\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   train_micro_batch_size_per_gpu  4\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   use_quantizer_kernel ......... False\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   wall_clock_breakdown ......... True\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   world_size ................... 1\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   zero_allow_untested_optimizer  False\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   zero_config .................. {\n",
      "    \"stage\": 3, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 9.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 5, \n",
      "        \"buffer_size\": 1.000000e+08, \n",
      "        \"max_in_cpu\": 1, \n",
      "        \"pin_memory\": true\n",
      "    }, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": true, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false, \n",
      "        \"pipeline\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+08, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   zero_enabled ................. True\n",
      "[2022-07-02 03:29:18,142] [INFO] [config.py:1048:print]   zero_optimization_stage ...... 3\n",
      "[2022-07-02 03:29:18,143] [INFO] [config.py:1050:print]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_param_persitence_threshold\": 1.000000e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 5.000000e+07, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_bucket_size\": 9.000000e+07, \n",
      "        \"sub_group_size\": 1.000000e+08, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"buffer_count\": 4, \n",
      "            \"pipeline_read\": false, \n",
      "            \"pipeline_write\": false, \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"max_in_cpu\": 1, \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"zero_allow_untested_optimizer\": false, \n",
      "    \"aio\": {\n",
      "        \"block_size\": 1.048576e+06, \n",
      "        \"queue_depth\": 16, \n",
      "        \"single_submit\": false, \n",
      "        \"overlap_events\": true, \n",
      "        \"thread_count\": 2\n",
      "    }\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005762577056884766 seconds\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT2 ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000468 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT2 datasets ...\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      "done with setups ...\n",
      "time (ms) | model and optimizer: 31389.85 | train/valid/test data iterators: 1646.08\n",
      "training ...\n",
      "[2022-07-02 03:29:52,300] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 21887.40\n",
      "[2022-07-02 03:29:52,301] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 3193.45 | backward_microstep: 7369.04 | backward_inner_microstep: 7287.21 | backward_allreduce_microstep: 81.67 | step_microstep: 21933.52\n",
      "[2022-07-02 03:29:52,301] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 3193.46 | backward: 7369.04 | backward_inner: 7287.22 | backward_allreduce: 81.70 | step: 21933.52\n",
      "[2022-07-02 03:30:08,517] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6178.83\n",
      "[2022-07-02 03:30:08,518] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2578.25 | backward_microstep: 7412.17 | backward_inner_microstep: 7334.73 | backward_allreduce_microstep: 77.32 | step_microstep: 6222.27\n",
      "[2022-07-02 03:30:08,518] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2578.27 | backward: 7412.18 | backward_inner: 7334.74 | backward_allreduce: 77.35 | step: 6222.27\n",
      "[2022-07-02 03:30:24,950] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6234.44\n",
      "[2022-07-02 03:30:24,951] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2611.59 | backward_microstep: 7536.98 | backward_inner_microstep: 7460.10 | backward_allreduce_microstep: 76.77 | step_microstep: 6279.77\n",
      "[2022-07-02 03:30:24,951] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2611.62 | backward: 7536.98 | backward_inner: 7460.11 | backward_allreduce: 76.78 | step: 6279.77\n",
      "[2022-07-02 03:30:41,343] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6159.99\n",
      "[2022-07-02 03:30:41,344] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2647.85 | backward_microstep: 7535.15 | backward_inner_microstep: 7458.15 | backward_allreduce_microstep: 76.87 | step_microstep: 6205.13\n",
      "[2022-07-02 03:30:41,345] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2647.86 | backward: 7535.15 | backward_inner: 7458.17 | backward_allreduce: 76.90 | step: 6205.13\n",
      "[2022-07-02 03:30:57,724] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6180.07\n",
      "[2022-07-02 03:30:57,725] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2631.46 | backward_microstep: 7519.75 | backward_inner_microstep: 7442.38 | backward_allreduce_microstep: 77.23 | step_microstep: 6223.96\n",
      "[2022-07-02 03:30:57,725] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2631.51 | backward: 7519.75 | backward_inner: 7442.39 | backward_allreduce: 77.27 | step: 6223.96\n",
      "[2022-07-02 03:31:14,070] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6156.05\n",
      "[2022-07-02 03:31:14,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2629.83 | backward_microstep: 7510.92 | backward_inner_microstep: 7434.07 | backward_allreduce_microstep: 76.73 | step_microstep: 6200.85\n",
      "[2022-07-02 03:31:14,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2629.85 | backward: 7510.92 | backward_inner: 7434.08 | backward_allreduce: 76.75 | step: 6200.85\n",
      "[2022-07-02 03:31:30,459] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6161.19\n",
      "[2022-07-02 03:31:30,460] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2639.14 | backward_microstep: 7540.56 | backward_inner_microstep: 7463.60 | backward_allreduce_microstep: 76.84 | step_microstep: 6204.52\n",
      "[2022-07-02 03:31:30,460] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2639.15 | backward: 7540.56 | backward_inner: 7463.61 | backward_allreduce: 76.86 | step: 6204.53\n",
      "[2022-07-02 03:31:46,824] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6156.21\n",
      "[2022-07-02 03:31:46,825] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2620.94 | backward_microstep: 7539.21 | backward_inner_microstep: 7462.23 | backward_allreduce_microstep: 76.81 | step_microstep: 6199.84\n",
      "[2022-07-02 03:31:46,825] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2620.99 | backward: 7539.19 | backward_inner: 7462.25 | backward_allreduce: 76.86 | step: 6199.84\n",
      "[2022-07-02 03:32:03,182] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6164.52\n",
      "[2022-07-02 03:32:03,183] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2614.74 | backward_microstep: 7528.99 | backward_inner_microstep: 7452.08 | backward_allreduce_microstep: 76.79 | step_microstep: 6209.97\n",
      "[2022-07-02 03:32:03,183] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2614.78 | backward: 7528.99 | backward_inner: 7452.09 | backward_allreduce: 76.80 | step: 6209.97\n",
      "[2022-07-02 03:32:19,526] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6153.35\n",
      "[2022-07-02 03:32:19,527] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:32:19,528] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.24432062168159668\n",
      "[2022-07-02 03:32:19,528] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2616.97 | backward_microstep: 7524.54 | backward_inner_microstep: 7447.48 | backward_allreduce_microstep: 76.94 | step_microstep: 6198.24\n",
      "[2022-07-02 03:32:19,528] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2617.01 | backward: 7524.54 | backward_inner: 7447.49 | backward_allreduce: 76.96 | step: 6198.25\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 17973.3 | learning rate: 4.687E-07 | lm loss: 1.126530E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "after 10 iterations memory (MB) | allocated: 792.986328125 | max allocated: 5485.412109375 | reserved: 8252.0 | max reserved: 8252.0\n",
      "time (ms) | forward: 2682.38 | backward: 7501.83 | backward-backward: 7501.80 | backward-allreduce: 0.00 | optimizer: 7788.09 | batch generator: 1.89\n",
      "Effective Tera Flops per GPU: 0.46 and total parameters 2.522 B\n",
      "[2022-07-02 03:32:35,867] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6146.23\n",
      "[2022-07-02 03:32:35,869] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2619.63 | backward_microstep: 7523.82 | backward_inner_microstep: 7446.81 | backward_allreduce_microstep: 76.87 | step_microstep: 6189.36\n",
      "[2022-07-02 03:32:35,869] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2619.67 | backward: 7523.82 | backward_inner: 7446.84 | backward_allreduce: 76.89 | step: 6189.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 03:32:52,208] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6144.94\n",
      "[2022-07-02 03:32:52,209] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2611.37 | backward_microstep: 7534.86 | backward_inner_microstep: 7457.65 | backward_allreduce_microstep: 77.04 | step_microstep: 6189.25\n",
      "[2022-07-02 03:32:52,209] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2611.43 | backward: 7534.85 | backward_inner: 7457.67 | backward_allreduce: 77.07 | step: 6189.25\n",
      "[2022-07-02 03:33:08,597] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6175.22\n",
      "[2022-07-02 03:33:08,599] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2633.82 | backward_microstep: 7531.74 | backward_inner_microstep: 7454.43 | backward_allreduce_microstep: 77.19 | step_microstep: 6219.24\n",
      "[2022-07-02 03:33:08,599] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2633.83 | backward: 7531.74 | backward_inner: 7454.45 | backward_allreduce: 77.21 | step: 6219.24\n",
      "[2022-07-02 03:33:24,982] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6185.17\n",
      "[2022-07-02 03:33:24,983] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2615.70 | backward_microstep: 7534.01 | backward_inner_microstep: 7456.98 | backward_allreduce_microstep: 76.87 | step_microstep: 6229.76\n",
      "[2022-07-02 03:33:24,983] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2615.72 | backward: 7534.02 | backward_inner: 7457.00 | backward_allreduce: 76.92 | step: 6229.76\n",
      "[2022-07-02 03:33:41,355] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6172.09\n",
      "[2022-07-02 03:33:41,356] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2622.13 | backward_microstep: 7527.96 | backward_inner_microstep: 7453.28 | backward_allreduce_microstep: 74.51 | step_microstep: 6218.27\n",
      "[2022-07-02 03:33:41,356] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2622.18 | backward: 7527.96 | backward_inner: 7453.30 | backward_allreduce: 74.55 | step: 6218.27\n",
      "[2022-07-02 03:33:57,726] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6156.83\n",
      "[2022-07-02 03:33:57,727] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2635.23 | backward_microstep: 7530.01 | backward_inner_microstep: 7452.48 | backward_allreduce_microstep: 77.42 | step_microstep: 6200.44\n",
      "[2022-07-02 03:33:57,727] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2635.24 | backward: 7530.01 | backward_inner: 7452.49 | backward_allreduce: 77.44 | step: 6200.46\n",
      "[2022-07-02 03:34:14,083] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6158.25\n",
      "[2022-07-02 03:34:14,084] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2616.60 | backward_microstep: 7533.14 | backward_inner_microstep: 7456.04 | backward_allreduce_microstep: 76.97 | step_microstep: 6203.01\n",
      "[2022-07-02 03:34:14,084] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2616.63 | backward: 7533.14 | backward_inner: 7456.05 | backward_allreduce: 77.00 | step: 6203.02\n",
      "[2022-07-02 03:34:30,425] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6152.84\n",
      "[2022-07-02 03:34:30,426] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2621.36 | backward_microstep: 7518.93 | backward_inner_microstep: 7440.69 | backward_allreduce_microstep: 78.09 | step_microstep: 6196.77\n",
      "[2022-07-02 03:34:30,426] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2621.38 | backward: 7518.93 | backward_inner: 7440.71 | backward_allreduce: 78.13 | step: 6196.77\n",
      "[2022-07-02 03:34:46,827] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6204.89\n",
      "[2022-07-02 03:34:46,828] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2619.72 | backward_microstep: 7528.41 | backward_inner_microstep: 7451.07 | backward_allreduce_microstep: 77.18 | step_microstep: 6249.46\n",
      "[2022-07-02 03:34:46,828] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2619.74 | backward: 7528.41 | backward_inner: 7451.10 | backward_allreduce: 77.22 | step: 6249.47\n",
      "[2022-07-02 03:35:03,206] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6187.47\n",
      "[2022-07-02 03:35:03,207] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:35:03,207] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.24439119016265134\n",
      "[2022-07-02 03:35:03,207] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2618.76 | backward_microstep: 7523.23 | backward_inner_microstep: 7445.98 | backward_allreduce_microstep: 77.14 | step_microstep: 6232.08\n",
      "[2022-07-02 03:35:03,207] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2618.80 | backward: 7523.23 | backward_inner: 7446.00 | backward_allreduce: 77.15 | step: 6232.09\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 16367.9 | learning rate: 9.375E-07 | lm loss: 1.122032E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2624.84 | backward: 7528.71 | backward-backward: 7528.68 | backward-allreduce: 0.00 | optimizer: 6213.03 | batch generator: 1.38\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "[2022-07-02 03:35:19,574] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6159.54\n",
      "[2022-07-02 03:35:19,575] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2614.92 | backward_microstep: 7541.15 | backward_inner_microstep: 7463.76 | backward_allreduce_microstep: 77.29 | step_microstep: 6204.37\n",
      "[2022-07-02 03:35:19,575] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2614.94 | backward: 7541.15 | backward_inner: 7463.77 | backward_allreduce: 77.30 | step: 6204.38\n",
      "[2022-07-02 03:35:35,984] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6189.12\n",
      "[2022-07-02 03:35:35,985] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2650.92 | backward_microstep: 7521.90 | backward_inner_microstep: 7443.97 | backward_allreduce_microstep: 77.81 | step_microstep: 6232.38\n",
      "[2022-07-02 03:35:35,985] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2650.94 | backward: 7521.90 | backward_inner: 7443.99 | backward_allreduce: 77.83 | step: 6232.39\n",
      "[2022-07-02 03:35:52,366] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6173.12\n",
      "[2022-07-02 03:35:52,367] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2628.68 | backward_microstep: 7531.57 | backward_inner_microstep: 7454.30 | backward_allreduce_microstep: 77.16 | step_microstep: 6217.41\n",
      "[2022-07-02 03:35:52,367] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2628.68 | backward: 7531.57 | backward_inner: 7454.31 | backward_allreduce: 77.17 | step: 6217.41\n",
      "[2022-07-02 03:36:08,776] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6179.33\n",
      "[2022-07-02 03:36:08,777] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2623.45 | backward_microstep: 7558.60 | backward_inner_microstep: 7481.12 | backward_allreduce_microstep: 77.31 | step_microstep: 6223.27\n",
      "[2022-07-02 03:36:08,778] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2623.47 | backward: 7558.59 | backward_inner: 7481.16 | backward_allreduce: 77.34 | step: 6223.28\n",
      "[2022-07-02 03:36:25,142] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6157.81\n",
      "[2022-07-02 03:36:25,144] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2635.23 | backward_microstep: 7523.89 | backward_inner_microstep: 7446.73 | backward_allreduce_microstep: 77.05 | step_microstep: 6202.37\n",
      "[2022-07-02 03:36:25,144] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2635.24 | backward: 7523.89 | backward_inner: 7446.75 | backward_allreduce: 77.06 | step: 6202.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 03:36:41,526] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6192.25\n",
      "[2022-07-02 03:36:41,527] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2620.18 | backward_microstep: 7522.49 | backward_inner_microstep: 7445.07 | backward_allreduce_microstep: 77.31 | step_microstep: 6236.01\n",
      "[2022-07-02 03:36:41,527] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2620.19 | backward: 7522.49 | backward_inner: 7445.09 | backward_allreduce: 77.32 | step: 6236.01\n",
      "[2022-07-02 03:36:57,888] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6161.02\n",
      "[2022-07-02 03:36:57,889] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2625.05 | backward_microstep: 7526.49 | backward_inner_microstep: 7449.16 | backward_allreduce_microstep: 77.20 | step_microstep: 6205.88\n",
      "[2022-07-02 03:36:57,889] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2625.10 | backward: 7526.49 | backward_inner: 7449.17 | backward_allreduce: 77.23 | step: 6205.89\n",
      "[2022-07-02 03:37:14,298] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6177.18\n",
      "[2022-07-02 03:37:14,299] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2655.38 | backward_microstep: 7529.35 | backward_inner_microstep: 7452.14 | backward_allreduce_microstep: 77.07 | step_microstep: 6220.37\n",
      "[2022-07-02 03:37:14,299] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2655.40 | backward: 7529.34 | backward_inner: 7452.15 | backward_allreduce: 77.10 | step: 6220.37\n",
      "[2022-07-02 03:37:30,639] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6146.46\n",
      "[2022-07-02 03:37:30,640] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2612.35 | backward_microstep: 7532.15 | backward_inner_microstep: 7454.77 | backward_allreduce_microstep: 77.26 | step_microstep: 6191.41\n",
      "[2022-07-02 03:37:30,640] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2612.36 | backward: 7532.15 | backward_inner: 7454.78 | backward_allreduce: 77.29 | step: 6191.41\n",
      "[2022-07-02 03:37:47,023] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6165.59\n",
      "[2022-07-02 03:37:47,024] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:37:47,024] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.24433732716460282\n",
      "[2022-07-02 03:37:47,025] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2625.83 | backward_microstep: 7543.44 | backward_inner_microstep: 7466.17 | backward_allreduce_microstep: 77.15 | step_microstep: 6210.80\n",
      "[2022-07-02 03:37:47,025] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2625.86 | backward: 7543.45 | backward_inner: 7466.18 | backward_allreduce: 77.17 | step: 6210.81\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 16381.8 | learning rate: 1.406E-06 | lm loss: 1.108166E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2632.59 | backward: 7533.20 | backward-backward: 7533.17 | backward-allreduce: 0.00 | optimizer: 6214.70 | batch generator: 1.39\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "[2022-07-02 03:38:03,428] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6162.78\n",
      "[2022-07-02 03:38:03,429] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2649.97 | backward_microstep: 7538.86 | backward_inner_microstep: 7461.53 | backward_allreduce_microstep: 77.17 | step_microstep: 6207.83\n",
      "[2022-07-02 03:38:03,429] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2649.98 | backward: 7538.86 | backward_inner: 7461.57 | backward_allreduce: 77.20 | step: 6207.84\n",
      "[2022-07-02 03:38:19,788] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6162.40\n",
      "[2022-07-02 03:38:19,789] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2620.56 | backward_microstep: 7527.93 | backward_inner_microstep: 7450.36 | backward_allreduce_microstep: 77.42 | step_microstep: 6206.90\n",
      "[2022-07-02 03:38:19,790] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2620.57 | backward: 7527.93 | backward_inner: 7450.37 | backward_allreduce: 77.46 | step: 6206.90\n",
      "[2022-07-02 03:38:36,154] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6151.02\n",
      "[2022-07-02 03:38:36,155] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2628.53 | backward_microstep: 7536.93 | backward_inner_microstep: 7459.42 | backward_allreduce_microstep: 77.34 | step_microstep: 6195.54\n",
      "[2022-07-02 03:38:36,155] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2628.56 | backward: 7536.93 | backward_inner: 7459.45 | backward_allreduce: 77.38 | step: 6195.54\n",
      "[2022-07-02 03:38:52,554] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6161.33\n",
      "[2022-07-02 03:38:52,555] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2643.20 | backward_microstep: 7547.32 | backward_inner_microstep: 7469.80 | backward_allreduce_microstep: 77.36 | step_microstep: 6204.79\n",
      "[2022-07-02 03:38:52,555] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2643.22 | backward: 7547.32 | backward_inner: 7469.83 | backward_allreduce: 77.39 | step: 6204.79\n",
      "[2022-07-02 03:39:08,927] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6170.35\n",
      "[2022-07-02 03:39:08,928] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2624.44 | backward_microstep: 7529.74 | backward_inner_microstep: 7452.02 | backward_allreduce_microstep: 77.59 | step_microstep: 6213.91\n",
      "[2022-07-02 03:39:08,928] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2624.45 | backward: 7529.74 | backward_inner: 7452.04 | backward_allreduce: 77.61 | step: 6213.91\n",
      "[2022-07-02 03:39:25,306] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6179.50\n",
      "[2022-07-02 03:39:25,307] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2626.22 | backward_microstep: 7523.77 | backward_inner_microstep: 7446.23 | backward_allreduce_microstep: 77.41 | step_microstep: 6224.16\n",
      "[2022-07-02 03:39:25,307] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2626.25 | backward: 7523.77 | backward_inner: 7446.25 | backward_allreduce: 77.43 | step: 6224.17\n",
      "[2022-07-02 03:39:41,692] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6158.67\n",
      "[2022-07-02 03:39:41,693] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2651.89 | backward_microstep: 7526.57 | backward_inner_microstep: 7449.22 | backward_allreduce_microstep: 77.23 | step_microstep: 6202.63\n",
      "[2022-07-02 03:39:41,693] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2651.96 | backward: 7526.57 | backward_inner: 7449.24 | backward_allreduce: 77.25 | step: 6202.63\n",
      "[2022-07-02 03:39:58,079] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6171.43\n",
      "[2022-07-02 03:39:58,080] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2620.78 | backward_microstep: 7545.64 | backward_inner_microstep: 7468.46 | backward_allreduce_microstep: 77.03 | step_microstep: 6216.22\n",
      "[2022-07-02 03:39:58,081] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2620.80 | backward: 7545.65 | backward_inner: 7468.47 | backward_allreduce: 77.07 | step: 6216.22\n",
      "[2022-07-02 03:40:14,452] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6158.44\n",
      "[2022-07-02 03:40:14,454] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2627.53 | backward_microstep: 7537.90 | backward_inner_microstep: 7459.78 | backward_allreduce_microstep: 78.00 | step_microstep: 6202.90\n",
      "[2022-07-02 03:40:14,454] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2627.57 | backward: 7537.90 | backward_inner: 7459.79 | backward_allreduce: 78.02 | step: 6202.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 03:40:30,827] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6135.75\n",
      "[2022-07-02 03:40:30,828] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:40:30,828] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.24431760335652652\n",
      "[2022-07-02 03:40:30,828] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2645.92 | backward_microstep: 7544.43 | backward_inner_microstep: 7465.60 | backward_allreduce_microstep: 78.71 | step_microstep: 6179.48\n",
      "[2022-07-02 03:40:30,828] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2645.94 | backward: 7544.43 | backward_inner: 7465.60 | backward_allreduce: 78.74 | step: 6179.48\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 16380.4 | learning rate: 1.875E-06 | lm loss: 1.087060E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2637.36 | backward: 7536.01 | backward-backward: 7535.97 | backward-allreduce: 0.00 | optimizer: 6205.71 | batch generator: 1.43\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "[2022-07-02 03:40:47,172] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6159.76\n",
      "[2022-07-02 03:40:47,173] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2616.74 | backward_microstep: 7518.41 | backward_inner_microstep: 7441.14 | backward_allreduce_microstep: 77.16 | step_microstep: 6203.17\n",
      "[2022-07-02 03:40:47,174] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2616.75 | backward: 7518.41 | backward_inner: 7441.15 | backward_allreduce: 77.17 | step: 6203.17\n",
      "[2022-07-02 03:41:03,565] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6182.82\n",
      "[2022-07-02 03:41:03,566] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2614.28 | backward_microstep: 7547.77 | backward_inner_microstep: 7470.35 | backward_allreduce_microstep: 77.30 | step_microstep: 6226.45\n",
      "[2022-07-02 03:41:03,567] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2614.29 | backward: 7547.77 | backward_inner: 7470.36 | backward_allreduce: 77.32 | step: 6226.46\n",
      "[2022-07-02 03:41:19,938] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6160.57\n",
      "[2022-07-02 03:41:19,939] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2640.71 | backward_microstep: 7523.25 | backward_inner_microstep: 7444.97 | backward_allreduce_microstep: 78.16 | step_microstep: 6204.39\n",
      "[2022-07-02 03:41:19,939] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2640.72 | backward: 7523.25 | backward_inner: 7444.98 | backward_allreduce: 78.18 | step: 6204.40\n",
      "[2022-07-02 03:41:36,314] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6166.25\n",
      "[2022-07-02 03:41:36,315] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2632.09 | backward_microstep: 7528.60 | backward_inner_microstep: 7451.06 | backward_allreduce_microstep: 77.41 | step_microstep: 6210.07\n",
      "[2022-07-02 03:41:36,315] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2632.10 | backward: 7528.60 | backward_inner: 7451.09 | backward_allreduce: 77.42 | step: 6210.08\n",
      "[2022-07-02 03:41:52,687] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6176.75\n",
      "[2022-07-02 03:41:52,688] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2627.50 | backward_microstep: 7520.91 | backward_inner_microstep: 7443.60 | backward_allreduce_microstep: 77.21 | step_microstep: 6220.14\n",
      "[2022-07-02 03:41:52,688] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2627.53 | backward: 7520.91 | backward_inner: 7443.61 | backward_allreduce: 77.22 | step: 6220.14\n",
      "[2022-07-02 03:42:09,090] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6159.78\n",
      "[2022-07-02 03:42:09,091] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2647.19 | backward_microstep: 7546.04 | backward_inner_microstep: 7468.63 | backward_allreduce_microstep: 77.30 | step_microstep: 6204.58\n",
      "[2022-07-02 03:42:09,091] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2647.21 | backward: 7546.04 | backward_inner: 7468.64 | backward_allreduce: 77.32 | step: 6204.58\n",
      "[2022-07-02 03:42:25,485] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6188.28\n",
      "[2022-07-02 03:42:25,486] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2633.39 | backward_microstep: 7524.52 | backward_inner_microstep: 7446.96 | backward_allreduce_microstep: 77.43 | step_microstep: 6231.98\n",
      "[2022-07-02 03:42:25,486] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2633.41 | backward: 7524.52 | backward_inner: 7446.98 | backward_allreduce: 77.46 | step: 6231.98\n",
      "[2022-07-02 03:42:41,828] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6153.00\n",
      "[2022-07-02 03:42:41,830] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2616.97 | backward_microstep: 7525.40 | backward_inner_microstep: 7448.21 | backward_allreduce_microstep: 77.07 | step_microstep: 6196.89\n",
      "[2022-07-02 03:42:41,830] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2616.99 | backward: 7525.40 | backward_inner: 7448.23 | backward_allreduce: 77.09 | step: 6196.89\n",
      "[2022-07-02 03:42:58,179] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6137.05\n",
      "[2022-07-02 03:42:58,180] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2636.26 | backward_microstep: 7529.36 | backward_inner_microstep: 7452.07 | backward_allreduce_microstep: 77.16 | step_microstep: 6180.58\n",
      "[2022-07-02 03:42:58,181] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2636.27 | backward: 7529.36 | backward_inner: 7452.08 | backward_allreduce: 77.19 | step: 6180.59\n",
      "[2022-07-02 03:43:14,557] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 6185.73\n",
      "[2022-07-02 03:43:14,558] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 03:43:14,558] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.2443284260810585\n",
      "[2022-07-02 03:43:14,558] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2617.84 | backward_microstep: 7526.08 | backward_inner_microstep: 7449.01 | backward_allreduce_microstep: 76.95 | step_microstep: 6229.39\n",
      "[2022-07-02 03:43:14,558] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2617.85 | backward: 7526.08 | backward_inner: 7449.02 | backward_allreduce: 76.97 | step: 6229.39\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 16373.0 | learning rate: 2.344E-06 | lm loss: 1.058423E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 2631.62 | backward: 7529.13 | backward-backward: 7529.10 | backward-allreduce: 0.00 | optimizer: 6211.04 | batch generator: 1.36\n",
      "Effective Tera Flops per GPU: 0.5 and total parameters 2.522 B\n",
      "rank: 0 | time: 2022-07-02 03:43:14 | exiting the program at iteration 50\n",
      "[2022-07-02 03:43:23,960] [INFO] [launch.py:159:main] Process 32042 exits successfully.\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 100 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-100_hs-2048_bs-4_ws-4_2022-07-02.1656733406.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 100 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 4 --gl-ray-max-concurrency 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 100\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 100\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.160 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.512 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.698 seconds\n",
      "time to initialize megatron (seconds): 3.706\n",
      "[after megatron is initialized] datetime: 2022-07-02 03:43:32 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 5140951040\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             5.141 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.22\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 03:44:51 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000549 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 03:44:51 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 78505.31 | train/valid/test-data-iterators-setup: 575.87\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 03:44:51 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 28772.3 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 9.948907E+00 | loss scale: 1.0 | grad norm: 2.968 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 5.85 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 28.772263598442077;  SamplesPerSecond: 0.13902277748548733\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 6408.6064453125 | max allocated: 12178.0068359375 | reserved: 14302.0 | max reserved: 14302.0\n",
      "time (ms) | e2e-time: 28772.65 | forward-compute: 4137.13 | backward-compute: 24624.25 | backward-embedding-all-reduce: 0.02 | optimizer: 2.11 | batch-generator: 1.87 | offloading-func-call-overhead: 8437.55 | offloading-fwd-overhead: 2793.19 | offloading-bwd-overhead: 14729.99 | offloading-fwd-2gpu-overhead: 677.74 | offloading-fwd-2cpu-overhead: 2113.55 | offloading-bwd-2gpu-overhead: 404.34 | offloading-bwd-2cpu-overhead: 14322.90\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 25388.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 9.189906E+00 | loss scale: 1.0 | grad norm: 2.271 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.64 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.388419103622436;  SamplesPerSecond: 0.15755214941403253\n",
      "time (ms) | e2e-time: 25388.33 | forward-compute: 3738.87 | backward-compute: 21638.49 | backward-embedding-all-reduce: 0.02 | optimizer: 2.05 | batch-generator: 1.32 | offloading-func-call-overhead: 51.60 | offloading-fwd-overhead: 3422.37 | offloading-bwd-overhead: 18739.56 | offloading-fwd-2gpu-overhead: 983.00 | offloading-fwd-2cpu-overhead: 2437.51 | offloading-bwd-2gpu-overhead: 433.17 | offloading-bwd-2cpu-overhead: 18303.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 25391.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.323940E+00 | loss scale: 1.0 | grad norm: 148.220 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.63 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.391076254844666;  SamplesPerSecond: 0.15753566173615002\n",
      "time (ms) | e2e-time: 25391.07 | forward-compute: 3707.36 | backward-compute: 21672.75 | backward-embedding-all-reduce: 0.02 | optimizer: 2.03 | batch-generator: 1.34 | offloading-func-call-overhead: 51.72 | offloading-fwd-overhead: 3372.11 | offloading-bwd-overhead: 19204.44 | offloading-fwd-2gpu-overhead: 680.60 | offloading-fwd-2cpu-overhead: 2689.65 | offloading-bwd-2gpu-overhead: 586.48 | offloading-bwd-2cpu-overhead: 18615.08\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 25445.2 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.272960E+00 | loss scale: 1.0 | grad norm: 1.867 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.62 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.445174717903136;  SamplesPerSecond: 0.15720072840315827\n",
      "time (ms) | e2e-time: 25445.18 | forward-compute: 3824.55 | backward-compute: 21609.68 | backward-embedding-all-reduce: 0.02 | optimizer: 2.04 | batch-generator: 1.37 | offloading-func-call-overhead: 51.22 | offloading-fwd-overhead: 3505.90 | offloading-bwd-overhead: 18347.21 | offloading-fwd-2gpu-overhead: 984.78 | offloading-fwd-2cpu-overhead: 2519.23 | offloading-bwd-2gpu-overhead: 540.39 | offloading-bwd-2cpu-overhead: 17803.97\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 25448.1 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 8.829635E+00 | loss scale: 1.0 | grad norm: 1.702 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.62 and total parameters 5.141 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 25.448073983192444;  SamplesPerSecond: 0.15718281873283846\n",
      "time (ms) | e2e-time: 25448.07 | forward-compute: 3752.19 | backward-compute: 21684.93 | backward-embedding-all-reduce: 0.02 | optimizer: 2.04 | batch-generator: 1.29 | offloading-func-call-overhead: 51.04 | offloading-fwd-overhead: 3441.61 | offloading-bwd-overhead: 18309.30 | offloading-fwd-2gpu-overhead: 723.53 | offloading-fwd-2cpu-overhead: 2716.24 | offloading-bwd-2gpu-overhead: 506.58 | offloading-bwd-2cpu-overhead: 17799.90\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 04:06:36 \n",
      "[after training is done] datetime: 2022-07-02 04:06:36 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ./examples/run.sh -m \"megatron-lm\" -l 32 -h 2048\n",
    "%docker_exec ./examples/run.sh -m \"l2l\" -l 78 -h 2048\n",
    "%docker_exec ./examples/run.sh -m \"zero-offload\" -l 48 -h 2048\n",
    "%docker_exec ./examples/run.sh -m \"zero-infinity\" -l 48 -h 2048\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 100 -h 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To print/draw the relevant information from log files\n",
    "\n",
    "> - extract and save useful infomation from the detailed logs to `./results/case1.csv` <br>\n",
    "> - visualize as a figure saved into `out/metric_model_scale.png` <br>\n",
    "> - `docker_exec cat ./results/case1.csv` can look through the numeric values <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./out/metric_model_scale.png?1937322503\" style=\"height:180px\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%docker_exec ./examples/case1_extract.sh\n",
    "%docker_exec python ./examples/case1_draw.py\n",
    "\n",
    "import random; __counter__ = random.randint(0,2e9)\n",
    "from IPython.display import HTML, display\n",
    "display(HTML('<img src=\"./out/metric_model_scale.png?%d\" style=\"height:180px\">' % __counter__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CASE - Throughput  on the largest trainable model size supported by each baseline (Figure 7a in Section VI.B)\n",
    "\n",
    "In this case, we use GPT-like models to exploit the largest trainable model size supported by each baseline and compare the performance against STRONGHOLD on each largest model size. Model size changes via increasing/decreasing the number of transformer layers.\n",
    "\n",
    "Here, we evaluate (Megatron-LM, L2L, ZeRO-Offload, ZeRO-Infinity) v.s. STRONGHOLD on a virtual machine with one 32GB V100, 90GB CPU RAM and 12 CPU Cores. During this process, we configure the `Heads=16, Sequence Length=1024, Batch Size=4` in all GPT-like models and training setups.\n",
    "\n",
    "The throughput has been tested in this notebook, shown in the following table. Please run the next cells to reproduce it. Thanks.\n",
    "\n",
    "| Methods | Throughput | Trainable Size | Layers | Hidden Size | Heads | Sequence Length | Batch Size |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| Megatron-LM | **0.7496** |1.717 B | 32 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.6647** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "|\n",
    "| L2L | **0.0529** | 4.033 B| 78 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.2271** | 4.033 B| 78 | 2048 | 16 | 1024 | 4 |\n",
    "|\n",
    "| ZeRO-Offload | **0.2523** |2.522 B | 48 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.3999**| 2.522 B| 48 | 2048 | 16 | 1024 | 4 |\n",
    "|\n",
    "| ZeRO-Infinity | **0.2439** | 2.522 B| 48 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.3999**| 2.522 B| 48 | 2048 | 16 | 1024 | 4 |\n",
    "\n",
    "PS: Limitations of CPU cores and bandwidth in the virtual machine hurts the performance of STRONGHOLD a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `ps aux` to check if there exists other running processes launched by other reviwers in case of GPU overlead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   4240  3512 pts/0    Ss+  07:09   0:00 /bin/bash\r\n",
      "root         209  0.0  0.0   4340  3808 pts/1    Ss+  07:10   0:00 /bin/bash\r\n",
      "root        3556  0.0  0.0   3976  3172 pts/2    Ss+  07:15   0:00 /bin/bash -c \r\n",
      "root        3714  0.0  0.0   5892  2848 pts/2    R+   07:15   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ps aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 7a in the submitted paper. Please refers to Section VI.B on page 9 for more details. <br><br>Runs around 40 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-15_2022-07-02.1656734808.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.508 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.704 seconds\n",
      "time to initialize megatron (seconds): 3.713\n",
      "[after megatron is initialized] datetime: 2022-07-02 04:06:55 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.18\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 04:07:23 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000573 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 04:07:24 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 28172.76 | train/valid/test-data-iterators-setup: 415.56\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 04:07:24 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 6618.7 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.106684E+01 | loss scale: 1.0 | grad norm: 1089.760 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.5 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.618663311004639;  SamplesPerSecond: 0.6043516359790244\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 23593.408203125 | reserved: 28904.0 | max reserved: 28904.0\n",
      "time (ms) | e2e-time: 6618.73 | forward-compute: 857.24 | backward-compute: 5750.48 | backward-embedding-all-reduce: 0.02 | optimizer: 2.49 | batch-generator: 1.68 | offloading-func-call-overhead: 1505.92 | offloading-fwd-overhead: 572.00 | offloading-bwd-overhead: 2.01 | offloading-fwd-2gpu-overhead: 261.11 | offloading-fwd-2cpu-overhead: 310.23 | offloading-bwd-2gpu-overhead: 0.74 | offloading-bwd-2cpu-overhead: 0.43\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 6001.7 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.091050E+01 | loss scale: 1.0 | grad norm: 11.120 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.37 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.0017084121704105;  SamplesPerSecond: 0.6664768971262753\n",
      "time (ms) | e2e-time: 6001.61 | forward-compute: 861.51 | backward-compute: 5129.36 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.24 | offloading-func-call-overhead: 14.09 | offloading-fwd-overhead: 761.42 | offloading-bwd-overhead: 2.16 | offloading-fwd-2gpu-overhead: 360.51 | offloading-fwd-2cpu-overhead: 400.32 | offloading-bwd-2gpu-overhead: 0.81 | offloading-bwd-2cpu-overhead: 0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 5990.7 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.076840E+01 | loss scale: 1.0 | grad norm: 1363562471.041 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.39 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.990670108795166;  SamplesPerSecond: 0.6677049357345557\n",
      "time (ms) | e2e-time: 5990.68 | forward-compute: 857.77 | backward-compute: 5122.16 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.17 | offloading-func-call-overhead: 15.44 | offloading-fwd-overhead: 748.51 | offloading-bwd-overhead: 2.13 | offloading-fwd-2gpu-overhead: 348.03 | offloading-fwd-2cpu-overhead: 399.89 | offloading-bwd-2gpu-overhead: 0.78 | offloading-bwd-2cpu-overhead: 0.48\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 5883.4 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.037473E+01 | loss scale: 1.0 | grad norm: 8.783 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.56 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.883359789848328;  SamplesPerSecond: 0.6798836282122259\n",
      "time (ms) | e2e-time: 5883.35 | forward-compute: 832.50 | backward-compute: 5040.12 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.18 | offloading-func-call-overhead: 13.66 | offloading-fwd-overhead: 737.09 | offloading-bwd-overhead: 2.15 | offloading-fwd-2gpu-overhead: 334.22 | offloading-fwd-2cpu-overhead: 402.29 | offloading-bwd-2gpu-overhead: 0.78 | offloading-bwd-2cpu-overhead: 0.52\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 5964.0 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.024192E+01 | loss scale: 1.0 | grad norm: 196.587 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.43 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 5.96402497291565;  SamplesPerSecond: 0.6706880031799245\n",
      "time (ms) | e2e-time: 5964.03 | forward-compute: 877.20 | backward-compute: 5076.09 | backward-embedding-all-reduce: 0.01 | optimizer: 2.47 | batch-generator: 1.22 | offloading-func-call-overhead: 25.15 | offloading-fwd-overhead: 759.18 | offloading-bwd-overhead: 2.30 | offloading-fwd-2gpu-overhead: 349.94 | offloading-fwd-2cpu-overhead: 408.21 | offloading-bwd-2gpu-overhead: 0.76 | offloading-bwd-2cpu-overhead: 0.69\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 04:12:28 \n",
      "[after training is done] datetime: 2022-07-02 04:12:28 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 48 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-48_hs-2048_bs-4_ws-15_2022-07-02.1656735152.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 48 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 48\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 48\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.155 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.498 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.680 seconds\n",
      "time to initialize megatron (seconds): 3.662\n",
      "[after megatron is initialized] datetime: 2022-07-02 04:12:39 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2522320896\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             2.522 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.21\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 04:13:19 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000595 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 04:13:20 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 40557.73 | train/valid/test-data-iterators-setup: 458.03\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 04:13:20 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 11192.9 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.082800E+01 | loss scale: 1.0 | grad norm: 4859.807 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.38 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 11.19293270111084;  SamplesPerSecond: 0.3573683597331932\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 23737.337890625 | reserved: 29610.0 | max reserved: 29610.0\n",
      "time (ms) | e2e-time: 11193.04 | forward-compute: 1440.21 | backward-compute: 9741.82 | backward-embedding-all-reduce: 0.02 | optimizer: 2.40 | batch-generator: 1.90 | offloading-func-call-overhead: 2892.89 | offloading-fwd-overhead: 934.74 | offloading-bwd-overhead: 3.42 | offloading-fwd-2gpu-overhead: 402.50 | offloading-fwd-2cpu-overhead: 531.30 | offloading-bwd-2gpu-overhead: 1.51 | offloading-bwd-2cpu-overhead: 0.65\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 10100.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.058723E+01 | loss scale: 1.0 | grad norm: 13.000 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.18 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 10.10037271976471;  SamplesPerSecond: 0.3960249894711985\n",
      "time (ms) | e2e-time: 10100.30 | forward-compute: 1388.98 | backward-compute: 8700.60 | backward-embedding-all-reduce: 0.02 | optimizer: 2.35 | batch-generator: 1.24 | offloading-func-call-overhead: 23.12 | offloading-fwd-overhead: 1240.98 | offloading-bwd-overhead: 83.89 | offloading-fwd-2gpu-overhead: 550.29 | offloading-fwd-2cpu-overhead: 689.79 | offloading-bwd-2gpu-overhead: 1.56 | offloading-bwd-2cpu-overhead: 80.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 10075.5 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.051130E+01 | loss scale: 1.0 | grad norm: 52.520 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.2 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 10.075518250465393;  SamplesPerSecond: 0.397001911024799\n",
      "time (ms) | e2e-time: 10075.51 | forward-compute: 1358.87 | backward-compute: 8705.88 | backward-embedding-all-reduce: 0.02 | optimizer: 2.36 | batch-generator: 1.21 | offloading-func-call-overhead: 35.92 | offloading-fwd-overhead: 1192.92 | offloading-bwd-overhead: 3.57 | offloading-fwd-2gpu-overhead: 535.56 | offloading-fwd-2cpu-overhead: 656.47 | offloading-bwd-2gpu-overhead: 1.51 | offloading-bwd-2cpu-overhead: 0.80\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 9939.3 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.010002E+01 | loss scale: 1.0 | grad norm: 8.393 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.32 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 9.93930606842041;  SamplesPerSecond: 0.4024425822552111\n",
      "time (ms) | e2e-time: 9939.31 | forward-compute: 1310.46 | backward-compute: 8618.08 | backward-embedding-all-reduce: 0.02 | optimizer: 2.38 | batch-generator: 1.25 | offloading-func-call-overhead: 24.31 | offloading-fwd-overhead: 1167.53 | offloading-bwd-overhead: 201.71 | offloading-fwd-2gpu-overhead: 512.22 | offloading-fwd-2cpu-overhead: 654.40 | offloading-bwd-2gpu-overhead: 2.30 | offloading-bwd-2cpu-overhead: 198.14\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 10098.0 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.759796E+00 | loss scale: 1.0 | grad norm: 586065550.624 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.18 and total parameters 2.522 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 10.098022055625915;  SamplesPerSecond: 0.39611717799442503\n",
      "time (ms) | e2e-time: 10098.03 | forward-compute: 1357.37 | backward-compute: 8729.92 | backward-embedding-all-reduce: 0.02 | optimizer: 2.36 | batch-generator: 1.24 | offloading-func-call-overhead: 24.07 | offloading-fwd-overhead: 1209.16 | offloading-bwd-overhead: 3.63 | offloading-fwd-2gpu-overhead: 523.03 | offloading-fwd-2cpu-overhead: 685.19 | offloading-bwd-2gpu-overhead: 1.54 | offloading-bwd-2cpu-overhead: 0.71\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 04:21:54 \n",
      "[after training is done] datetime: 2022-07-02 04:21:54 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 78 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-78_hs-2048_bs-4_ws-15_2022-07-02.1656735719.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 78 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 78\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 78\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.493 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.699 seconds\n",
      "time to initialize megatron (seconds): 3.686\n",
      "[after megatron is initialized] datetime: 2022-07-02 04:22:06 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4033069056\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             4.033 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.24\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 04:23:09 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000570 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 04:23:09 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 62564.83 | train/valid/test-data-iterators-setup: 528.72\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 04:23:09 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 20027.2 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.066295E+01 | loss scale: 1.0 | grad norm: 2295.732 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.6 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 20.02720456123352;  SamplesPerSecond: 0.19972832392908013\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 24969.345703125 | reserved: 31226.0 | max reserved: 31226.0\n",
      "time (ms) | e2e-time: 20027.33 | forward-compute: 2815.05 | backward-compute: 17201.10 | backward-embedding-all-reduce: 0.02 | optimizer: 2.21 | batch-generator: 1.85 | offloading-func-call-overhead: 5797.96 | offloading-fwd-overhead: 1906.78 | offloading-bwd-overhead: 6.11 | offloading-fwd-2gpu-overhead: 916.59 | offloading-fwd-2cpu-overhead: 988.57 | offloading-bwd-2gpu-overhead: 2.94 | offloading-bwd-2cpu-overhead: 1.11\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 17667.2 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.054288E+01 | loss scale: 1.0 | grad norm: 31451477.865 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.48 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.66720070838928;  SamplesPerSecond: 0.22640825029516973\n",
      "time (ms) | e2e-time: 17667.14 | forward-compute: 2555.79 | backward-compute: 15100.43 | backward-embedding-all-reduce: 0.02 | optimizer: 2.16 | batch-generator: 1.35 | offloading-func-call-overhead: 42.22 | offloading-fwd-overhead: 2304.24 | offloading-bwd-overhead: 104.41 | offloading-fwd-2gpu-overhead: 1115.68 | offloading-fwd-2cpu-overhead: 1187.01 | offloading-bwd-2gpu-overhead: 3.05 | offloading-bwd-2cpu-overhead: 99.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 17625.8 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.023125E+01 | loss scale: 1.0 | grad norm: 1466714772.580 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.5 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.625775718688963;  SamplesPerSecond: 0.22694036641795684\n",
      "time (ms) | e2e-time: 17625.81 | forward-compute: 2607.50 | backward-compute: 15007.40 | backward-embedding-all-reduce: 0.02 | optimizer: 2.17 | batch-generator: 1.39 | offloading-func-call-overhead: 41.11 | offloading-fwd-overhead: 2361.02 | offloading-bwd-overhead: 609.36 | offloading-fwd-2gpu-overhead: 1161.72 | offloading-fwd-2cpu-overhead: 1197.84 | offloading-bwd-2gpu-overhead: 3.26 | offloading-bwd-2cpu-overhead: 603.55\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 17623.1 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.002733E+01 | loss scale: 1.0 | grad norm: 100.084 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.5 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.623104548454286;  SamplesPerSecond: 0.22697476423646581\n",
      "time (ms) | e2e-time: 17623.07 | forward-compute: 2535.64 | backward-compute: 15076.55 | backward-embedding-all-reduce: 0.02 | optimizer: 2.15 | batch-generator: 1.33 | offloading-func-call-overhead: 42.82 | offloading-fwd-overhead: 2275.74 | offloading-bwd-overhead: 217.12 | offloading-fwd-2gpu-overhead: 1107.33 | offloading-fwd-2cpu-overhead: 1166.84 | offloading-bwd-2gpu-overhead: 3.27 | offloading-bwd-2cpu-overhead: 211.72\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 17932.0 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.963859E+00 | loss scale: 1.0 | grad norm: 112474.305 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.37 and total parameters 4.033 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 17.931956076622008;  SamplesPerSecond: 0.2230654582750636\n",
      "time (ms) | e2e-time: 17932.07 | forward-compute: 2574.31 | backward-compute: 15346.76 | backward-embedding-all-reduce: 0.02 | optimizer: 2.16 | batch-generator: 1.37 | offloading-func-call-overhead: 48.47 | offloading-fwd-overhead: 2310.92 | offloading-bwd-overhead: 6.57 | offloading-fwd-2gpu-overhead: 1114.04 | offloading-fwd-2cpu-overhead: 1195.40 | offloading-bwd-2gpu-overhead: 3.21 | offloading-bwd-2cpu-overhead: 1.20\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 04:38:18 \n",
      "[after training is done] datetime: 2022-07-02 04:38:18 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 15\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 48 -h 2048 -w 15\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 78 -h 2048 -w 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To print/draw the relevant information from log files\n",
    "\n",
    "> - extract and save useful infomation from the detailed logs to `./results/case2.csv` <br>\n",
    "> - visualize as a figure saved into `out/metric_throughput_vs.png` <br>\n",
    "> - `docker_exec cat ./results/case2.csv` can look through the numeric values <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./out/metric_throughput_vs.png?1196981085\" style=\"height:180px\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%docker_exec ./examples/case2_extract.sh\n",
    "%docker_exec python ./examples/case2_draw.py\n",
    "\n",
    "import random; __counter__ = random.randint(0,2e9)\n",
    "from IPython.display import HTML, display\n",
    "display(HTML('<img src=\"./out/metric_throughput_vs.png?%d\" style=\"height:180px\">' % __counter__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitations of CPU cores and bandwidth in the virtual machine, STRONGHOLD might slow slightly compared with Megatron-LM, but still outperfom other offloading solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 CASE - Throughput on the largest trainable model size of Megatron-LM (Figure 8a in Section VI.B)\n",
    "\n",
    "This case shows the throughput performance of running Megatron-LM, L2L, ZeRO-Offload, ZeRO-Infinity and STRONGHOLD, respectively, on a 1.717 B model that is the largest trainable model size supported by Megatron-LM. The evaluation is conducted on a virtual machine with one 32GB V100, 90GB CPU RAM and 12 CPU Cores.\n",
    "\n",
    "The throughput results have been tested in this notebook, shown in the following table. Please run the following cells to reproduce it. Thanks.\n",
    "\n",
    "| Methods | Throughput | Trainable Size | Layers | Hidden Size | Heads | Sequence Length | Batch Size |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| Megatron-LM | **0.7496** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| L2L | **0.1729**| 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Offload | **0.3711**| 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| ZeRO-Infinity | **0.3587** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "| STRONGHOLD | **0.6647** | 1.717 B| 32 | 2048 | 16 | 1024 | 4 |\n",
    "\n",
    "PS: Limitations of CPU cores and bandwidth in the virtual machine hurts the performance of STRONGHOLD a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `ps aux` to check if there exists other running processes launched by other reviwers in case of GPU overlead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   4340  2716 pts/0    Ss+  02:10   0:00 /bin/bash\r\n",
      "root       36316  0.0  0.0   3976  3164 pts/1    Ss+  04:38   0:00 /bin/bash -c \r\n",
      "root       36474  0.0  0.0   5892  2844 pts/1    R+   04:38   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ps aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 8a in the submitted paper. Please refers to Section VI.B on page 9 for more details. <br><br>Runs around 45 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../L2L-Megatron-LM/examples/sc22-gpt-l2l.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_l2l_l-32_hs-2048_bs-4_ws-4_2022-07-02.1656736709.txt && cd -\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_l2l ...................................... True\n",
      "  enbale_strongh .................................. None\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt2_345m_ds\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ checkpoints/gpt2_345m_ds\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.156 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.523 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/L2L-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module ds_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module ds_cpu_adam...\n",
      ">>> done with compiling and loading strongh utils. Compilation time: 0.556 seconds\n",
      "time to initialize megatron (seconds): 3.482\n",
      "[after megatron is initialized] datetime: 2022-07-02 04:38:35 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_345m_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.27\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 04:38:42 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000557 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 04:38:43 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 7351.69 | train/valid/test-data-iterators-setup: 773.60\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 04:38:43 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 28715.1 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.120928E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 1.96 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 28.71507022380829;  SamplesPerSecond: 0.1392996767489537\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 7142.48583984375 | max allocated: 10785.58935546875 | reserved: 15412.0 | max reserved: 15412.0\n",
      "time (ms) | forward-compute: 6868.95 | backward-compute: 16240.59 | backward-params-all-reduce: 16.82 | backward-embedding-all-reduce: 0.04 | optimizer: 5476.87 | batch-generator: 1.93\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 24464.9 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.120716E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.3 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 24.464899206161498;  SamplesPerSecond: 0.16349954955026333\n",
      "time (ms) | forward-compute: 4990.55 | backward-compute: 15934.03 | backward-params-all-reduce: 16.82 | backward-embedding-all-reduce: 0.04 | optimizer: 3474.51 | batch-generator: 1.36\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 22770.9 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.119732E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.47 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 22.7709210395813;  SamplesPerSecond: 0.17566263538690616\n",
      "time (ms) | forward-compute: 4168.86 | backward-compute: 15906.91 | backward-params-all-reduce: 16.83 | backward-embedding-all-reduce: 0.04 | optimizer: 2663.61 | batch-generator: 1.37\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 22349.2 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.121551E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.52 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 22.349222087860106;  SamplesPerSecond: 0.17897714668882206\n",
      "time (ms) | forward-compute: 4002.82 | backward-compute: 15803.96 | backward-params-all-reduce: 16.83 | backward-embedding-all-reduce: 0.04 | optimizer: 2514.38 | batch-generator: 1.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       50/      50 | elapsed time per iteration (ms): 22823.0 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.121118E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 2.46 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 22.822991847991943;  SamplesPerSecond: 0.17526185991044535\n",
      "time (ms) | forward-compute: 4267.56 | backward-compute: 15703.58 | backward-params-all-reduce: 16.84 | backward-embedding-all-reduce: 0.04 | optimizer: 2815.08 | batch-generator: 1.38\n",
      "saving checkpoint at iteration      50 to checkpoints/gpt2_345m_ds\n",
      "  successfully saved checkpoint at iteration      50 to checkpoints/gpt2_345m_ds\n",
      "time (ms) | save-checkpoint: 69139.43\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 05:00:03 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-offloading.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-offload_l-32_hs-2048_bs-4_ws-4_2022-07-02.1656738007.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 05:00:08,868] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-07-02 05:00:10,490] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json --zero-stage 2 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-07-02 05:00:11,366] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_2_config.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 32\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 2\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-07-02 05:00:14,847] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-07-02 05:00:14,848] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-07-02 05:00:14,848] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.15 GB, percent = 3.5%\n",
      "[2022-07-02 05:00:14,849] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f19efdc6310>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-07-02 05:00:14,849] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-07-02 05:00:14,993] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-07-02 05:00:14,994] [INFO] [utils.py:823:see_memory_usage] MA 6.39 GB         Max_MA 6.39 GB         CA 6.39 GB         Max_CA 6 GB \n",
      "[2022-07-02 05:00:14,994] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.17 GB, percent = 3.5%\n",
      " > number of parameters on model parallel rank 0            1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-07-02 05:00:14,997] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f19efdc6310>\n",
      "train_batch = 4, micro_batch=None\n",
      "[2022-07-02 05:00:15,001] [WARNING] [config.py:1100:_do_warning_check] DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM (1.0) > 0, setting to zero\n",
      "[2022-07-02 05:00:15,011] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-07-02 05:00:15,011] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-07-02 05:00:15,012] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-07-02 05:00:15,012] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-07-02 05:00:15,073] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-07-02 05:00:15,073] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-07-02 05:00:15,073] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-07-02 05:00:15,098] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-07-02 05:00:15,098] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-07-02 05:00:15,099] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
      "[2022-07-02 05:00:15,099] [INFO] [stage2.py:113:__init__] Reduce bucket size 50000000\n",
      "[2022-07-02 05:00:15,099] [INFO] [stage2.py:114:__init__] Allgather bucket size 50000000\n",
      "[2022-07-02 05:00:15,099] [INFO] [stage2.py:115:__init__] CPU Offload: True\n",
      "[2022-07-02 05:00:15,099] [INFO] [stage2.py:116:__init__] Round robin gradient partitioning: False\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.23087596893310547 seconds\n",
      "Rank: 0 partition count [1, 1] and sizes[(1715732480, False), (856064, False)] \n",
      "[2022-07-02 05:00:32,298] [INFO] [utils.py:822:see_memory_usage] Before initializing optimizer states\n",
      "[2022-07-02 05:00:32,299] [INFO] [utils.py:823:see_memory_usage] MA 6.78 GB         Max_MA 6.78 GB         CA 12.8 GB         Max_CA 13 GB \n",
      "[2022-07-02 05:00:32,299] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 10.55 GB, percent = 11.7%\n",
      "[2022-07-02 05:00:42,781] [INFO] [utils.py:822:see_memory_usage] After initializing optimizer states\n",
      "[2022-07-02 05:00:42,782] [INFO] [utils.py:823:see_memory_usage] MA 6.78 GB         Max_MA 6.78 GB         CA 12.8 GB         Max_CA 13 GB \n",
      "[2022-07-02 05:00:42,783] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 29.88 GB, percent = 33.1%\n",
      "[2022-07-02 05:00:42,783] [INFO] [stage2.py:483:__init__] optimizer state initialized\n",
      "[2022-07-02 05:02:42,968] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 182.70 | optimizer_step: 5004.58\n",
      "[2022-07-02 05:02:42,968] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:02:42,968] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.3728414180436411\n",
      "[2022-07-02 05:02:42,968] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.05 | backward_microstep: 4301.43 | backward_inner_microstep: 4262.50 | backward_allreduce_microstep: 38.84 | step_microstep: 5227.69\n",
      "[2022-07-02 05:02:42,969] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.11 | backward: 4301.43 | backward_inner: 4262.51 | backward_allreduce: 38.84 | step: 5227.70\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 11873.3 | learning rate: 4.687E-07 | lm loss: 1.091905E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "after 10 iterations memory (MB) | allocated: 6943.26611328125 | max allocated: 10359.83984375 | reserved: 20720.0 | max reserved: 20720.0\n",
      "time (ms) | forward: 1279.34 | backward: 4298.55 | backward-backward: 4298.51 | backward-allreduce: 0.00 | optimizer: 6295.00 | batch generator: 1.91\n",
      "Effective Tera Flops per GPU: 0.47 and total parameters 1.717 B\n",
      "[2022-07-02 05:02:53,700] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 185.59 | optimizer_step: 5002.59\n",
      "[2022-07-02 05:02:53,701] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.80 | backward_microstep: 4307.85 | backward_inner_microstep: 4268.70 | backward_allreduce_microstep: 39.03 | step_microstep: 5228.51\n",
      "[2022-07-02 05:02:53,701] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.88 | backward: 4307.85 | backward_inner: 4268.72 | backward_allreduce: 39.03 | step: 5228.51\n",
      "[2022-07-02 05:03:04,451] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 183.01 | optimizer_step: 5019.83\n",
      "[2022-07-02 05:03:04,452] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.42 | backward_microstep: 4312.33 | backward_inner_microstep: 4273.34 | backward_allreduce_microstep: 38.90 | step_microstep: 5243.21\n",
      "[2022-07-02 05:03:04,452] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.50 | backward: 4312.33 | backward_inner: 4273.35 | backward_allreduce: 38.90 | step: 5243.21\n",
      "[2022-07-02 05:03:15,200] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 183.11 | optimizer_step: 5014.29\n",
      "[2022-07-02 05:03:15,201] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.58 | backward_microstep: 4317.06 | backward_inner_microstep: 4278.12 | backward_allreduce_microstep: 38.85 | step_microstep: 5237.65\n",
      "[2022-07-02 05:03:15,201] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.66 | backward: 4317.06 | backward_inner: 4278.13 | backward_allreduce: 38.85 | step: 5237.65\n",
      "[2022-07-02 05:03:25,930] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 183.67 | optimizer_step: 5006.48\n",
      "[2022-07-02 05:03:25,930] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.54 | backward_microstep: 4301.64 | backward_inner_microstep: 4262.63 | backward_allreduce_microstep: 38.90 | step_microstep: 5230.35\n",
      "[2022-07-02 05:03:25,931] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.61 | backward: 4301.64 | backward_inner: 4262.65 | backward_allreduce: 38.89 | step: 5230.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 05:03:36,650] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 182.28 | optimizer_step: 5001.92\n",
      "[2022-07-02 05:03:36,651] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.34 | backward_microstep: 4301.74 | backward_inner_microstep: 4262.77 | backward_allreduce_microstep: 38.88 | step_microstep: 5224.43\n",
      "[2022-07-02 05:03:36,651] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.40 | backward: 4301.74 | backward_inner: 4262.78 | backward_allreduce: 38.89 | step: 5224.43\n",
      "[2022-07-02 05:03:47,387] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.74 | optimizer_gradients: 185.19 | optimizer_step: 5003.76\n",
      "[2022-07-02 05:03:47,388] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.15 | backward_microstep: 4311.46 | backward_inner_microstep: 4272.51 | backward_allreduce_microstep: 38.87 | step_microstep: 5229.08\n",
      "[2022-07-02 05:03:47,388] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.21 | backward: 4311.46 | backward_inner: 4272.51 | backward_allreduce: 38.87 | step: 5229.08\n",
      "[2022-07-02 05:03:58,113] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 182.91 | optimizer_step: 4999.85\n",
      "[2022-07-02 05:03:58,114] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.72 | backward_microstep: 4305.29 | backward_inner_microstep: 4266.42 | backward_allreduce_microstep: 38.79 | step_microstep: 5222.96\n",
      "[2022-07-02 05:03:58,114] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.80 | backward: 4305.29 | backward_inner: 4266.43 | backward_allreduce: 38.79 | step: 5222.97\n",
      "[2022-07-02 05:04:08,854] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 185.82 | optimizer_step: 5014.66\n",
      "[2022-07-02 05:04:08,855] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.92 | backward_microstep: 4305.79 | backward_inner_microstep: 4266.75 | backward_allreduce_microstep: 38.95 | step_microstep: 5240.67\n",
      "[2022-07-02 05:04:08,855] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.00 | backward: 4305.79 | backward_inner: 4266.76 | backward_allreduce: 38.95 | step: 5240.67\n",
      "[2022-07-02 05:04:19,588] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 183.82 | optimizer_step: 5007.61\n",
      "[2022-07-02 05:04:19,588] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.00 | backward_microstep: 4305.21 | backward_inner_microstep: 4266.27 | backward_allreduce_microstep: 38.85 | step_microstep: 5231.49\n",
      "[2022-07-02 05:04:19,589] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.07 | backward: 4305.21 | backward_inner: 4266.28 | backward_allreduce: 38.85 | step: 5231.49\n",
      "[2022-07-02 05:04:30,328] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 185.83 | optimizer_step: 5013.83\n",
      "[2022-07-02 05:04:30,329] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:04:30,329] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.3727387139784832\n",
      "[2022-07-02 05:04:30,329] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.24 | backward_microstep: 4304.56 | backward_inner_microstep: 4265.53 | backward_allreduce_microstep: 38.94 | step_microstep: 5240.11\n",
      "[2022-07-02 05:04:30,329] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.32 | backward: 4304.56 | backward_inner: 4265.54 | backward_allreduce: 38.94 | step: 5240.12\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 10736.1 | learning rate: 9.375E-07 | lm loss: 9.697246E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1194.83 | backward: 4307.39 | backward-backward: 4307.36 | backward-allreduce: 0.00 | optimizer: 5233.14 | batch generator: 1.22\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n",
      "[2022-07-02 05:04:41,066] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 184.13 | optimizer_step: 5006.82\n",
      "[2022-07-02 05:04:41,067] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.09 | backward_microstep: 4307.57 | backward_inner_microstep: 4268.69 | backward_allreduce_microstep: 38.78 | step_microstep: 5231.15\n",
      "[2022-07-02 05:04:41,067] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.17 | backward: 4307.56 | backward_inner: 4268.68 | backward_allreduce: 38.78 | step: 5231.15\n",
      "[2022-07-02 05:04:51,799] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 190.83 | optimizer_step: 5001.81\n",
      "[2022-07-02 05:04:51,799] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.16 | backward_microstep: 4306.60 | backward_inner_microstep: 4267.49 | backward_allreduce_microstep: 39.02 | step_microstep: 5232.91\n",
      "[2022-07-02 05:04:51,799] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.23 | backward: 4306.60 | backward_inner: 4267.50 | backward_allreduce: 39.02 | step: 5232.91\n",
      "[2022-07-02 05:05:02,554] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 184.43 | optimizer_step: 5030.42\n",
      "[2022-07-02 05:05:02,554] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.09 | backward_microstep: 4307.23 | backward_inner_microstep: 4268.40 | backward_allreduce_microstep: 38.75 | step_microstep: 5255.05\n",
      "[2022-07-02 05:05:02,555] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.16 | backward: 4307.23 | backward_inner: 4268.40 | backward_allreduce: 38.75 | step: 5255.05\n",
      "[2022-07-02 05:05:13,292] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.74 | optimizer_gradients: 183.51 | optimizer_step: 5010.03\n",
      "[2022-07-02 05:05:13,292] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.58 | backward_microstep: 4308.47 | backward_inner_microstep: 4269.55 | backward_allreduce_microstep: 38.83 | step_microstep: 5233.69\n",
      "[2022-07-02 05:05:13,292] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.66 | backward: 4308.47 | backward_inner: 4269.56 | backward_allreduce: 38.83 | step: 5233.69\n",
      "[2022-07-02 05:05:24,034] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 182.55 | optimizer_step: 5009.68\n",
      "[2022-07-02 05:05:24,035] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.13 | backward_microstep: 4315.28 | backward_inner_microstep: 4275.68 | backward_allreduce_microstep: 39.51 | step_microstep: 5232.37\n",
      "[2022-07-02 05:05:24,035] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.21 | backward: 4315.27 | backward_inner: 4275.68 | backward_allreduce: 39.51 | step: 5232.37\n",
      "[2022-07-02 05:05:34,759] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 183.27 | optimizer_step: 4995.76\n",
      "[2022-07-02 05:05:34,760] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1194.03 | backward_microstep: 4308.69 | backward_inner_microstep: 4269.67 | backward_allreduce_microstep: 38.94 | step_microstep: 5219.24\n",
      "[2022-07-02 05:05:34,760] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1194.11 | backward: 4308.69 | backward_inner: 4269.67 | backward_allreduce: 38.94 | step: 5219.25\n",
      "[2022-07-02 05:05:45,483] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 182.38 | optimizer_step: 4994.02\n",
      "[2022-07-02 05:05:45,484] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.59 | backward_microstep: 4312.60 | backward_inner_microstep: 4273.04 | backward_allreduce_microstep: 39.46 | step_microstep: 5216.65\n",
      "[2022-07-02 05:05:45,484] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.68 | backward: 4312.60 | backward_inner: 4273.04 | backward_allreduce: 39.47 | step: 5216.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 05:05:56,229] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 187.43 | optimizer_step: 5011.95\n",
      "[2022-07-02 05:05:56,230] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.35 | backward_microstep: 4311.41 | backward_inner_microstep: 4270.40 | backward_allreduce_microstep: 40.92 | step_microstep: 5239.63\n",
      "[2022-07-02 05:05:56,230] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.44 | backward: 4311.41 | backward_inner: 4270.41 | backward_allreduce: 40.93 | step: 5239.63\n",
      "[2022-07-02 05:06:06,979] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 186.48 | optimizer_step: 5017.63\n",
      "[2022-07-02 05:06:06,980] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.27 | backward_microstep: 4311.78 | backward_inner_microstep: 4272.71 | backward_allreduce_microstep: 38.98 | step_microstep: 5244.28\n",
      "[2022-07-02 05:06:06,980] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.35 | backward: 4311.78 | backward_inner: 4272.71 | backward_allreduce: 38.99 | step: 5244.28\n",
      "[2022-07-02 05:06:17,721] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 185.31 | optimizer_step: 5014.66\n",
      "[2022-07-02 05:06:17,721] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:06:17,721] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.37266887072232496\n",
      "[2022-07-02 05:06:17,721] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.96 | backward_microstep: 4307.08 | backward_inner_microstep: 4267.96 | backward_allreduce_microstep: 39.02 | step_microstep: 5240.43\n",
      "[2022-07-02 05:06:17,722] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.04 | backward: 4307.07 | backward_inner: 4267.97 | backward_allreduce: 39.03 | step: 5240.44\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 10739.2 | learning rate: 1.406E-06 | lm loss: 9.170977E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1194.00 | backward: 4309.77 | backward-backward: 4309.73 | backward-allreduce: 0.00 | optimizer: 5234.81 | batch generator: 1.20\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n",
      "[2022-07-02 05:06:28,461] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 184.21 | optimizer_step: 5012.88\n",
      "[2022-07-02 05:06:28,462] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.43 | backward_microstep: 4305.78 | backward_inner_microstep: 4266.48 | backward_allreduce_microstep: 39.21 | step_microstep: 5237.34\n",
      "[2022-07-02 05:06:28,462] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.52 | backward: 4305.78 | backward_inner: 4266.49 | backward_allreduce: 39.21 | step: 5237.34\n",
      "[2022-07-02 05:06:39,210] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 184.19 | optimizer_step: 5014.66\n",
      "[2022-07-02 05:06:39,211] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.11 | backward_microstep: 4315.68 | backward_inner_microstep: 4276.58 | backward_allreduce_microstep: 39.02 | step_microstep: 5239.15\n",
      "[2022-07-02 05:06:39,211] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.19 | backward: 4315.68 | backward_inner: 4276.58 | backward_allreduce: 39.02 | step: 5239.15\n",
      "[2022-07-02 05:06:49,943] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 185.05 | optimizer_step: 5013.92\n",
      "[2022-07-02 05:06:49,944] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1193.06 | backward_microstep: 4297.87 | backward_inner_microstep: 4258.79 | backward_allreduce_microstep: 38.99 | step_microstep: 5239.22\n",
      "[2022-07-02 05:06:49,944] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1193.14 | backward: 4297.87 | backward_inner: 4258.79 | backward_allreduce: 38.99 | step: 5239.22\n",
      "[2022-07-02 05:07:00,679] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 185.77 | optimizer_step: 5008.31\n",
      "[2022-07-02 05:07:00,679] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1188.88 | backward_microstep: 4309.38 | backward_inner_microstep: 4270.25 | backward_allreduce_microstep: 39.04 | step_microstep: 5234.42\n",
      "[2022-07-02 05:07:00,680] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1188.95 | backward: 4309.38 | backward_inner: 4270.26 | backward_allreduce: 39.04 | step: 5234.43\n",
      "[2022-07-02 05:07:11,406] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 182.60 | optimizer_step: 5005.02\n",
      "[2022-07-02 05:07:11,407] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.10 | backward_microstep: 4306.17 | backward_inner_microstep: 4267.17 | backward_allreduce_microstep: 38.92 | step_microstep: 5227.84\n",
      "[2022-07-02 05:07:11,407] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.18 | backward: 4306.17 | backward_inner: 4267.17 | backward_allreduce: 38.92 | step: 5227.84\n",
      "[2022-07-02 05:07:22,169] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 213.34 | optimizer_step: 5012.81\n",
      "[2022-07-02 05:07:22,170] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.28 | backward_microstep: 4302.94 | backward_inner_microstep: 4263.90 | backward_allreduce_microstep: 38.95 | step_microstep: 5266.37\n",
      "[2022-07-02 05:07:22,170] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.35 | backward: 4302.94 | backward_inner: 4263.91 | backward_allreduce: 38.95 | step: 5266.38\n",
      "[2022-07-02 05:07:32,918] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 182.82 | optimizer_step: 5017.66\n",
      "[2022-07-02 05:07:32,918] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.30 | backward_microstep: 4313.23 | backward_inner_microstep: 4274.08 | backward_allreduce_microstep: 39.06 | step_microstep: 5240.65\n",
      "[2022-07-02 05:07:32,918] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.38 | backward: 4313.23 | backward_inner: 4274.09 | backward_allreduce: 39.07 | step: 5240.65\n",
      "[2022-07-02 05:07:43,653] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 185.70 | optimizer_step: 5006.53\n",
      "[2022-07-02 05:07:43,654] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.13 | backward_microstep: 4308.28 | backward_inner_microstep: 4269.13 | backward_allreduce_microstep: 39.07 | step_microstep: 5232.62\n",
      "[2022-07-02 05:07:43,654] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.20 | backward: 4308.28 | backward_inner: 4269.13 | backward_allreduce: 39.07 | step: 5232.63\n",
      "[2022-07-02 05:07:54,402] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.74 | optimizer_gradients: 182.79 | optimizer_step: 5021.25\n",
      "[2022-07-02 05:07:54,403] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.72 | backward_microstep: 4308.47 | backward_inner_microstep: 4269.42 | backward_allreduce_microstep: 38.96 | step_microstep: 5244.17\n",
      "[2022-07-02 05:07:54,403] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.80 | backward: 4308.47 | backward_inner: 4269.43 | backward_allreduce: 38.96 | step: 5244.17\n",
      "[2022-07-02 05:08:05,140] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 182.41 | optimizer_step: 5014.70\n",
      "[2022-07-02 05:08:05,140] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:08:05,140] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.3726121263496686\n",
      "[2022-07-02 05:08:05,141] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.89 | backward_microstep: 4304.50 | backward_inner_microstep: 4265.50 | backward_allreduce_microstep: 38.91 | step_microstep: 5237.52\n",
      "[2022-07-02 05:08:05,141] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.96 | backward: 4304.50 | backward_inner: 4265.51 | backward_allreduce: 38.91 | step: 5237.52\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 10741.9 | learning rate: 1.875E-06 | lm loss: 8.940726E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1193.71 | backward: 4307.33 | backward-backward: 4307.29 | backward-allreduce: 0.00 | optimizer: 5240.22 | batch generator: 1.26\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 05:08:15,883] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 184.50 | optimizer_step: 5012.36\n",
      "[2022-07-02 05:08:15,884] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.01 | backward_microstep: 4309.50 | backward_inner_microstep: 4270.47 | backward_allreduce_microstep: 38.94 | step_microstep: 5237.02\n",
      "[2022-07-02 05:08:15,884] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.07 | backward: 4309.50 | backward_inner: 4270.47 | backward_allreduce: 38.95 | step: 5237.02\n",
      "[2022-07-02 05:08:26,618] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 183.56 | optimizer_step: 5009.68\n",
      "[2022-07-02 05:08:26,619] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.52 | backward_microstep: 4307.84 | backward_inner_microstep: 4268.75 | backward_allreduce_microstep: 39.00 | step_microstep: 5233.43\n",
      "[2022-07-02 05:08:26,619] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.59 | backward: 4307.84 | backward_inner: 4268.76 | backward_allreduce: 39.00 | step: 5233.43\n",
      "[2022-07-02 05:08:37,369] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.76 | optimizer_gradients: 183.03 | optimizer_step: 5018.75\n",
      "[2022-07-02 05:08:37,370] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.45 | backward_microstep: 4313.52 | backward_inner_microstep: 4274.43 | backward_allreduce_microstep: 39.00 | step_microstep: 5242.04\n",
      "[2022-07-02 05:08:37,370] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.52 | backward: 4313.53 | backward_inner: 4274.44 | backward_allreduce: 39.01 | step: 5242.04\n",
      "[2022-07-02 05:08:48,120] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.74 | optimizer_gradients: 184.46 | optimizer_step: 5027.99\n",
      "[2022-07-02 05:08:48,120] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1189.71 | backward_microstep: 4305.64 | backward_inner_microstep: 4266.50 | backward_allreduce_microstep: 39.05 | step_microstep: 5252.62\n",
      "[2022-07-02 05:08:48,120] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1189.78 | backward: 4305.64 | backward_inner: 4266.51 | backward_allreduce: 39.05 | step: 5252.62\n",
      "[2022-07-02 05:08:58,846] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.78 | optimizer_gradients: 182.37 | optimizer_step: 5002.38\n",
      "[2022-07-02 05:08:58,847] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.48 | backward_microstep: 4308.05 | backward_inner_microstep: 4268.96 | backward_allreduce_microstep: 39.00 | step_microstep: 5224.93\n",
      "[2022-07-02 05:08:58,847] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.54 | backward: 4308.05 | backward_inner: 4268.96 | backward_allreduce: 39.00 | step: 5224.93\n",
      "[2022-07-02 05:09:09,599] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 185.67 | optimizer_step: 5021.67\n",
      "[2022-07-02 05:09:09,599] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1191.29 | backward_microstep: 4311.03 | backward_inner_microstep: 4272.02 | backward_allreduce_microstep: 38.93 | step_microstep: 5247.53\n",
      "[2022-07-02 05:09:09,599] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1191.35 | backward: 4311.03 | backward_inner: 4272.02 | backward_allreduce: 38.93 | step: 5247.53\n",
      "[2022-07-02 05:09:20,332] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.77 | optimizer_gradients: 183.95 | optimizer_step: 5006.28\n",
      "[2022-07-02 05:09:20,333] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1192.55 | backward_microstep: 4307.94 | backward_inner_microstep: 4268.90 | backward_allreduce_microstep: 38.96 | step_microstep: 5230.44\n",
      "[2022-07-02 05:09:20,333] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1192.61 | backward: 4307.94 | backward_inner: 4268.91 | backward_allreduce: 38.96 | step: 5230.44\n",
      "[2022-07-02 05:09:31,062] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.74 | optimizer_gradients: 184.19 | optimizer_step: 5006.10\n",
      "[2022-07-02 05:09:31,063] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.58 | backward_microstep: 4305.93 | backward_inner_microstep: 4266.89 | backward_allreduce_microstep: 38.95 | step_microstep: 5230.43\n",
      "[2022-07-02 05:09:31,063] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.65 | backward: 4305.94 | backward_inner: 4266.90 | backward_allreduce: 38.95 | step: 5230.43\n",
      "[2022-07-02 05:09:41,818] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 185.53 | optimizer_step: 5027.52\n",
      "[2022-07-02 05:09:41,819] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.43 | backward_microstep: 4309.45 | backward_inner_microstep: 4268.07 | backward_allreduce_microstep: 41.28 | step_microstep: 5253.38\n",
      "[2022-07-02 05:09:41,819] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.50 | backward: 4309.45 | backward_inner: 4268.09 | backward_allreduce: 41.29 | step: 5253.38\n",
      "[2022-07-02 05:09:52,576] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 35.75 | optimizer_gradients: 184.07 | optimizer_step: 5033.76\n",
      "[2022-07-02 05:09:52,576] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:09:52,577] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.37256627722850977\n",
      "[2022-07-02 05:09:52,577] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1190.39 | backward_microstep: 4306.80 | backward_inner_microstep: 4267.64 | backward_allreduce_microstep: 39.07 | step_microstep: 5258.16\n",
      "[2022-07-02 05:09:52,577] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1190.46 | backward: 4306.80 | backward_inner: 4267.65 | backward_allreduce: 39.07 | step: 5258.16\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 10743.6 | learning rate: 2.344E-06 | lm loss: 8.779677E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1193.03 | backward: 4308.67 | backward-backward: 4308.63 | backward-allreduce: 0.00 | optimizer: 5241.25 | batch generator: 1.23\n",
      "Effective Tera Flops per GPU: 0.52 and total parameters 1.717 B\n",
      "rank: 0 | time: 2022-07-02 05:09:52 | exiting the program at iteration 50\n",
      "[2022-07-02 05:09:56,930] [INFO] [launch.py:159:main] Process 37234 exits successfully.\n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../DeepSpeedExample-Megatron-LM/examples/sc22-gpt-zero-infinity-cpu.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_zero-infinity_l-32_hs-2048_bs-4_ws-4_2022-07-02.1656738599.txt && cd -\n",
      "deepspeed --num_nodes 1 --num_gpus 1 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 05:09:59,972] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-07-02 05:10:01,594] [INFO] [runner.py:398:main] cmd = /root/.pyenv/versions/3.9.10/envs/py3.9.10/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 pretrain_gpt2.py --model-parallel-size 1 --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --batch-size 4 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save checkpoints/gpt2_ds --load checkpoints/gpt2_ds --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 1.5e-4 --lr-decay-style cosine --min-lr 1.0e-5 --weight-decay 1e-2 --clip-grad 1.0 --warmup 0.01 --checkpoint-activations --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --deepspeed --deepspeed_config /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json --zero-stage 3 --zero-reduce-bucket-size 50000000 --zero-allgather-bucket-size 5000000000 --zero-contigious-gradients --zero-reduce-scatter --checkpoint-activations --checkpoint-num-layers 1 --partition-activations --synchronize-each-layer --contigious-checkpointing\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda11.4\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda11.4\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-07-02 05:10:02,473] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-07-02 05:10:02,474] [INFO] [launch.py:100:main] dist_world_size=1\n",
      "[2022-07-02 05:10:02,474] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... 4\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... True\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ True\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... mmap\n",
      "  data_path ....................... /home/sys/STRONGHOLD/data/my-gpt2-en_text_document\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... True\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ /home/sys/STRONGHOLD/DeepSpeedExample-Megatron-LM/examples/ds_zero_stage_infinity-cpu.json\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 1000\n",
      "  exit_interval ................... 50\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 2048\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ checkpoints/gpt2_ds\n",
      "  local_rank ...................... 0\n",
      "  log_interval .................... 10\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. 0.00015\n",
      "  lr_decay_iters .................. 320000\n",
      "  lr_decay_style .................. cosine\n",
      "  lr_decay_tokens ................. None\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 1024\n",
      "  memory_centric_tiled_linear ..... False\n",
      "  merge_file ...................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  min_lr .......................... 1e-05\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 16\n",
      "  num_layers ...................... 32\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... True\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  remote_device ................... none\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ checkpoints/gpt2_ds\n",
      "  save_interval ................... 10000\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  scattered_embeddings ............ False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... 1024\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 949,50,1\n",
      "  split_transformers .............. False\n",
      "  synchronize_each_layer .......... True\n",
      "  tensorboard_dir ................. None\n",
      "  tile_factor ..................... 1\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  tokens .......................... 0\n",
      "  train_iters ..................... 50\n",
      "  train_tokens .................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  use_pin_memory .................. False\n",
      "  vocab_file ...................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  warmup .......................... 0.01\n",
      "  warmup_iters .................... None\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 5000000000\n",
      "  zero_contigious_gradients ....... True\n",
      "  zero_reduce_bucket_size ......... 50000000\n",
      "  zero_reduce_scatter ............. True\n",
      "  zero_stage ...................... 3\n",
      "---------------- end of arguments ----------------\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "building GPT2 model ...\n",
      "[2022-07-02 05:10:05,998] [INFO] [utils.py:822:see_memory_usage] Before Building Model\n",
      "[2022-07-02 05:10:05,999] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2022-07-02 05:10:05,999] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 3.16 GB, percent = 3.5%\n",
      "[2022-07-02 05:10:06,000] [WARNING] [partition_parameters.py:457:__init__] zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f92ec33a310>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-07-02 05:10:11,102] [INFO] [utils.py:822:see_memory_usage] After Building Model\n",
      "[2022-07-02 05:10:11,103] [INFO] [utils.py:823:see_memory_usage] MA 0.0 GB         Max_MA 0.38 GB         CA 0.39 GB         Max_CA 0 GB \n",
      "[2022-07-02 05:10:11,103] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 9.74 GB, percent = 10.8%\n",
      " > number of parameters on model parallel rank 0            1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "DeepSpeed is enabled.\n",
      "[2022-07-02 05:10:11,107] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.7, git-hash=unknown, git-branch=unknown\n",
      "beginging get_train_batch_size = <function get_train_batch_size at 0x7f92ec33a310>\n",
      "train_batch = None, micro_batch=4\n",
      "[2022-07-02 05:10:11,117] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups using mpu\n",
      "[2022-07-02 05:10:11,117] [INFO] [logging.py:69:log_dist] [Rank 0] Initializing deepspeed groups with model parallel size 1, expert parallel size 1, and data parallel size 1\n",
      "[2022-07-02 05:10:11,117] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2022-07-02 05:10:11,118] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
      "[2022-07-02 05:10:11,119] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2022-07-02 05:10:11,119] [INFO] [engine.py:1084:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2022-07-02 05:10:11,119] [INFO] [engine.py:1090:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2022-07-02 05:10:11,144] [INFO] [engine.py:1106:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2022-07-02 05:10:11,144] [INFO] [utils.py:43:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2022-07-02 05:10:11,144] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "Initializing ZeRO Stage 3\n",
      "[2022-07-02 05:10:11,148] [INFO] [stage3.py:639:__init__] Reduce bucket size 90000000\n",
      "[2022-07-02 05:10:11,148] [INFO] [stage3.py:640:__init__] Allgather bucket size 50000000.0\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu114/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.22704815864562988 seconds\n",
      "[2022-07-02 05:10:26,579] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 9786.19\n",
      "[2022-07-02 05:10:26,594] [INFO] [stage3.py:811:__init__] optimizer state initialized\n",
      "[2022-07-02 05:10:27,351] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2022-07-02 05:10:27,351] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-07-02 05:10:27,351] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f92eb2083d0>\n",
      "[2022-07-02 05:10:27,351] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:10:27,352] [INFO] [config.py:1044:print] DeepSpeedEngine configuration:\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   allreduce_always_fp32 ........ False\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   amp_enabled .................. False\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   amp_params ................... False\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   bfloat16_enabled ............. False\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-07-02 05:10:27,353] [INFO] [config.py:1048:print]   curriculum_enabled ........... False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   curriculum_params ............ False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   dataloader_drop_last ......... False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   disable_allgather ............ False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   dump_state ................... False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_enabled ........... False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   eigenvalue_verbose ........... False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   elasticity_enabled ........... False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   fp16_enabled ................. False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   fp16_mixed_quantize .......... False\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   global_rank .................. 0\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   gradient_accumulation_steps .. 1\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   gradient_clipping ............ 1.0\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-07-02 05:10:27,354] [INFO] [config.py:1048:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   loss_scale ................... 0\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   memory_breakdown ............. False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   optimizer_name ............... None\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   optimizer_params ............. None\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   pld_enabled .................. False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   pld_params ................... False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   prescale_gradients ........... False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_change_rate ......... 0.001\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_groups .............. 1\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_offset .............. 1000\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_period .............. 1000\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_rounding ............ 0\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_start_bits .......... 16\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_target_bits ......... 8\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_training_enabled .... False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_type ................ 0\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   quantize_verbose ............. False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   scheduler_name ............... None\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   scheduler_params ............. None\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   sparse_attention ............. None\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   sparse_gradients_enabled ..... False\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   steps_per_print .............. 10\n",
      "[2022-07-02 05:10:27,355] [INFO] [config.py:1048:print]   tensorboard_enabled .......... False\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   tensorboard_output_path ...... \n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   train_batch_size ............. 4\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   train_micro_batch_size_per_gpu  4\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   use_quantizer_kernel ......... False\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   wall_clock_breakdown ......... True\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   world_size ................... 1\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   zero_allow_untested_optimizer  False\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   zero_config .................. {\n",
      "    \"stage\": 3, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 9.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 5, \n",
      "        \"buffer_size\": 1.000000e+08, \n",
      "        \"max_in_cpu\": 1, \n",
      "        \"pin_memory\": true\n",
      "    }, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": true, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false, \n",
      "        \"pipeline\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+08, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   zero_enabled ................. True\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1048:print]   zero_optimization_stage ...... 3\n",
      "[2022-07-02 05:10:27,356] [INFO] [config.py:1050:print]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_param_persitence_threshold\": 1.000000e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 5.000000e+07, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_bucket_size\": 9.000000e+07, \n",
      "        \"sub_group_size\": 1.000000e+08, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"buffer_count\": 4, \n",
      "            \"pipeline_read\": false, \n",
      "            \"pipeline_write\": false, \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"max_in_cpu\": 1, \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"zero_allow_untested_optimizer\": false, \n",
      "    \"aio\": {\n",
      "        \"block_size\": 1.048576e+06, \n",
      "        \"queue_depth\": 16, \n",
      "        \"single_submit\": false, \n",
      "        \"overlap_events\": true, \n",
      "        \"thread_count\": 2\n",
      "    }\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu114 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005660057067871094 seconds\n",
      "WARNING: could not find the metadata file checkpoints/gpt2_ds/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT2 ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000483 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT2 datasets ...\n",
      "setting training data start iteration to 0\n",
      "setting validation data start iteration to 0\n",
      "done with setups ...\n",
      "time (ms) | model and optimizer: 21404.61 | train/valid/test data iterators: 1238.24\n",
      "training ...\n",
      "[2022-07-02 05:10:50,962] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 14901.91\n",
      "[2022-07-02 05:10:50,963] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2431.17 | backward_microstep: 4990.71 | backward_inner_microstep: 4935.46 | backward_allreduce_microstep: 55.11 | step_microstep: 14932.53\n",
      "[2022-07-02 05:10:50,963] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 2431.18 | backward: 4990.71 | backward_inner: 4935.46 | backward_allreduce: 55.12 | step: 14932.54\n",
      "[2022-07-02 05:11:01,948] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4160.90\n",
      "[2022-07-02 05:11:01,948] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1776.06 | backward_microstep: 5013.98 | backward_inner_microstep: 4959.22 | backward_allreduce_microstep: 54.65 | step_microstep: 4191.81\n",
      "[2022-07-02 05:11:01,948] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1776.07 | backward: 5013.98 | backward_inner: 4959.24 | backward_allreduce: 54.66 | step: 4191.81\n",
      "[2022-07-02 05:11:13,036] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4187.09\n",
      "[2022-07-02 05:11:13,037] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1790.07 | backward_microstep: 5078.20 | backward_inner_microstep: 5023.21 | backward_allreduce_microstep: 54.88 | step_microstep: 4216.18\n",
      "[2022-07-02 05:11:13,037] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1790.09 | backward: 5078.20 | backward_inner: 5023.23 | backward_allreduce: 54.89 | step: 4216.19\n",
      "[2022-07-02 05:11:24,146] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4194.36\n",
      "[2022-07-02 05:11:24,147] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1799.81 | backward_microstep: 5082.22 | backward_inner_microstep: 5027.12 | backward_allreduce_microstep: 55.00 | step_microstep: 4224.49\n",
      "[2022-07-02 05:11:24,147] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1799.85 | backward: 5082.22 | backward_inner: 5027.13 | backward_allreduce: 55.01 | step: 4224.49\n",
      "[2022-07-02 05:11:35,251] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4182.63\n",
      "[2022-07-02 05:11:35,252] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.53 | backward_microstep: 5097.06 | backward_inner_microstep: 5042.33 | backward_allreduce_microstep: 54.62 | step_microstep: 4212.45\n",
      "[2022-07-02 05:11:35,252] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.54 | backward: 5097.06 | backward_inner: 5042.34 | backward_allreduce: 54.64 | step: 4212.45\n",
      "[2022-07-02 05:11:46,348] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4172.41\n",
      "[2022-07-02 05:11:46,349] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1811.83 | backward_microstep: 5079.44 | backward_inner_microstep: 5024.32 | backward_allreduce_microstep: 55.01 | step_microstep: 4201.99\n",
      "[2022-07-02 05:11:46,349] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1811.85 | backward: 5079.44 | backward_inner: 5024.34 | backward_allreduce: 55.02 | step: 4201.99\n",
      "[2022-07-02 05:11:57,439] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4180.73\n",
      "[2022-07-02 05:11:57,439] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1788.08 | backward_microstep: 5088.20 | backward_inner_microstep: 5033.54 | backward_allreduce_microstep: 54.55 | step_microstep: 4210.40\n",
      "[2022-07-02 05:11:57,440] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1788.09 | backward: 5088.20 | backward_inner: 5033.55 | backward_allreduce: 54.56 | step: 4210.40\n",
      "[2022-07-02 05:12:08,565] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4192.77\n",
      "[2022-07-02 05:12:08,566] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1800.40 | backward_microstep: 5099.30 | backward_inner_microstep: 5044.41 | backward_allreduce_microstep: 54.76 | step_microstep: 4222.28\n",
      "[2022-07-02 05:12:08,566] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1800.44 | backward: 5099.30 | backward_inner: 5044.42 | backward_allreduce: 54.79 | step: 4222.28\n",
      "[2022-07-02 05:12:19,655] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4171.23\n",
      "[2022-07-02 05:12:19,656] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.15 | backward_microstep: 5096.43 | backward_inner_microstep: 5041.63 | backward_allreduce_microstep: 54.69 | step_microstep: 4200.58\n",
      "[2022-07-02 05:12:19,656] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.17 | backward: 5096.43 | backward_inner: 5041.64 | backward_allreduce: 54.71 | step: 4200.58\n",
      "[2022-07-02 05:12:30,768] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4199.83\n",
      "[2022-07-02 05:12:30,768] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.6874999999999996e-07, 4.6874999999999996e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:12:30,768] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=0.3603886881693864\n",
      "[2022-07-02 05:12:30,769] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1785.70 | backward_microstep: 5093.19 | backward_inner_microstep: 5038.30 | backward_allreduce_microstep: 54.79 | step_microstep: 4229.67\n",
      "[2022-07-02 05:12:30,769] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1785.73 | backward: 5093.19 | backward_inner: 5038.31 | backward_allreduce: 54.80 | step: 4229.68\n",
      " iteration       10/      50 | elapsed time per iteration (ms): 12217.0 | learning rate: 4.687E-07 | lm loss: 1.114251E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "after 10 iterations memory (MB) | allocated: 791.330078125 | max allocated: 4971.755859375 | reserved: 7734.0 | max reserved: 7734.0\n",
      "time (ms) | forward: 1859.81 | backward: 5071.97 | backward-backward: 5071.93 | backward-allreduce: 0.00 | optimizer: 5284.50 | batch generator: 1.82\n",
      "Effective Tera Flops per GPU: 0.46 and total parameters 1.717 B\n",
      "[2022-07-02 05:12:41,864] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4178.32\n",
      "[2022-07-02 05:12:41,865] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1807.24 | backward_microstep: 5074.78 | backward_inner_microstep: 5019.82 | backward_allreduce_microstep: 54.84 | step_microstep: 4207.84\n",
      "[2022-07-02 05:12:41,865] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1807.27 | backward: 5074.78 | backward_inner: 5019.84 | backward_allreduce: 54.86 | step: 4207.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 05:12:52,954] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4171.79\n",
      "[2022-07-02 05:12:52,955] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1794.41 | backward_microstep: 5089.87 | backward_inner_microstep: 5034.74 | backward_allreduce_microstep: 54.98 | step_microstep: 4201.52\n",
      "[2022-07-02 05:12:52,955] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1794.46 | backward: 5089.87 | backward_inner: 5034.76 | backward_allreduce: 55.02 | step: 4201.52\n",
      "[2022-07-02 05:13:04,019] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4174.32\n",
      "[2022-07-02 05:13:04,020] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1781.02 | backward_microstep: 5075.60 | backward_inner_microstep: 5020.75 | backward_allreduce_microstep: 54.72 | step_microstep: 4204.49\n",
      "[2022-07-02 05:13:04,020] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1781.04 | backward: 5075.60 | backward_inner: 5020.77 | backward_allreduce: 54.74 | step: 4204.49\n",
      "[2022-07-02 05:13:15,115] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4210.53\n",
      "[2022-07-02 05:13:15,116] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1781.74 | backward_microstep: 5070.11 | backward_inner_microstep: 5014.96 | backward_allreduce_microstep: 55.01 | step_microstep: 4240.91\n",
      "[2022-07-02 05:13:15,116] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1781.76 | backward: 5070.11 | backward_inner: 5014.98 | backward_allreduce: 55.04 | step: 4240.91\n",
      "[2022-07-02 05:13:26,219] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4179.20\n",
      "[2022-07-02 05:13:26,220] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1794.02 | backward_microstep: 5097.02 | backward_inner_microstep: 5042.27 | backward_allreduce_microstep: 54.61 | step_microstep: 4209.01\n",
      "[2022-07-02 05:13:26,220] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1794.05 | backward: 5097.02 | backward_inner: 5042.28 | backward_allreduce: 54.64 | step: 4209.01\n",
      "[2022-07-02 05:13:37,335] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4179.65\n",
      "[2022-07-02 05:13:37,336] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1807.37 | backward_microstep: 5094.54 | backward_inner_microstep: 5039.40 | backward_allreduce_microstep: 55.01 | step_microstep: 4209.51\n",
      "[2022-07-02 05:13:37,336] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1807.39 | backward: 5094.55 | backward_inner: 5039.43 | backward_allreduce: 55.03 | step: 4209.51\n",
      "[2022-07-02 05:13:48,436] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4189.66\n",
      "[2022-07-02 05:13:48,437] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.15 | backward_microstep: 5086.21 | backward_inner_microstep: 5031.00 | backward_allreduce_microstep: 55.07 | step_microstep: 4219.50\n",
      "[2022-07-02 05:13:48,437] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.18 | backward: 5086.21 | backward_inner: 5031.03 | backward_allreduce: 55.09 | step: 4219.50\n",
      "[2022-07-02 05:13:59,516] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4177.31\n",
      "[2022-07-02 05:13:59,517] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1784.26 | backward_microstep: 5084.84 | backward_inner_microstep: 5029.73 | backward_allreduce_microstep: 54.97 | step_microstep: 4206.59\n",
      "[2022-07-02 05:13:59,517] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1784.28 | backward: 5084.84 | backward_inner: 5029.76 | backward_allreduce: 55.00 | step: 4206.59\n",
      "[2022-07-02 05:14:10,603] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4176.06\n",
      "[2022-07-02 05:14:10,604] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1786.44 | backward_microstep: 5090.94 | backward_inner_microstep: 5035.70 | backward_allreduce_microstep: 55.09 | step_microstep: 4206.23\n",
      "[2022-07-02 05:14:10,604] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1786.47 | backward: 5090.94 | backward_inner: 5035.73 | backward_allreduce: 55.12 | step: 4206.23\n",
      "[2022-07-02 05:14:21,698] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4186.64\n",
      "[2022-07-02 05:14:21,698] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.374999999999999e-07, 9.374999999999999e-07], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:14:21,698] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=0.3605656188836113\n",
      "[2022-07-02 05:14:21,699] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1784.04 | backward_microstep: 5089.60 | backward_inner_microstep: 5034.39 | backward_allreduce_microstep: 55.09 | step_microstep: 4217.02\n",
      "[2022-07-02 05:14:21,699] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1784.06 | backward: 5089.61 | backward_inner: 5034.41 | backward_allreduce: 55.12 | step: 4217.02\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 11093.0 | learning rate: 9.375E-07 | lm loss: 1.037666E+01 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1794.03 | backward: 5085.45 | backward-backward: 5085.41 | backward-allreduce: 0.00 | optimizer: 4212.53 | batch generator: 1.33\n",
      "Effective Tera Flops per GPU: 0.51 and total parameters 1.717 B\n",
      "[2022-07-02 05:14:32,796] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4174.58\n",
      "[2022-07-02 05:14:32,797] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1805.44 | backward_microstep: 5082.71 | backward_inner_microstep: 5027.45 | backward_allreduce_microstep: 55.12 | step_microstep: 4204.24\n",
      "[2022-07-02 05:14:32,797] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1805.46 | backward: 5082.71 | backward_inner: 5027.47 | backward_allreduce: 55.15 | step: 4204.24\n",
      "[2022-07-02 05:14:43,885] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4185.31\n",
      "[2022-07-02 05:14:43,886] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1796.66 | backward_microstep: 5073.98 | backward_inner_microstep: 5019.27 | backward_allreduce_microstep: 54.59 | step_microstep: 4214.93\n",
      "[2022-07-02 05:14:43,886] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1796.69 | backward: 5073.98 | backward_inner: 5019.29 | backward_allreduce: 54.61 | step: 4214.93\n",
      "[2022-07-02 05:14:54,970] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4183.36\n",
      "[2022-07-02 05:14:54,970] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.68 | backward_microstep: 5080.03 | backward_inner_microstep: 5024.95 | backward_allreduce_microstep: 54.95 | step_microstep: 4212.62\n",
      "[2022-07-02 05:14:54,971] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.70 | backward: 5080.03 | backward_inner: 5024.98 | backward_allreduce: 54.97 | step: 4212.62\n",
      "[2022-07-02 05:15:06,057] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4179.34\n",
      "[2022-07-02 05:15:06,058] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.55 | backward_microstep: 5086.95 | backward_inner_microstep: 5031.84 | backward_allreduce_microstep: 54.99 | step_microstep: 4208.97\n",
      "[2022-07-02 05:15:06,058] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.59 | backward: 5086.95 | backward_inner: 5031.85 | backward_allreduce: 55.01 | step: 4208.98\n",
      "[2022-07-02 05:15:17,158] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4187.77\n",
      "[2022-07-02 05:15:17,159] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1788.77 | backward_microstep: 5091.36 | backward_inner_microstep: 5036.27 | backward_allreduce_microstep: 54.95 | step_microstep: 4217.26\n",
      "[2022-07-02 05:15:17,159] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1788.80 | backward: 5091.36 | backward_inner: 5036.30 | backward_allreduce: 54.97 | step: 4217.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 05:15:28,275] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4190.73\n",
      "[2022-07-02 05:15:28,276] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1807.17 | backward_microstep: 5084.71 | backward_inner_microstep: 5029.00 | backward_allreduce_microstep: 55.57 | step_microstep: 4220.59\n",
      "[2022-07-02 05:15:28,276] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1807.22 | backward: 5084.71 | backward_inner: 5029.02 | backward_allreduce: 55.60 | step: 4220.60\n",
      "[2022-07-02 05:15:39,367] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4183.35\n",
      "[2022-07-02 05:15:39,368] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.76 | backward_microstep: 5085.81 | backward_inner_microstep: 5030.20 | backward_allreduce_microstep: 55.47 | step_microstep: 4213.01\n",
      "[2022-07-02 05:15:39,368] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.79 | backward: 5085.81 | backward_inner: 5030.21 | backward_allreduce: 55.50 | step: 4213.02\n",
      "[2022-07-02 05:15:50,454] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4174.47\n",
      "[2022-07-02 05:15:50,455] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.87 | backward_microstep: 5088.28 | backward_inner_microstep: 5033.03 | backward_allreduce_microstep: 55.08 | step_microstep: 4203.96\n",
      "[2022-07-02 05:15:50,455] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.92 | backward: 5088.28 | backward_inner: 5033.06 | backward_allreduce: 55.12 | step: 4203.97\n",
      "[2022-07-02 05:16:01,530] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4176.99\n",
      "[2022-07-02 05:16:01,531] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.10 | backward_microstep: 5078.24 | backward_inner_microstep: 5023.05 | backward_allreduce_microstep: 55.06 | step_microstep: 4206.39\n",
      "[2022-07-02 05:16:01,531] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.16 | backward: 5078.24 | backward_inner: 5023.08 | backward_allreduce: 55.08 | step: 4206.40\n",
      "[2022-07-02 05:16:12,641] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4187.06\n",
      "[2022-07-02 05:16:12,642] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.4062499999999999e-06, 1.4062499999999999e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:16:12,642] [INFO] [timer.py:181:stop] 0/30, SamplesPerSec=0.3606000886211385\n",
      "[2022-07-02 05:16:12,642] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.60 | backward_microstep: 5098.85 | backward_inner_microstep: 5044.03 | backward_allreduce_microstep: 54.67 | step_microstep: 4216.62\n",
      "[2022-07-02 05:16:12,642] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.64 | backward: 5098.84 | backward_inner: 5044.05 | backward_allreduce: 54.70 | step: 4216.63\n",
      " iteration       30/      50 | elapsed time per iteration (ms): 11094.3 | learning rate: 1.406E-06 | lm loss: 9.653947E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1796.10 | backward: 5085.19 | backward-backward: 5085.16 | backward-allreduce: 0.00 | optimizer: 4212.12 | batch generator: 1.33\n",
      "Effective Tera Flops per GPU: 0.51 and total parameters 1.717 B\n",
      "[2022-07-02 05:16:23,794] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4204.02\n",
      "[2022-07-02 05:16:23,795] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1824.75 | backward_microstep: 5087.85 | backward_inner_microstep: 5032.22 | backward_allreduce_microstep: 55.48 | step_microstep: 4234.46\n",
      "[2022-07-02 05:16:23,795] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1824.78 | backward: 5087.85 | backward_inner: 5032.24 | backward_allreduce: 55.52 | step: 4234.46\n",
      "[2022-07-02 05:16:34,883] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4181.94\n",
      "[2022-07-02 05:16:34,883] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1789.50 | backward_microstep: 5083.60 | backward_inner_microstep: 5027.70 | backward_allreduce_microstep: 55.78 | step_microstep: 4211.50\n",
      "[2022-07-02 05:16:34,883] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1789.53 | backward: 5083.60 | backward_inner: 5027.71 | backward_allreduce: 55.80 | step: 4211.50\n",
      "[2022-07-02 05:16:46,000] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4188.46\n",
      "[2022-07-02 05:16:46,000] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1796.83 | backward_microstep: 5098.45 | backward_inner_microstep: 5043.26 | backward_allreduce_microstep: 55.05 | step_microstep: 4217.90\n",
      "[2022-07-02 05:16:46,001] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1796.87 | backward: 5098.45 | backward_inner: 5043.28 | backward_allreduce: 55.08 | step: 4217.90\n",
      "[2022-07-02 05:16:57,096] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4184.21\n",
      "[2022-07-02 05:16:57,097] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1791.19 | backward_microstep: 5087.42 | backward_inner_microstep: 5032.53 | backward_allreduce_microstep: 54.76 | step_microstep: 4213.90\n",
      "[2022-07-02 05:16:57,097] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1791.21 | backward: 5087.42 | backward_inner: 5032.54 | backward_allreduce: 54.79 | step: 4213.90\n",
      "[2022-07-02 05:17:08,192] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4172.68\n",
      "[2022-07-02 05:17:08,192] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1797.96 | backward_microstep: 5091.56 | backward_inner_microstep: 5036.64 | backward_allreduce_microstep: 54.79 | step_microstep: 4202.26\n",
      "[2022-07-02 05:17:08,193] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1797.98 | backward: 5091.56 | backward_inner: 5036.66 | backward_allreduce: 54.81 | step: 4202.27\n",
      "[2022-07-02 05:17:19,309] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4190.66\n",
      "[2022-07-02 05:17:19,310] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1813.59 | backward_microstep: 5080.14 | backward_inner_microstep: 5024.86 | backward_allreduce_microstep: 55.11 | step_microstep: 4220.11\n",
      "[2022-07-02 05:17:19,310] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1813.61 | backward: 5080.14 | backward_inner: 5024.88 | backward_allreduce: 55.16 | step: 4220.11\n",
      "[2022-07-02 05:17:30,398] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4180.29\n",
      "[2022-07-02 05:17:30,399] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.61 | backward_microstep: 5087.87 | backward_inner_microstep: 5032.71 | backward_allreduce_microstep: 55.02 | step_microstep: 4209.43\n",
      "[2022-07-02 05:17:30,399] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.63 | backward: 5087.87 | backward_inner: 5032.73 | backward_allreduce: 55.05 | step: 4209.44\n",
      "[2022-07-02 05:17:41,510] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4190.67\n",
      "[2022-07-02 05:17:41,511] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1787.00 | backward_microstep: 5100.71 | backward_inner_microstep: 5045.38 | backward_allreduce_microstep: 55.17 | step_microstep: 4220.63\n",
      "[2022-07-02 05:17:41,511] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1787.03 | backward: 5100.71 | backward_inner: 5045.41 | backward_allreduce: 55.21 | step: 4220.63\n",
      "[2022-07-02 05:17:52,634] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4189.88\n",
      "[2022-07-02 05:17:52,635] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1788.53 | backward_microstep: 5111.51 | backward_inner_microstep: 5056.27 | backward_allreduce_microstep: 55.12 | step_microstep: 4219.33\n",
      "[2022-07-02 05:17:52,635] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1788.57 | backward: 5111.51 | backward_inner: 5056.28 | backward_allreduce: 55.14 | step: 4219.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-02 05:18:03,778] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4224.63\n",
      "[2022-07-02 05:18:03,779] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.8749999999999998e-06, 1.8749999999999998e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:18:03,779] [INFO] [timer.py:181:stop] 0/40, SamplesPerSec=0.36045088277337134\n",
      "[2022-07-02 05:18:03,779] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1795.24 | backward_microstep: 5088.82 | backward_inner_microstep: 5033.60 | backward_allreduce_microstep: 55.08 | step_microstep: 4255.51\n",
      "[2022-07-02 05:18:03,779] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1795.26 | backward: 5088.83 | backward_inner: 5033.62 | backward_allreduce: 55.11 | step: 4255.50\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 11113.7 | learning rate: 1.875E-06 | lm loss: 9.305648E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1800.17 | backward: 5091.89 | backward-backward: 5091.86 | backward-allreduce: 0.00 | optimizer: 4220.78 | batch generator: 1.37\n",
      "Effective Tera Flops per GPU: 0.51 and total parameters 1.717 B\n",
      "[2022-07-02 05:18:14,916] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4201.06\n",
      "[2022-07-02 05:18:14,917] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1814.19 | backward_microstep: 5085.82 | backward_inner_microstep: 5030.18 | backward_allreduce_microstep: 55.49 | step_microstep: 4231.32\n",
      "[2022-07-02 05:18:14,917] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1814.23 | backward: 5085.82 | backward_inner: 5030.19 | backward_allreduce: 55.54 | step: 4231.32\n",
      "[2022-07-02 05:18:26,047] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4195.81\n",
      "[2022-07-02 05:18:26,047] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1794.89 | backward_microstep: 5105.55 | backward_inner_microstep: 5050.02 | backward_allreduce_microstep: 55.39 | step_microstep: 4225.97\n",
      "[2022-07-02 05:18:26,047] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1794.90 | backward: 5105.55 | backward_inner: 5050.04 | backward_allreduce: 55.42 | step: 4225.97\n",
      "[2022-07-02 05:18:37,208] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4244.49\n",
      "[2022-07-02 05:18:37,209] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1801.86 | backward_microstep: 5081.52 | backward_inner_microstep: 5026.51 | backward_allreduce_microstep: 54.89 | step_microstep: 4273.96\n",
      "[2022-07-02 05:18:37,209] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1801.88 | backward: 5081.52 | backward_inner: 5026.53 | backward_allreduce: 54.91 | step: 4273.96\n",
      "[2022-07-02 05:18:48,320] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4190.46\n",
      "[2022-07-02 05:18:48,321] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1806.84 | backward_microstep: 5079.34 | backward_inner_microstep: 5023.73 | backward_allreduce_microstep: 55.49 | step_microstep: 4221.34\n",
      "[2022-07-02 05:18:48,321] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1806.86 | backward: 5079.34 | backward_inner: 5023.75 | backward_allreduce: 55.51 | step: 4221.34\n",
      "[2022-07-02 05:18:59,457] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4194.73\n",
      "[2022-07-02 05:18:59,458] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1810.49 | backward_microstep: 5097.93 | backward_inner_microstep: 5041.82 | backward_allreduce_microstep: 55.97 | step_microstep: 4224.69\n",
      "[2022-07-02 05:18:59,458] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1810.52 | backward: 5097.93 | backward_inner: 5041.83 | backward_allreduce: 56.00 | step: 4224.70\n",
      "[2022-07-02 05:19:10,568] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4185.70\n",
      "[2022-07-02 05:19:10,569] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1797.03 | backward_microstep: 5093.94 | backward_inner_microstep: 5038.37 | backward_allreduce_microstep: 55.45 | step_microstep: 4216.73\n",
      "[2022-07-02 05:19:10,569] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1797.04 | backward: 5093.94 | backward_inner: 5038.39 | backward_allreduce: 55.46 | step: 4216.73\n",
      "[2022-07-02 05:19:21,702] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4195.50\n",
      "[2022-07-02 05:19:21,703] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1797.59 | backward_microstep: 5106.60 | backward_inner_microstep: 5051.15 | backward_allreduce_microstep: 55.32 | step_microstep: 4225.33\n",
      "[2022-07-02 05:19:21,703] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1797.63 | backward: 5106.60 | backward_inner: 5051.19 | backward_allreduce: 55.33 | step: 4225.33\n",
      "[2022-07-02 05:19:32,815] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4185.14\n",
      "[2022-07-02 05:19:32,816] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1796.97 | backward_microstep: 5096.99 | backward_inner_microstep: 5041.47 | backward_allreduce_microstep: 55.38 | step_microstep: 4214.60\n",
      "[2022-07-02 05:19:32,816] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1797.00 | backward: 5096.99 | backward_inner: 5041.50 | backward_allreduce: 55.40 | step: 4214.60\n",
      "[2022-07-02 05:19:43,929] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4198.32\n",
      "[2022-07-02 05:19:43,930] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1790.68 | backward_microstep: 5091.41 | backward_inner_microstep: 5035.96 | backward_allreduce_microstep: 55.33 | step_microstep: 4228.27\n",
      "[2022-07-02 05:19:43,930] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1790.70 | backward: 5091.41 | backward_inner: 5035.98 | backward_allreduce: 55.35 | step: 4228.28\n",
      "[2022-07-02 05:19:55,107] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4224.89\n",
      "[2022-07-02 05:19:55,108] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.34375e-06, 2.34375e-06], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2022-07-02 05:19:55,108] [INFO] [timer.py:181:stop] 0/50, SamplesPerSec=0.3602342653120246\n",
      "[2022-07-02 05:19:55,108] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1823.44 | backward_microstep: 5096.08 | backward_inner_microstep: 5040.61 | backward_allreduce_microstep: 55.34 | step_microstep: 4255.26\n",
      "[2022-07-02 05:19:55,109] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1823.46 | backward: 5096.08 | backward_inner: 5040.63 | backward_allreduce: 55.37 | step: 4255.26\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 11132.9 | learning rate: 2.344E-06 | lm loss: 9.047054E+00 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward: 1806.34 | backward: 5093.61 | backward-backward: 5093.58 | backward-allreduce: 0.00 | optimizer: 4232.01 | batch generator: 1.36\n",
      "Effective Tera Flops per GPU: 0.51 and total parameters 1.717 B\n",
      "rank: 0 | time: 2022-07-02 05:19:55 | exiting the program at iteration 50\n",
      "[2022-07-02 05:20:02,057] [INFO] [launch.py:159:main] Process 37595 exits successfully.\n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ./examples/run.sh -m \"l2l\" -l 32 -h 2048\n",
    "%docker_exec ./examples/run.sh -m \"zero-offload\" -l 32 -h 2048 \n",
    "%docker_exec ./examples/run.sh -m \"zero-infinity\" -l 32 -h 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To print/draw the relevant information from log files\n",
    "\n",
    "> - extract and save useful infomation from the detailed logs to `./results/case3.csv` <br>\n",
    "> - visualize as a figure saved into `out/metric_throughput.png` <br>\n",
    "> - `docker_exec cat ./results/case3.csv` can look through the numeric values <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./out/metric_throughput.png?1937196995\" style=\"height:180px\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%docker_exec ./examples/case3_extract.sh\n",
    "%docker_exec python ./examples/case3_draw.py\n",
    "\n",
    "import random; __counter__ = random.randint(0,2e9)\n",
    "from IPython.display import HTML, display\n",
    "display(HTML('<img src=\"./out/metric_throughput.png?%d\" style=\"height:180px\">' % __counter__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 CASE - Nearly linear scaling as model size increases (Figure 8b in Section VI.B)\n",
    "\n",
    "In this case, we evaluate the performance (elapsed time per iteration - ms) as the model size increases. Similar to previous cases, the model size changes via increasing/decreasing the number of transformer layers. \n",
    "\n",
    "You would see the `elapsed time per iteration` linearly rise with the number of transformer layers (representing model size), proving STRONGHOLD's scalability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `ps aux` to check if there exists other running processes launched by other reviwers in case of GPU overlead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   4340  2716 pts/0    Ss+  02:10   0:00 /bin/bash\r\n",
      "root       38139  2.0  0.0   3976  3128 pts/1    Ss+  05:20   0:00 /bin/bash -c \r\n",
      "root       38297  0.0  0.0   5892  2896 pts/1    R+   05:20   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ps aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 8b in the submitted paper. Please refers to section VI.B on page 9 for more details.  <br><br>Runs around 60 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 92 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-92_hs-2048_bs-4_ws-15_2022-07-02.1656739206.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 92 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 92\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 92\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.494 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.698 seconds\n",
      "time to initialize megatron (seconds): 3.685\n",
      "[after megatron is initialized] datetime: 2022-07-02 05:20:13 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4738084864\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             4.738 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.22\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 05:21:26 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000567 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 05:21:27 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 73752.82 | train/valid/test-data-iterators-setup: 554.52\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 05:21:27 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 23971.4 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.065541E+01 | loss scale: 1.0 | grad norm: 101071.578 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.48 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 23.971434426307678;  SamplesPerSecond: 0.16686527509635227\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 25545.447265625 | reserved: 26236.0 | max reserved: 30634.0\n",
      "time (ms) | e2e-time: 23971.78 | forward-compute: 3389.81 | backward-compute: 20570.76 | backward-embedding-all-reduce: 0.02 | optimizer: 2.08 | batch-generator: 1.87 | offloading-func-call-overhead: 6665.49 | offloading-fwd-overhead: 2221.45 | offloading-bwd-overhead: 7.26 | offloading-fwd-2gpu-overhead: 1070.68 | offloading-fwd-2cpu-overhead: 1148.88 | offloading-bwd-2gpu-overhead: 3.62 | offloading-bwd-2cpu-overhead: 1.30\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 21066.3 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.017760E+01 | loss scale: 1.0 | grad norm: 3.527 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.37 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 21.066316080093383;  SamplesPerSecond: 0.18987657760341878\n",
      "time (ms) | e2e-time: 21066.23 | forward-compute: 3141.05 | backward-compute: 17914.25 | backward-embedding-all-reduce: 0.02 | optimizer: 2.09 | batch-generator: 1.37 | offloading-func-call-overhead: 63.44 | offloading-fwd-overhead: 2830.00 | offloading-bwd-overhead: 56.50 | offloading-fwd-2gpu-overhead: 1437.49 | offloading-fwd-2cpu-overhead: 1390.55 | offloading-bwd-2gpu-overhead: 3.82 | offloading-bwd-2cpu-overhead: 50.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 20956.8 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.006344E+01 | loss scale: 1.0 | grad norm: 10177.623 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.41 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 20.95683088302612;  SamplesPerSecond: 0.19086855366284317\n",
      "time (ms) | e2e-time: 20956.82 | forward-compute: 3162.93 | backward-compute: 17782.95 | backward-embedding-all-reduce: 0.02 | optimizer: 2.06 | batch-generator: 1.37 | offloading-func-call-overhead: 51.73 | offloading-fwd-overhead: 2865.03 | offloading-bwd-overhead: 8.45 | offloading-fwd-2gpu-overhead: 1396.72 | offloading-fwd-2cpu-overhead: 1466.29 | offloading-bwd-2gpu-overhead: 4.31 | offloading-bwd-2cpu-overhead: 1.42\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 20949.4 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.971938E+00 | loss scale: 1.0 | grad norm: 4452708.549 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.41 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 20.949375677108765;  SamplesPerSecond: 0.1909364776140213\n",
      "time (ms) | e2e-time: 20949.38 | forward-compute: 3126.86 | backward-compute: 17811.59 | backward-embedding-all-reduce: 0.02 | optimizer: 2.04 | batch-generator: 1.37 | offloading-func-call-overhead: 52.81 | offloading-fwd-overhead: 2830.33 | offloading-bwd-overhead: 193.46 | offloading-fwd-2gpu-overhead: 1361.88 | offloading-fwd-2cpu-overhead: 1466.59 | offloading-bwd-2gpu-overhead: 3.78 | offloading-bwd-2cpu-overhead: 187.15\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 21051.7 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.795580E+00 | loss scale: 1.0 | grad norm: 52587.120 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.38 and total parameters 4.738 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 21.05170738697052;  SamplesPerSecond: 0.19000834119876234\n",
      "time (ms) | e2e-time: 21051.70 | forward-compute: 3184.48 | backward-compute: 17856.31 | backward-embedding-all-reduce: 0.02 | optimizer: 2.07 | batch-generator: 1.35 | offloading-func-call-overhead: 51.13 | offloading-fwd-overhead: 2879.61 | offloading-bwd-overhead: 235.22 | offloading-fwd-2gpu-overhead: 1451.32 | offloading-fwd-2cpu-overhead: 1426.31 | offloading-bwd-2gpu-overhead: 3.83 | offloading-bwd-2cpu-overhead: 228.89\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 05:39:27 \n",
      "[after training is done] datetime: 2022-07-02 05:39:27 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 64 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-64_hs-2048_bs-4_ws-15_2022-07-02.1656740376.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 64 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 64\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 64\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.155 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.510 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.701 seconds\n",
      "time to initialize megatron (seconds): 3.694\n",
      "[after megatron is initialized] datetime: 2022-07-02 05:39:43 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3328053248\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             3.328 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.24\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 05:40:35 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000577 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 05:40:35 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 52180.47 | train/valid/test-data-iterators-setup: 490.28\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 05:40:35 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 15816.0 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.072083E+01 | loss scale: 1.0 | grad norm: 3.777 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.9 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 15.815969467163086;  SamplesPerSecond: 0.2529089353836165\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 24377.384765625 | reserved: 30652.0 | max reserved: 30652.0\n",
      "time (ms) | e2e-time: 15816.18 | forward-compute: 2106.09 | backward-compute: 13698.97 | backward-embedding-all-reduce: 0.02 | optimizer: 2.27 | batch-generator: 1.86 | offloading-func-call-overhead: 4271.17 | offloading-fwd-overhead: 1387.30 | offloading-bwd-overhead: 5.09 | offloading-fwd-2gpu-overhead: 678.28 | offloading-fwd-2cpu-overhead: 707.31 | offloading-bwd-2gpu-overhead: 2.29 | offloading-bwd-2cpu-overhead: 1.04\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 14347.6 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.034950E+01 | loss scale: 1.0 | grad norm: 3195.621 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.6 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.347553539276124;  SamplesPerSecond: 0.2787931746726078\n",
      "time (ms) | e2e-time: 14347.49 | forward-compute: 2064.45 | backward-compute: 12272.22 | backward-embedding-all-reduce: 0.02 | optimizer: 2.14 | batch-generator: 1.23 | offloading-func-call-overhead: 34.50 | offloading-fwd-overhead: 1860.17 | offloading-bwd-overhead: 171.58 | offloading-fwd-2gpu-overhead: 882.58 | offloading-fwd-2cpu-overhead: 976.36 | offloading-bwd-2gpu-overhead: 2.68 | offloading-bwd-2cpu-overhead: 167.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 14004.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.054702E+01 | loss scale: 1.0 | grad norm: 10.246 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.79 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.004148483276367;  SamplesPerSecond: 0.28562964787018397\n",
      "time (ms) | e2e-time: 14004.13 | forward-compute: 1975.78 | backward-compute: 12017.51 | backward-embedding-all-reduce: 0.02 | optimizer: 2.24 | batch-generator: 1.35 | offloading-func-call-overhead: 34.49 | offloading-fwd-overhead: 1761.04 | offloading-bwd-overhead: 282.85 | offloading-fwd-2gpu-overhead: 843.53 | offloading-fwd-2cpu-overhead: 916.28 | offloading-bwd-2gpu-overhead: 2.40 | offloading-bwd-2cpu-overhead: 278.75\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 14200.0 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.035212E+01 | loss scale: 1.0 | grad norm: 1317.855 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.68 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.199998378753662;  SamplesPerSecond: 0.28169017300627897\n",
      "time (ms) | e2e-time: 14200.02 | forward-compute: 1998.72 | backward-compute: 12190.42 | backward-embedding-all-reduce: 0.02 | optimizer: 2.25 | batch-generator: 1.33 | offloading-func-call-overhead: 35.46 | offloading-fwd-overhead: 1787.93 | offloading-bwd-overhead: 291.84 | offloading-fwd-2gpu-overhead: 855.37 | offloading-fwd-2cpu-overhead: 931.32 | offloading-bwd-2gpu-overhead: 2.60 | offloading-bwd-2cpu-overhead: 287.43\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 14112.7 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.014537E+01 | loss scale: 1.0 | grad norm: 3.523 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.73 and total parameters 3.328 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 14.112730073928834;  SamplesPerSecond: 0.2834320488697934\n",
      "time (ms) | e2e-time: 14112.70 | forward-compute: 2037.67 | backward-compute: 12064.18 | backward-embedding-all-reduce: 0.02 | optimizer: 2.24 | batch-generator: 1.31 | offloading-func-call-overhead: 34.53 | offloading-fwd-overhead: 1831.12 | offloading-bwd-overhead: 360.78 | offloading-fwd-2gpu-overhead: 855.75 | offloading-fwd-2cpu-overhead: 973.99 | offloading-bwd-2gpu-overhead: 2.42 | offloading-bwd-2cpu-overhead: 356.53\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 05:52:40 \n",
      "[after training is done] datetime: 2022-07-02 05:52:40 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 56 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-56_hs-2048_bs-4_ws-15_2022-07-02.1656741167.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 56 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 56\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 56\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.156 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.491 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.709 seconds\n",
      "time to initialize megatron (seconds): 3.689\n",
      "[after megatron is initialized] datetime: 2022-07-02 05:52:54 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2925187072\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             2.925 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.19\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 05:53:40 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000567 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 05:53:41 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 46211.45 | train/valid/test-data-iterators-setup: 485.90\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 05:53:41 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 13411.7 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.066479E+01 | loss scale: 1.0 | grad norm: 9.433 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.15 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 13.411726093292236;  SamplesPerSecond: 0.2982464726893406\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19087.3203125 | max allocated: 24361.384765625 | reserved: 30762.0 | max reserved: 30762.0\n",
      "time (ms) | e2e-time: 13411.89 | forward-compute: 1844.13 | backward-compute: 11556.78 | backward-embedding-all-reduce: 0.02 | optimizer: 2.31 | batch-generator: 1.80 | offloading-func-call-overhead: 3657.04 | offloading-fwd-overhead: 1220.52 | offloading-bwd-overhead: 4.12 | offloading-fwd-2gpu-overhead: 576.00 | offloading-fwd-2cpu-overhead: 643.46 | offloading-bwd-2gpu-overhead: 1.88 | offloading-bwd-2cpu-overhead: 0.77\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 11828.0 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 9.921777E+00 | loss scale: 1.0 | grad norm: 9216.532 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.1 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 11.82798159122467;  SamplesPerSecond: 0.3381811147700509\n",
      "time (ms) | e2e-time: 11827.90 | forward-compute: 1691.60 | backward-compute: 10125.57 | backward-embedding-all-reduce: 0.02 | optimizer: 2.27 | batch-generator: 1.23 | offloading-func-call-overhead: 31.93 | offloading-fwd-overhead: 1503.75 | offloading-bwd-overhead: 90.66 | offloading-fwd-2gpu-overhead: 703.48 | offloading-fwd-2cpu-overhead: 799.08 | offloading-bwd-2gpu-overhead: 2.21 | offloading-bwd-2cpu-overhead: 86.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 11808.3 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.802625E+00 | loss scale: 1.0 | grad norm: 229509.988 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.12 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 11.808258152008056;  SamplesPerSecond: 0.3387459817110942\n",
      "time (ms) | e2e-time: 11808.34 | forward-compute: 1667.69 | backward-compute: 10129.85 | backward-embedding-all-reduce: 0.02 | optimizer: 2.27 | batch-generator: 1.21 | offloading-func-call-overhead: 39.55 | offloading-fwd-overhead: 1475.02 | offloading-bwd-overhead: 141.16 | offloading-fwd-2gpu-overhead: 703.93 | offloading-fwd-2cpu-overhead: 769.98 | offloading-bwd-2gpu-overhead: 1.97 | offloading-bwd-2cpu-overhead: 137.69\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 11814.8 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.964223E+00 | loss scale: 1.0 | grad norm: 2.830 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.11 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 11.814820075035096;  SamplesPerSecond: 0.33855784299687003\n",
      "time (ms) | e2e-time: 11814.74 | forward-compute: 1706.36 | backward-compute: 10097.66 | backward-embedding-all-reduce: 0.02 | optimizer: 2.28 | batch-generator: 1.27 | offloading-func-call-overhead: 29.50 | offloading-fwd-overhead: 1524.42 | offloading-bwd-overhead: 31.71 | offloading-fwd-2gpu-overhead: 743.65 | offloading-fwd-2cpu-overhead: 779.21 | offloading-bwd-2gpu-overhead: 1.96 | offloading-bwd-2cpu-overhead: 28.28\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 11757.8 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.979256E+00 | loss scale: 1.0 | grad norm: 4907631.079 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.15 and total parameters 2.925 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 11.757825684547424;  SamplesPerSecond: 0.34019895406826367\n",
      "time (ms) | e2e-time: 11757.83 | forward-compute: 1662.32 | backward-compute: 10084.79 | backward-embedding-all-reduce: 0.02 | optimizer: 2.30 | batch-generator: 1.21 | offloading-func-call-overhead: 28.41 | offloading-fwd-overhead: 1483.41 | offloading-bwd-overhead: 32.11 | offloading-fwd-2gpu-overhead: 704.30 | offloading-fwd-2cpu-overhead: 777.86 | offloading-bwd-2gpu-overhead: 1.95 | offloading-bwd-2cpu-overhead: 28.68\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:03:47 \n",
      "[after training is done] datetime: 2022-07-02 06:03:47 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 40 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-40_hs-2048_bs-4_ws-15_2022-07-02.1656741833.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 40 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.516 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.703 seconds\n",
      "time to initialize megatron (seconds): 3.705\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:04:00 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2119454720\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             2.119 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.19\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:04:34 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000583 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:04:35 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 34607.47 | train/valid/test-data-iterators-setup: 449.96\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:04:35 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8930.2 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.090801E+01 | loss scale: 1.0 | grad norm: 21.327 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.78 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.930150699615478;  SamplesPerSecond: 0.44792077250972206\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 19088.3203125 | max allocated: 23801.361328125 | reserved: 30148.0 | max reserved: 30148.0\n",
      "time (ms) | e2e-time: 8930.23 | forward-compute: 1156.86 | backward-compute: 7762.39 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.75 | offloading-func-call-overhead: 2297.58 | offloading-fwd-overhead: 753.94 | offloading-bwd-overhead: 3.01 | offloading-fwd-2gpu-overhead: 355.02 | offloading-fwd-2cpu-overhead: 398.19 | offloading-bwd-2gpu-overhead: 1.10 | offloading-bwd-2cpu-overhead: 0.86\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7910.2 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.004745E+01 | loss scale: 1.0 | grad norm: 283175.505 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.78 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.910180640220642;  SamplesPerSecond: 0.5056774531369521\n",
      "time (ms) | e2e-time: 7910.07 | forward-compute: 1108.78 | backward-compute: 6790.61 | backward-embedding-all-reduce: 0.02 | optimizer: 2.40 | batch-generator: 1.24 | offloading-func-call-overhead: 19.89 | offloading-fwd-overhead: 978.28 | offloading-bwd-overhead: 2.84 | offloading-fwd-2gpu-overhead: 440.10 | offloading-fwd-2cpu-overhead: 537.43 | offloading-bwd-2gpu-overhead: 1.17 | offloading-bwd-2cpu-overhead: 0.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7891.7 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.006158E+01 | loss scale: 1.0 | grad norm: 8.309 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.8 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.891727471351624;  SamplesPerSecond: 0.5068598750426586\n",
      "time (ms) | e2e-time: 7891.73 | forward-compute: 1062.78 | backward-compute: 6818.28 | backward-embedding-all-reduce: 0.02 | optimizer: 2.37 | batch-generator: 1.14 | offloading-func-call-overhead: 18.26 | offloading-fwd-overhead: 935.75 | offloading-bwd-overhead: 2.84 | offloading-fwd-2gpu-overhead: 423.85 | offloading-fwd-2cpu-overhead: 511.15 | offloading-bwd-2gpu-overhead: 1.15 | offloading-bwd-2cpu-overhead: 0.62\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7971.0 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.797252E+00 | loss scale: 1.0 | grad norm: 520.923 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.71 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.970950675010681;  SamplesPerSecond: 0.5018221995200892\n",
      "time (ms) | e2e-time: 7970.97 | forward-compute: 1073.63 | backward-compute: 6886.66 | backward-embedding-all-reduce: 0.02 | optimizer: 2.40 | batch-generator: 1.22 | offloading-func-call-overhead: 18.25 | offloading-fwd-overhead: 940.22 | offloading-bwd-overhead: 2.82 | offloading-fwd-2gpu-overhead: 424.47 | offloading-fwd-2cpu-overhead: 514.93 | offloading-bwd-2gpu-overhead: 1.19 | offloading-bwd-2cpu-overhead: 0.59\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7944.1 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.470537E+00 | loss scale: 1.0 | grad norm: 16.654 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.74 and total parameters 2.119 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.9441002368927;  SamplesPerSecond: 0.5035183193464566\n",
      "time (ms) | e2e-time: 7944.09 | forward-compute: 1070.25 | backward-compute: 6863.16 | backward-embedding-all-reduce: 0.02 | optimizer: 2.37 | batch-generator: 1.24 | offloading-func-call-overhead: 19.42 | offloading-fwd-overhead: 928.44 | offloading-bwd-overhead: 2.88 | offloading-fwd-2gpu-overhead: 398.52 | offloading-fwd-2cpu-overhead: 529.18 | offloading-bwd-2gpu-overhead: 1.15 | offloading-bwd-2cpu-overhead: 0.58\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:11:21 \n",
      "[after training is done] datetime: 2022-07-02 06:11:21 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 24 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-24_hs-2048_bs-4_ws-15_2022-07-02.1656742286.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 24 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.154 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.503 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.690 seconds\n",
      "time to initialize megatron (seconds): 3.678\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:11:33 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1313722368\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.314 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.22\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:11:55 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000601 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:11:55 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 22024.88 | train/valid/test-data-iterators-setup: 393.47\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:11:55 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 4500.9 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.062243E+01 | loss scale: 1.0 | grad norm: 571467.321 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.56 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.500909781455993;  SamplesPerSecond: 0.8887092152969228\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 16783.1015625 | max allocated: 20793.791015625 | reserved: 25814.0 | max reserved: 25814.0\n",
      "time (ms) | e2e-time: 4500.92 | forward-compute: 473.31 | backward-compute: 4016.98 | backward-embedding-all-reduce: 0.02 | optimizer: 2.41 | batch-generator: 1.66 | offloading-func-call-overhead: 907.86 | offloading-fwd-overhead: 290.65 | offloading-bwd-overhead: 1.38 | offloading-fwd-2gpu-overhead: 4.60 | offloading-fwd-2cpu-overhead: 285.48 | offloading-bwd-2gpu-overhead: 0.38 | offloading-bwd-2cpu-overhead: 0.35\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 4124.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.011920E+01 | loss scale: 1.0 | grad norm: 149942138844672320.000 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.44 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.12436625957489;  SamplesPerSecond: 0.9698459710540576\n",
      "time (ms) | e2e-time: 4124.29 | forward-compute: 397.45 | backward-compute: 3716.35 | backward-embedding-all-reduce: 0.02 | optimizer: 2.51 | batch-generator: 1.17 | offloading-func-call-overhead: 9.81 | offloading-fwd-overhead: 301.78 | offloading-bwd-overhead: 1.56 | offloading-fwd-2gpu-overhead: 3.96 | offloading-fwd-2cpu-overhead: 297.27 | offloading-bwd-2gpu-overhead: 0.39 | offloading-bwd-2cpu-overhead: 0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 4121.6 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.004763E+01 | loss scale: 1.0 | grad norm: 4.123 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.44 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.121577477455139;  SamplesPerSecond: 0.9705021977337166\n",
      "time (ms) | e2e-time: 4121.57 | forward-compute: 497.78 | backward-compute: 3613.31 | backward-embedding-all-reduce: 0.01 | optimizer: 2.50 | batch-generator: 1.16 | offloading-func-call-overhead: 10.40 | offloading-fwd-overhead: 409.09 | offloading-bwd-overhead: 1.86 | offloading-fwd-2gpu-overhead: 12.47 | offloading-fwd-2cpu-overhead: 396.16 | offloading-bwd-2gpu-overhead: 0.39 | offloading-bwd-2cpu-overhead: 0.39\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 4152.3 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.520592E+00 | loss scale: 1.0 | grad norm: 45.700 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.37 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.152256774902344;  SamplesPerSecond: 0.9633315608459873\n",
      "time (ms) | e2e-time: 4152.26 | forward-compute: 415.82 | backward-compute: 3725.94 | backward-embedding-all-reduce: 0.01 | optimizer: 2.49 | batch-generator: 1.12 | offloading-func-call-overhead: 8.88 | offloading-fwd-overhead: 333.57 | offloading-bwd-overhead: 1.43 | offloading-fwd-2gpu-overhead: 13.62 | offloading-fwd-2cpu-overhead: 319.50 | offloading-bwd-2gpu-overhead: 0.40 | offloading-bwd-2cpu-overhead: 0.39\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 4166.1 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.480372E+00 | loss scale: 1.0 | grad norm: 5.285 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.33 and total parameters 1.314 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 4.166137742996216;  SamplesPerSecond: 0.9601218794852586\n",
      "time (ms) | e2e-time: 4166.13 | forward-compute: 481.88 | backward-compute: 3673.76 | backward-embedding-all-reduce: 0.01 | optimizer: 2.49 | batch-generator: 1.15 | offloading-func-call-overhead: 9.61 | offloading-fwd-overhead: 399.81 | offloading-bwd-overhead: 1.46 | offloading-fwd-2gpu-overhead: 15.95 | offloading-fwd-2cpu-overhead: 383.41 | offloading-bwd-2gpu-overhead: 0.38 | offloading-bwd-2cpu-overhead: 0.38\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:15:26 \n",
      "[after training is done] datetime: 2022-07-02 06:15:26 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 16 2048 16 1024 4 15 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-16_hs-2048_bs-4_ws-15_2022-07-02.1656742529.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 16 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 15 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 16\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 15\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 16\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.155 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.508 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.692 seconds\n",
      "time to initialize megatron (seconds): 3.688\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:15:36 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 910856192\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             0.911 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.22\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:15:51 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000552 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.005 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:15:52 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 15809.69 | train/valid/test-data-iterators-setup: 387.91\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:15:52 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 2723.1 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.131046E+01 | loss scale: 1.0 | grad norm: inf | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.96 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.7231252908706667;  SamplesPerSecond: 1.4689004627918083\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 13708.4765625 | max allocated: 16917.89404296875 | reserved: 17542.0 | max reserved: 17542.0\n",
      "time (ms) | e2e-time: 2723.24 | forward-compute: 47.50 | backward-compute: 2664.91 | backward-embedding-all-reduce: 0.01 | optimizer: 2.39 | batch-generator: 1.70 | offloading-func-call-overhead: 132.48 | offloading-fwd-overhead: 1.07 | offloading-bwd-overhead: 0.63 | offloading-fwd-2gpu-overhead: 0.01 | offloading-fwd-2cpu-overhead: 0.79 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.21\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 2722.8 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.127647E+01 | loss scale: 1.0 | grad norm: 49692041799014.695 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.96 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.722784161567688;  SamplesPerSecond: 1.469084496839784\n",
      "time (ms) | e2e-time: 2722.67 | forward-compute: 33.88 | backward-compute: 2678.36 | backward-embedding-all-reduce: 0.01 | optimizer: 2.27 | batch-generator: 1.11 | offloading-func-call-overhead: 3.66 | offloading-fwd-overhead: 1.66 | offloading-bwd-overhead: 0.65 | offloading-fwd-2gpu-overhead: 0.01 | offloading-fwd-2cpu-overhead: 1.39 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 2729.6 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.114230E+01 | loss scale: 1.0 | grad norm: inf | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 10.93 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.7295880794525145;  SamplesPerSecond: 1.465422577901314\n",
      "time (ms) | e2e-time: 2729.54 | forward-compute: 34.53 | backward-compute: 2684.71 | backward-embedding-all-reduce: 0.01 | optimizer: 2.19 | batch-generator: 1.21 | offloading-func-call-overhead: 3.76 | offloading-fwd-overhead: 0.95 | offloading-bwd-overhead: 0.66 | offloading-fwd-2gpu-overhead: 0.01 | offloading-fwd-2cpu-overhead: 0.66 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.24\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 2660.2 | learning rate: 1.875E-06 | global batch size:     4 | loss scale: 1.0 | grad norm: nan | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 11.22 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.6601618766784667;  SamplesPerSecond: 1.5036678914421866\n",
      "time (ms) | e2e-time: 2660.16 | forward-compute: 34.24 | backward-compute: 2616.80 | backward-embedding-all-reduce: 0.01 | optimizer: 2.03 | batch-generator: 1.26 | offloading-func-call-overhead: 3.64 | offloading-fwd-overhead: 1.70 | offloading-bwd-overhead: 0.64 | offloading-fwd-2gpu-overhead: 0.01 | offloading-fwd-2cpu-overhead: 1.43 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.23\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 2627.3 | learning rate: 2.344E-06 | global batch size:     4 | loss scale: 1.0 | grad norm: nan | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 11.36 and total parameters 0.911 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 2.6272608757019045;  SamplesPerSecond: 1.522498217437715\n",
      "time (ms) | e2e-time: 2627.25 | forward-compute: 34.62 | backward-compute: 2584.09 | backward-embedding-all-reduce: 0.01 | optimizer: 1.92 | batch-generator: 1.23 | offloading-func-call-overhead: 3.70 | offloading-fwd-overhead: 1.45 | offloading-bwd-overhead: 0.64 | offloading-fwd-2gpu-overhead: 0.01 | offloading-fwd-2cpu-overhead: 1.17 | offloading-bwd-2gpu-overhead: 0.05 | offloading-bwd-2cpu-overhead: 0.23\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:18:07 \n",
      "[after training is done] datetime: 2022-07-02 06:18:07 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 92 -h 2048 -w 15\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 64 -h 2048 -w 15\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 56 -h 2048 -w 15\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 40 -h 2048 -w 15\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 24 -h 2048 -w 15\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 16 -h 2048 -w 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To print/draw the relevant information from log files\n",
    "\n",
    "> - extract and save useful infomation from the detailed logs to `./results/case4.csv` <br>\n",
    "> - visualize as a figure saved into `out/metric_linear_scaling.png` <br>\n",
    "> - `docker_exec cat ./results/case4.csv` can look through the numeric values <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./out/metric_linear_scaling.png?738133565\" style=\"height:180px\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%docker_exec ./examples/case4_extract.sh\n",
    "%docker_exec python ./examples/case4_draw.py\n",
    "\n",
    "import random; __counter__ = random.randint(0,2e9)\n",
    "from IPython.display import HTML, display\n",
    "display(HTML('<img src=\"./out/metric_linear_scaling.png?%d\" style=\"height:180px\">' % __counter__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 CASE - Impact of working window size (Figure 9 in Section VI.C)\n",
    "\n",
    "Working window size affects the throughput. The larger window can better overlap GPU computation with data transfer, leading to higher training throughput. But, a larger window size means more GPU memory occupancy.\n",
    "\n",
    "This case evaluates the impact of working window size for STRONGHOLD with 1.7B model. You will see that at the first stage, the larger window size can gain more benefits, while at the end of the stage, enlarging window size shows no influence because the current window size can hide the data transformation process.\n",
    "\n",
    "PS: The bandwidth restriction in the virtual machine might slightly hurt the performance of STRONGHOLD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `ps aux` to check if there exists other running processes launched by other reviwers in case of GPU overlead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
      "root           1  0.0  0.0   4340  2724 pts/0    Ss+  02:10   0:00 /bin/bash\r\n",
      "root       43293  0.0  0.0   3976  3160 pts/1    Ss+  06:18   0:00 /bin/bash -c \r\n",
      "root       43451  0.0  0.0   5892  2924 pts/1    R+   06:18   0:00 ps aux\r\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ps aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following results correspond to Figure 9 in the submitted paper. Please refers to Section VI.C on page 10 for more details. <br><br>Runs around 48 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 2 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-2_2022-07-02.1656742692.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 2 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 2\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 2\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.514 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.707 seconds\n",
      "time to initialize megatron (seconds): 3.725\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:18:18 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.24\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:18:45 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000610 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:18:45 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 26447.49 | train/valid/test-data-iterators-setup: 439.60\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:18:45 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8997.5 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.066589E+01 | loss scale: 1.0 | grad norm: 3.989 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.25 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.99746663570404;  SamplesPerSecond: 0.44456958407904174\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 4103.3857421875 | max allocated: 7472.5986328125 | reserved: 8730.0 | max reserved: 8730.0\n",
      "time (ms) | e2e-time: 8997.57 | forward-compute: 1243.45 | backward-compute: 7743.13 | backward-embedding-all-reduce: 0.01 | optimizer: 2.55 | batch-generator: 1.73 | offloading-func-call-overhead: 2636.63 | offloading-fwd-overhead: 826.52 | offloading-bwd-overhead: 4362.60 | offloading-fwd-2gpu-overhead: 255.14 | offloading-fwd-2cpu-overhead: 570.78 | offloading-bwd-2gpu-overhead: 314.97 | offloading-bwd-2cpu-overhead: 4046.77\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7961.1 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.097493E+01 | loss scale: 1.0 | grad norm: 37.261 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.07 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.96107587814331;  SamplesPerSecond: 0.5024446521081123\n",
      "time (ms) | e2e-time: 7960.96 | forward-compute: 1123.31 | backward-compute: 6826.96 | backward-embedding-all-reduce: 0.02 | optimizer: 2.52 | batch-generator: 1.13 | offloading-func-call-overhead: 15.60 | offloading-fwd-overhead: 1039.33 | offloading-bwd-overhead: 5410.40 | offloading-fwd-2gpu-overhead: 344.66 | offloading-fwd-2cpu-overhead: 694.07 | offloading-bwd-2gpu-overhead: 426.52 | offloading-bwd-2cpu-overhead: 4982.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7862.0 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.068723E+01 | loss scale: 1.0 | grad norm: 8.260 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.15 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.862011671066284;  SamplesPerSecond: 0.508775637502647\n",
      "time (ms) | e2e-time: 7862.01 | forward-compute: 1143.78 | backward-compute: 6707.56 | backward-embedding-all-reduce: 0.01 | optimizer: 2.48 | batch-generator: 1.12 | offloading-func-call-overhead: 15.54 | offloading-fwd-overhead: 1058.23 | offloading-bwd-overhead: 5319.03 | offloading-fwd-2gpu-overhead: 297.93 | offloading-fwd-2cpu-overhead: 759.71 | offloading-bwd-2gpu-overhead: 405.30 | offloading-bwd-2cpu-overhead: 4912.77\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7983.3 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.063645E+01 | loss scale: 1.0 | grad norm: 40.865 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.05 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.983258318901062;  SamplesPerSecond: 0.5010485493785978\n",
      "time (ms) | e2e-time: 7983.26 | forward-compute: 1137.65 | backward-compute: 6834.94 | backward-embedding-all-reduce: 0.01 | optimizer: 2.51 | batch-generator: 1.12 | offloading-func-call-overhead: 15.68 | offloading-fwd-overhead: 1054.31 | offloading-bwd-overhead: 5519.38 | offloading-fwd-2gpu-overhead: 341.33 | offloading-fwd-2cpu-overhead: 712.38 | offloading-bwd-2gpu-overhead: 396.24 | offloading-bwd-2cpu-overhead: 5122.22\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7908.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.069070E+01 | loss scale: 1.0 | grad norm: 6.698 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.11 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.908854341506958;  SamplesPerSecond: 0.5057622542126421\n",
      "time (ms) | e2e-time: 7908.86 | forward-compute: 1109.72 | backward-compute: 6788.47 | backward-embedding-all-reduce: 0.01 | optimizer: 2.50 | batch-generator: 1.15 | offloading-func-call-overhead: 15.61 | offloading-fwd-overhead: 1025.73 | offloading-bwd-overhead: 5664.38 | offloading-fwd-2gpu-overhead: 294.16 | offloading-fwd-2cpu-overhead: 730.97 | offloading-bwd-2gpu-overhead: 481.58 | offloading-bwd-2cpu-overhead: 5181.88\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:25:32 \n",
      "[after training is done] datetime: 2022-07-02 06:25:32 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 4 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-4_2022-07-02.1656743137.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 4 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 4\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.155 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.505 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.706 seconds\n",
      "time to initialize megatron (seconds): 3.695\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:25:43 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.23\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:26:10 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000533 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:26:11 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27309.03 | train/valid/test-data-iterators-setup: 429.87\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:26:11 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8709.8 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.039135E+01 | loss scale: 1.0 | grad norm: 9.327 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.46 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.709780168533324;  SamplesPerSecond: 0.4592538413829538\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 6408.6064453125 | max allocated: 9969.9208984375 | reserved: 12154.0 | max reserved: 12154.0\n",
      "time (ms) | e2e-time: 8709.87 | forward-compute: 1178.96 | backward-compute: 7519.82 | backward-embedding-all-reduce: 0.02 | optimizer: 2.48 | batch-generator: 1.69 | offloading-func-call-overhead: 2464.66 | offloading-fwd-overhead: 757.59 | offloading-bwd-overhead: 4286.17 | offloading-fwd-2gpu-overhead: 186.10 | offloading-fwd-2cpu-overhead: 570.89 | offloading-bwd-2gpu-overhead: 192.55 | offloading-bwd-2cpu-overhead: 4092.78\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7738.4 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 9.964964E+00 | loss scale: 1.0 | grad norm: 3.757 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.27 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.738399267196655;  SamplesPerSecond: 0.5169027678574482\n",
      "time (ms) | e2e-time: 7738.24 | forward-compute: 1038.60 | backward-compute: 6688.94 | backward-embedding-all-reduce: 0.02 | optimizer: 2.47 | batch-generator: 1.14 | offloading-func-call-overhead: 15.48 | offloading-fwd-overhead: 935.18 | offloading-bwd-overhead: 5238.69 | offloading-fwd-2gpu-overhead: 239.34 | offloading-fwd-2cpu-overhead: 695.27 | offloading-bwd-2gpu-overhead: 185.88 | offloading-bwd-2cpu-overhead: 5051.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7702.9 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.430505E+00 | loss scale: 1.0 | grad norm: 3.716 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.3 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.702899551391601;  SamplesPerSecond: 0.5192849748738269\n",
      "time (ms) | e2e-time: 7702.90 | forward-compute: 1045.38 | backward-compute: 6646.81 | backward-embedding-all-reduce: 0.01 | optimizer: 2.45 | batch-generator: 1.13 | offloading-func-call-overhead: 15.67 | offloading-fwd-overhead: 941.81 | offloading-bwd-overhead: 4778.44 | offloading-fwd-2gpu-overhead: 172.14 | offloading-fwd-2cpu-overhead: 769.09 | offloading-bwd-2gpu-overhead: 137.80 | offloading-bwd-2cpu-overhead: 4639.76\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7540.5 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 9.865253E+00 | loss scale: 1.0 | grad norm: 6.360 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.46 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.540488505363465;  SamplesPerSecond: 0.5304696104443161\n",
      "time (ms) | e2e-time: 7540.49 | forward-compute: 1037.08 | backward-compute: 6492.68 | backward-embedding-all-reduce: 0.01 | optimizer: 2.49 | batch-generator: 1.15 | offloading-func-call-overhead: 15.43 | offloading-fwd-overhead: 938.18 | offloading-bwd-overhead: 5332.31 | offloading-fwd-2gpu-overhead: 252.32 | offloading-fwd-2cpu-overhead: 685.25 | offloading-bwd-2gpu-overhead: 269.91 | offloading-bwd-2cpu-overhead: 5061.52\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7761.7 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.625390E+00 | loss scale: 1.0 | grad norm: 2.756 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.25 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.761740016937256;  SamplesPerSecond: 0.5153483614848492\n",
      "time (ms) | e2e-time: 7761.73 | forward-compute: 1040.81 | backward-compute: 6710.23 | backward-embedding-all-reduce: 0.01 | optimizer: 2.48 | batch-generator: 1.16 | offloading-func-call-overhead: 15.76 | offloading-fwd-overhead: 935.66 | offloading-bwd-overhead: 5533.14 | offloading-fwd-2gpu-overhead: 198.64 | offloading-fwd-2cpu-overhead: 736.44 | offloading-bwd-2gpu-overhead: 241.96 | offloading-bwd-2cpu-overhead: 5290.30\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:32:45 \n",
      "[after training is done] datetime: 2022-07-02 06:32:45 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 6 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-6_2022-07-02.1656743570.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 6 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 6\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 6\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.506 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.708 seconds\n",
      "time to initialize megatron (seconds): 3.709\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:32:56 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.23\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:33:23 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000589 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:33:24 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27046.01 | train/valid/test-data-iterators-setup: 425.18\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:33:24 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8292.9 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.060957E+01 | loss scale: 1.0 | grad norm: 3.354 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.78 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.292879629135133;  SamplesPerSecond: 0.48234150004383475\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 8713.8271484375 | max allocated: 12611.3759765625 | reserved: 15368.0 | max reserved: 15368.0\n",
      "time (ms) | e2e-time: 8292.93 | forward-compute: 1067.48 | backward-compute: 7214.62 | backward-embedding-all-reduce: 0.02 | optimizer: 2.52 | batch-generator: 1.66 | offloading-func-call-overhead: 2290.40 | offloading-fwd-overhead: 680.82 | offloading-bwd-overhead: 3757.97 | offloading-fwd-2gpu-overhead: 122.79 | offloading-fwd-2cpu-overhead: 557.40 | offloading-bwd-2gpu-overhead: 101.77 | offloading-bwd-2cpu-overhead: 3655.35\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7274.2 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.003025E+01 | loss scale: 1.0 | grad norm: 7.998 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.73 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.274199342727661;  SamplesPerSecond: 0.5498886972349716\n",
      "time (ms) | e2e-time: 7274.12 | forward-compute: 965.30 | backward-compute: 6298.27 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.19 | offloading-func-call-overhead: 17.19 | offloading-fwd-overhead: 859.34 | offloading-bwd-overhead: 4863.51 | offloading-fwd-2gpu-overhead: 249.10 | offloading-fwd-2cpu-overhead: 609.61 | offloading-bwd-2gpu-overhead: 199.11 | offloading-bwd-2cpu-overhead: 4663.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7213.0 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.018981E+01 | loss scale: 1.0 | grad norm: 41.109 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.8 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.212998676300049;  SamplesPerSecond: 0.5545543787693892\n",
      "time (ms) | e2e-time: 7213.00 | forward-compute: 948.47 | backward-compute: 6253.97 | backward-embedding-all-reduce: 0.01 | optimizer: 2.48 | batch-generator: 1.10 | offloading-func-call-overhead: 15.78 | offloading-fwd-overhead: 840.10 | offloading-bwd-overhead: 4986.02 | offloading-fwd-2gpu-overhead: 157.17 | offloading-fwd-2cpu-overhead: 682.30 | offloading-bwd-2gpu-overhead: 241.93 | offloading-bwd-2cpu-overhead: 4743.20\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7144.7 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.037998E+01 | loss scale: 1.0 | grad norm: 4068.716 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.87 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.144651603698731;  SamplesPerSecond: 0.5598593496048472\n",
      "time (ms) | e2e-time: 7144.66 | forward-compute: 954.96 | backward-compute: 6179.12 | backward-embedding-all-reduce: 0.02 | optimizer: 2.50 | batch-generator: 1.09 | offloading-func-call-overhead: 16.14 | offloading-fwd-overhead: 846.11 | offloading-bwd-overhead: 5009.91 | offloading-fwd-2gpu-overhead: 212.86 | offloading-fwd-2cpu-overhead: 632.63 | offloading-bwd-2gpu-overhead: 137.85 | offloading-bwd-2cpu-overhead: 4871.15\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7176.9 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.004955E+01 | loss scale: 1.0 | grad norm: 5.636 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.84 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.176896977424621;  SamplesPerSecond: 0.5573439346534095\n",
      "time (ms) | e2e-time: 7176.87 | forward-compute: 970.71 | backward-compute: 6195.61 | backward-embedding-all-reduce: 0.02 | optimizer: 2.48 | batch-generator: 1.16 | offloading-func-call-overhead: 15.82 | offloading-fwd-overhead: 870.89 | offloading-bwd-overhead: 4864.47 | offloading-fwd-2gpu-overhead: 150.15 | offloading-fwd-2cpu-overhead: 720.12 | offloading-bwd-2gpu-overhead: 182.19 | offloading-bwd-2cpu-overhead: 4681.39\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:39:35 \n",
      "[after training is done] datetime: 2022-07-02 06:39:35 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 8 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-8_2022-07-02.1656743979.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 8 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 8\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 8\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.157 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.504 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.706 seconds\n",
      "time to initialize megatron (seconds): 3.701\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:39:46 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.21\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:40:13 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000584 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:40:13 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27264.07 | train/valid/test-data-iterators-setup: 430.11\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:40:13 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 8122.4 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.082718E+01 | loss scale: 1.0 | grad norm: 3.446 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 6.93 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 8.122358536720276;  SamplesPerSecond: 0.4924677951504414\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 11019.0478515625 | max allocated: 15124.6982421875 | reserved: 19804.0 | max reserved: 19804.0\n",
      "time (ms) | e2e-time: 8122.41 | forward-compute: 985.95 | backward-compute: 7125.59 | backward-embedding-all-reduce: 0.02 | optimizer: 2.51 | batch-generator: 1.66 | offloading-func-call-overhead: 2135.54 | offloading-fwd-overhead: 620.74 | offloading-bwd-overhead: 2772.60 | offloading-fwd-2gpu-overhead: 136.43 | offloading-fwd-2cpu-overhead: 483.68 | offloading-bwd-2gpu-overhead: 51.83 | offloading-bwd-2cpu-overhead: 2719.92\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 7039.6 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.055900E+01 | loss scale: 1.0 | grad norm: 98.524 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.99 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.039569973945618;  SamplesPerSecond: 0.5682165266919046\n",
      "time (ms) | e2e-time: 7039.48 | forward-compute: 849.24 | backward-compute: 6179.66 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.14 | offloading-func-call-overhead: 15.99 | offloading-fwd-overhead: 744.02 | offloading-bwd-overhead: 2335.93 | offloading-fwd-2gpu-overhead: 218.62 | offloading-fwd-2cpu-overhead: 524.77 | offloading-bwd-2gpu-overhead: 6.23 | offloading-bwd-2cpu-overhead: 2328.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 7306.3 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 9.887196E+00 | loss scale: 1.0 | grad norm: 11.667 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.7 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.306340432167053;  SamplesPerSecond: 0.5474696993845939\n",
      "time (ms) | e2e-time: 7306.43 | forward-compute: 895.03 | backward-compute: 6400.73 | backward-embedding-all-reduce: 0.01 | optimizer: 2.46 | batch-generator: 1.16 | offloading-func-call-overhead: 16.62 | offloading-fwd-overhead: 789.98 | offloading-bwd-overhead: 3395.84 | offloading-fwd-2gpu-overhead: 162.99 | offloading-fwd-2cpu-overhead: 626.25 | offloading-bwd-2gpu-overhead: 22.92 | offloading-bwd-2cpu-overhead: 3372.06\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 7201.5 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.007727E+01 | loss scale: 1.0 | grad norm: 3801.006 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.81 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.201494789123535;  SamplesPerSecond: 0.5554402408290604\n",
      "time (ms) | e2e-time: 7201.41 | forward-compute: 873.67 | backward-compute: 6317.18 | backward-embedding-all-reduce: 0.02 | optimizer: 2.45 | batch-generator: 1.14 | offloading-func-call-overhead: 15.79 | offloading-fwd-overhead: 778.49 | offloading-bwd-overhead: 3086.55 | offloading-fwd-2gpu-overhead: 202.42 | offloading-fwd-2cpu-overhead: 575.45 | offloading-bwd-2gpu-overhead: 28.51 | offloading-bwd-2cpu-overhead: 3057.18\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 7230.5 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.880602E+00 | loss scale: 1.0 | grad norm: 8.179 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.78 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.23047034740448;  SamplesPerSecond: 0.5532143564402943\n",
      "time (ms) | e2e-time: 7230.47 | forward-compute: 857.87 | backward-compute: 6362.01 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.16 | offloading-func-call-overhead: 15.91 | offloading-fwd-overhead: 753.84 | offloading-bwd-overhead: 2886.61 | offloading-fwd-2gpu-overhead: 144.45 | offloading-fwd-2cpu-overhead: 608.78 | offloading-bwd-2gpu-overhead: 16.50 | offloading-bwd-2cpu-overhead: 2869.25\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:46:22 \n",
      "[after training is done] datetime: 2022-07-02 06:46:22 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 10 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-10_2022-07-02.1656744387.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 10 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 10\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 10\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.159 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.513 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.701 seconds\n",
      "time to initialize megatron (seconds): 3.707\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:46:33 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.22\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:47:01 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000639 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:47:01 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27900.79 | train/valid/test-data-iterators-setup: 421.78\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:47:01 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 7988.6 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.069844E+01 | loss scale: 1.0 | grad norm: 20.568 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.04 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.988566541671753;  SamplesPerSecond: 0.5007156138882116\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 13324.2685546875 | max allocated: 17494.0830078125 | reserved: 22392.0 | max reserved: 22392.0\n",
      "time (ms) | e2e-time: 7988.64 | forward-compute: 940.19 | backward-compute: 7037.68 | backward-embedding-all-reduce: 0.02 | optimizer: 2.50 | batch-generator: 1.70 | offloading-func-call-overhead: 2070.40 | offloading-fwd-overhead: 599.96 | offloading-bwd-overhead: 1364.94 | offloading-fwd-2gpu-overhead: 120.54 | offloading-fwd-2cpu-overhead: 478.78 | offloading-bwd-2gpu-overhead: 1.04 | offloading-bwd-2cpu-overhead: 1363.03\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 6484.0 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.075629E+01 | loss scale: 1.0 | grad norm: 1076762726.444 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.68 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.484015917778015;  SamplesPerSecond: 0.6169016317545911\n",
      "time (ms) | e2e-time: 6483.97 | forward-compute: 868.60 | backward-compute: 5604.82 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.17 | offloading-func-call-overhead: 15.48 | offloading-fwd-overhead: 763.63 | offloading-bwd-overhead: 863.06 | offloading-fwd-2gpu-overhead: 213.91 | offloading-fwd-2cpu-overhead: 549.10 | offloading-bwd-2gpu-overhead: 1.02 | offloading-bwd-2cpu-overhead: 861.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 6787.7 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.052729E+01 | loss scale: 1.0 | grad norm: 2652389.006 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.29 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.7876777172088625;  SamplesPerSecond: 0.589303170635041\n",
      "time (ms) | e2e-time: 6787.67 | forward-compute: 813.48 | backward-compute: 5963.67 | backward-embedding-all-reduce: 0.01 | optimizer: 2.45 | batch-generator: 1.14 | offloading-func-call-overhead: 15.22 | offloading-fwd-overhead: 712.31 | offloading-bwd-overhead: 2280.06 | offloading-fwd-2gpu-overhead: 129.19 | offloading-fwd-2cpu-overhead: 581.88 | offloading-bwd-2gpu-overhead: 1.03 | offloading-bwd-2cpu-overhead: 2278.07\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 6881.0 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.060887E+01 | loss scale: 1.0 | grad norm: 5135.751 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.17 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.880998659133911;  SamplesPerSecond: 0.5813109692573994\n",
      "time (ms) | e2e-time: 6880.97 | forward-compute: 841.14 | backward-compute: 6029.33 | backward-embedding-all-reduce: 0.01 | optimizer: 2.46 | batch-generator: 1.19 | offloading-func-call-overhead: 16.84 | offloading-fwd-overhead: 739.11 | offloading-bwd-overhead: 2309.12 | offloading-fwd-2gpu-overhead: 205.11 | offloading-fwd-2cpu-overhead: 533.38 | offloading-bwd-2gpu-overhead: 4.08 | offloading-bwd-2cpu-overhead: 2304.20\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 6667.1 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.039007E+01 | loss scale: 1.0 | grad norm: 6364.675 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.44 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.667085742950439;  SamplesPerSecond: 0.599962285505248\n",
      "time (ms) | e2e-time: 6667.08 | forward-compute: 835.81 | backward-compute: 5820.77 | backward-embedding-all-reduce: 0.02 | optimizer: 2.42 | batch-generator: 1.12 | offloading-func-call-overhead: 15.61 | offloading-fwd-overhead: 731.75 | offloading-bwd-overhead: 1682.85 | offloading-fwd-2gpu-overhead: 160.89 | offloading-fwd-2cpu-overhead: 570.24 | offloading-bwd-2gpu-overhead: 12.14 | offloading-bwd-2cpu-overhead: 1669.44\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:52:49 \n",
      "[after training is done] datetime: 2022-07-02 06:52:49 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 12 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-12_2022-07-02.1656744774.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 12 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 12\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 12\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.159 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.490 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.712 seconds\n",
      "time to initialize megatron (seconds): 3.702\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:53:00 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.19\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:53:28 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000576 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:53:28 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 27811.44 | train/valid/test-data-iterators-setup: 430.60\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:53:28 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 7418.1 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.080807E+01 | loss scale: 1.0 | grad norm: 7244.278 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.58 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.418055510520935;  SamplesPerSecond: 0.5392248675312351\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 15629.4892578125 | max allocated: 20072.4677734375 | reserved: 25804.0 | max reserved: 25804.0\n",
      "time (ms) | e2e-time: 7418.12 | forward-compute: 928.81 | backward-compute: 6478.44 | backward-embedding-all-reduce: 0.02 | optimizer: 2.50 | batch-generator: 1.67 | offloading-func-call-overhead: 1899.34 | offloading-fwd-overhead: 601.77 | offloading-bwd-overhead: 730.95 | offloading-fwd-2gpu-overhead: 119.60 | offloading-fwd-2cpu-overhead: 481.53 | offloading-bwd-2gpu-overhead: 0.92 | offloading-bwd-2cpu-overhead: 729.13\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 6619.1 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.047859E+01 | loss scale: 1.0 | grad norm: 3.779 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.5 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.619143533706665;  SamplesPerSecond: 0.604307789917351\n",
      "time (ms) | e2e-time: 6619.06 | forward-compute: 811.54 | backward-compute: 5796.90 | backward-embedding-all-reduce: 0.01 | optimizer: 2.44 | batch-generator: 1.16 | offloading-func-call-overhead: 15.15 | offloading-fwd-overhead: 712.90 | offloading-bwd-overhead: 1781.75 | offloading-fwd-2gpu-overhead: 198.36 | offloading-fwd-2cpu-overhead: 513.83 | offloading-bwd-2gpu-overhead: 0.94 | offloading-bwd-2cpu-overhead: 1779.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 6544.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.039556E+01 | loss scale: 1.0 | grad norm: 23.211 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.6 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.544146966934204;  SamplesPerSecond: 0.6112332165232401\n",
      "time (ms) | e2e-time: 6544.15 | forward-compute: 817.64 | backward-compute: 5715.91 | backward-embedding-all-reduce: 0.01 | optimizer: 2.45 | batch-generator: 1.18 | offloading-func-call-overhead: 16.28 | offloading-fwd-overhead: 709.04 | offloading-bwd-overhead: 1019.43 | offloading-fwd-2gpu-overhead: 179.04 | offloading-fwd-2cpu-overhead: 528.76 | offloading-bwd-2gpu-overhead: 0.99 | offloading-bwd-2cpu-overhead: 1017.53\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 6414.5 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.039095E+01 | loss scale: 1.0 | grad norm: 2140257538.361 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.77 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.4144775390625;  SamplesPerSecond: 0.623589368836517\n",
      "time (ms) | e2e-time: 6414.47 | forward-compute: 872.63 | backward-compute: 5531.25 | backward-embedding-all-reduce: 0.02 | optimizer: 2.42 | batch-generator: 1.14 | offloading-func-call-overhead: 15.27 | offloading-fwd-overhead: 765.39 | offloading-bwd-overhead: 520.32 | offloading-fwd-2gpu-overhead: 176.70 | offloading-fwd-2cpu-overhead: 588.07 | offloading-bwd-2gpu-overhead: 0.95 | offloading-bwd-2cpu-overhead: 518.35\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 6337.0 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 1.014726E+01 | loss scale: 1.0 | grad norm: 4542.972 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.88 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.336977148056031;  SamplesPerSecond: 0.6312157842051023\n",
      "time (ms) | e2e-time: 6337.00 | forward-compute: 814.93 | backward-compute: 5511.48 | backward-embedding-all-reduce: 0.02 | optimizer: 2.43 | batch-generator: 1.19 | offloading-func-call-overhead: 14.52 | offloading-fwd-overhead: 712.83 | offloading-bwd-overhead: 866.99 | offloading-fwd-2gpu-overhead: 114.02 | offloading-fwd-2cpu-overhead: 598.20 | offloading-bwd-2gpu-overhead: 0.94 | offloading-bwd-2cpu-overhead: 865.03\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 06:59:02 \n",
      "[after training is done] datetime: 2022-07-02 06:59:02 \n",
      "/home/sys/STRONGHOLD\n",
      "cd /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/.. && /home/sys/STRONGHOLD/examples/../SHv0-Megatron-LM/examples/sc22-gpt-sh.sh 32 2048 16 1024 4 14 2>&1 | tee /home/sys/STRONGHOLD/examples/../results/log_stronghold_l-32_hs-2048_bs-4_ws-14_2022-07-02.1656745146.txt && cd -\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory\n",
      "cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory\n",
      "PYTHONGIL=1 python pretrain_gpt.py --num-layers 32 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /home/sys/STRONGHOLD/data/my-gpt2-en_text_document --vocab-file /home/sys/STRONGHOLD/data/gpt2-vocab.json --merge-file /home/sys/STRONGHOLD/data/gpt2-merges.txt --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 14 --gl-ray-max-concurrency 12\n",
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 14\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['/home/sys/STRONGHOLD/data/my-gpt2-en_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_gl ....................................... True\n",
      "  enable_l2l ...................................... False\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 1000\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... 50\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  gl_debug_print .................................. False\n",
      "  gl_enable_ddp ................................... False\n",
      "  gl_ray_max_concurrency .......................... 12\n",
      "  gl_window_size .................................. 14\n",
      "  gl_world_size ................................... 1\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 128\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./checkpoints/gpt2\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... /home/sys/STRONGHOLD/data/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./checkpoints/gpt2\n",
      "  save_interval ................................... 10000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /home/sys/STRONGHOLD/data/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      ">-- rank=0; local_rank=0;\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      ".  > the rank=0 is ready...\n",
      ".   > rank=0; local_rank=0, device=0\n",
      "--------distributed env init done ----------\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "make: Entering directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/data'\n",
      "> compiling dataset index builder ...\n",
      ">>> done with dataset index builder. Compilation time: 0.158 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.530 seconds\n",
      "> compiling and loading optimizer utils ...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module optimizer_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module optimizer_utils...\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module offloading_utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module offloading_utils...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sys/STRONGHOLD/SHv0-Megatron-LM/megatron/optimizer/build/build.ninja...\n",
      "Building extension module deepspeed_cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module deepspeed_cpu_adam...\n",
      ">>> done with compiling and loading optimizer utilss. Compilation time: 0.690 seconds\n",
      "time to initialize megatron (seconds): 3.711\n",
      "[after megatron is initialized] datetime: 2022-07-02 06:59:13 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1716588544\n",
      "--- init model: rank=0   world-size=1---\n",
      " > number of parameters on pipeline model parallel rank 0,             tensor model parallel rank 0             1.717 Billion\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./checkpoints/gpt2/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.18\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-07-02 06:59:41 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      200\n",
      "    validation: 4000\n",
      "    test:       4000\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000571 seconds\n",
      "    number of documents: 6421508\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 6094011) total of 6094011 documents\n",
      "    validation:\n",
      "     document indices in [6094011, 6415086) total of 321075 documents\n",
      "    test:\n",
      "     document indices in [6415086, 6421508) total of 6422 documents\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_train_indexmap_200ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 3154519\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_valid_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 108654\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /home/sys/STRONGHOLD/data/my-gpt2-en_text_document_test_indexmap_4000ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 5485\n",
      "    total number of epochs: 3\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-07-02 06:59:41 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 28036.87 | train/valid/test-data-iterators-setup: 429.06\n",
      "training ...\n",
      "[before the start of training step] datetime: 2022-07-02 06:59:41 \n",
      " iteration       10/      50 | elapsed time per iteration (ms): 7230.1 | learning rate: 4.687E-07 | global batch size:     4 | lm loss: 1.092560E+01 | loss scale: 1.0 | grad norm: 7997236.509 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 7.78 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 7.230081987380982;  SamplesPerSecond: 0.5532440720563608\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 17934.7099609375 | max allocated: 22377.7978515625 | reserved: 28602.0 | max reserved: 28602.0\n",
      "time (ms) | e2e-time: 7230.13 | forward-compute: 1012.50 | backward-compute: 6206.67 | backward-embedding-all-reduce: 0.02 | optimizer: 2.42 | batch-generator: 1.71 | offloading-func-call-overhead: 1807.15 | offloading-fwd-overhead: 714.23 | offloading-bwd-overhead: 190.17 | offloading-fwd-2gpu-overhead: 183.35 | offloading-fwd-2cpu-overhead: 530.26 | offloading-bwd-2gpu-overhead: 0.84 | offloading-bwd-2cpu-overhead: 188.25\n",
      " iteration       20/      50 | elapsed time per iteration (ms): 6221.8 | learning rate: 9.375E-07 | global batch size:     4 | lm loss: 1.069896E+01 | loss scale: 1.0 | grad norm: 30.195 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 9.04 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.221842551231385;  SamplesPerSecond: 0.6428963714628791\n",
      "time (ms) | e2e-time: 6221.75 | forward-compute: 763.98 | backward-compute: 5447.09 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.20 | offloading-func-call-overhead: 14.55 | offloading-fwd-overhead: 658.07 | offloading-bwd-overhead: 613.51 | offloading-fwd-2gpu-overhead: 181.84 | offloading-fwd-2cpu-overhead: 475.58 | offloading-bwd-2gpu-overhead: 0.82 | offloading-bwd-2cpu-overhead: 611.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       30/      50 | elapsed time per iteration (ms): 6491.1 | learning rate: 1.406E-06 | global batch size:     4 | lm loss: 1.045013E+01 | loss scale: 1.0 | grad norm: 39.976 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.67 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.491117906570435;  SamplesPerSecond: 0.616226674291515\n",
      "time (ms) | e2e-time: 6491.14 | forward-compute: 787.34 | backward-compute: 5693.09 | backward-embedding-all-reduce: 0.02 | optimizer: 2.44 | batch-generator: 1.22 | offloading-func-call-overhead: 14.44 | offloading-fwd-overhead: 685.26 | offloading-bwd-overhead: 156.35 | offloading-fwd-2gpu-overhead: 128.67 | offloading-fwd-2cpu-overhead: 555.97 | offloading-bwd-2gpu-overhead: 0.86 | offloading-bwd-2cpu-overhead: 154.60\n",
      " iteration       40/      50 | elapsed time per iteration (ms): 6550.0 | learning rate: 1.875E-06 | global batch size:     4 | lm loss: 1.022740E+01 | loss scale: 1.0 | grad norm: 2447.818 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.59 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.5499849081039425;  SamplesPerSecond: 0.6106884299918028\n",
      "time (ms) | e2e-time: 6549.98 | forward-compute: 941.81 | backward-compute: 5597.47 | backward-embedding-all-reduce: 0.01 | optimizer: 2.46 | batch-generator: 1.15 | offloading-func-call-overhead: 15.04 | offloading-fwd-overhead: 834.49 | offloading-bwd-overhead: 40.04 | offloading-fwd-2gpu-overhead: 251.84 | offloading-fwd-2cpu-overhead: 581.92 | offloading-bwd-2gpu-overhead: 0.84 | offloading-bwd-2cpu-overhead: 38.31\n",
      " iteration       50/      50 | elapsed time per iteration (ms): 6289.5 | learning rate: 2.344E-06 | global batch size:     4 | lm loss: 9.965919E+00 | loss scale: 1.0 | grad norm: 9.101 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "Effective Tera Flops per GPU: 8.94 and total parameters 1.717 B\n",
      "NumWorkers: 1; SamplesPerStep: 4; IterationTime: 6.289536046981811;  SamplesPerSecond: 0.6359769576198706\n",
      "time (ms) | e2e-time: 6289.54 | forward-compute: 809.96 | backward-compute: 5468.88 | backward-embedding-all-reduce: 0.02 | optimizer: 2.46 | batch-generator: 1.18 | offloading-func-call-overhead: 13.61 | offloading-fwd-overhead: 711.31 | offloading-bwd-overhead: 125.67 | offloading-fwd-2gpu-overhead: 124.81 | offloading-fwd-2cpu-overhead: 585.88 | offloading-bwd-2gpu-overhead: 0.83 | offloading-bwd-2cpu-overhead: 123.97\n",
      "[exiting program at iteration 50] datetime: 2022-07-02 07:05:09 \n",
      "[after training is done] datetime: 2022-07-02 07:05:09 \n",
      "/home/sys/STRONGHOLD\n"
     ]
    }
   ],
   "source": [
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 2\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 4\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 6\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 8\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 10\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 12\n",
    "%docker_exec ./examples/run.sh -m \"stronghold\" -l 32 -h 2048 -w 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To print/draw the relevant information from log files\n",
    "\n",
    "> - extract and save useful infomation from the detailed logs to `./results/case5.csv` <br>\n",
    "> - visualize as a figure saved into `out/metric_window_size.png` <br>\n",
    "> - `docker_exec cat ./results/case5.csv` can look through the numeric values <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./out/metric_window_size.png?425030524\" style=\"height:180px\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%docker_exec ./examples/case5_extract.sh\n",
    "%docker_exec python ./examples/case5_draw.py\n",
    "\n",
    "import random; __counter__ = random.randint(0,2e9)\n",
    "from IPython.display import HTML, display\n",
    "display(HTML('<img src=\"./out/metric_window_size.png?%d\" style=\"height:180px\">' % __counter__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the time per iteration(ms) is slightly longer at the smaller window sizes. But, as the window size increases, the iteration time will stay at stable value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# The end of this Artifact Evaluation\n",
    "-----\n",
    "\n",
    "#### Many thanks for your review, time and efforts on this artifact evaluation.  <br> Many thanks for your understanding and bearing with some inconveniences on this notebook. \n",
    "\n",
    "The repository will be open as soon as possible. Users can reproduce other experiment figures using the released docker image or source code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
